# 0x11. Attention
## Details
 By: Alexa Orrico, Software Engineer at Holberton School Weight: 5Ongoing second chance project - startedSep 19, 2022 12:00 AM, must end bySep 29, 2022 12:00 AM An auto review will be launched at the deadline#### In a nutshell…
* Auto QA review:          0.0/43 mandatory      
* Altogether:         0.0%* Mandatory: 0.0%
* Optional: no optional tasks

 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/7/4704cf0750335400050c494f69844150e6319d1b.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220928%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220928T180029Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e87d97f7bc22599d1318810bdd4216329cf929f28e978706290aa1cc102350e8) 

## Resources:
Read or watch:
* [Attention for RNN Seq2Seq Models](https://intranet.hbtn.io/rltoken/spMoDTnM35UuwwxWLs_SoQ) 

* [Attention Model Intuition](https://intranet.hbtn.io/rltoken/7Ot5OlxrjDuAPyQdtHYxqA) 

* [Attention Model](https://intranet.hbtn.io/rltoken/pw8xV6DMI1yMKY05Rzhq0g) 

* [How Transformers work in deep learning and NLP: an intuitive introduction](https://intranet.hbtn.io/rltoken/iBgxQFTCee-R1IVsYUcQFw) 

* [Transformers](https://intranet.hbtn.io/rltoken/YNLfzhmf8fRm1k6XoaQ2qA) 

* [Bert, GPT : The Illustrated GPT-2 - Visualizing Transformer Language Models](https://intranet.hbtn.io/rltoken/BuJV0Fw7-lcXW06xxVhE5g) 

* [SQuAD](https://intranet.hbtn.io/rltoken/hj1iAmCUDCnH72SGnTxsXA) 

* [Glue](https://intranet.hbtn.io/rltoken/T17sCQuLN3yT8MlZ955AGA) 

* [Self supervised learning](https://intranet.hbtn.io/rltoken/6wjtunlD93Ajbz1Ss1l_lQ) 

- [How Does Attention Work in Encoder-Decoder Recurrent Neural Networks](/rltoken/KKW6LXmtOfzDrQCtX3trpg)- [Attention Model](/rltoken/pw8xV6DMI1yMKY05Rzhq0g)- [What is a Transformer?](/rltoken/ju6UjnXhDuMFS6LzEIlj0Q)- [How Transformers Work](/rltoken/BSCFsAVD7mI9Oye0yGAjcA)- [Transformer: A Novel Neural Network Architecture for Language Understanding](/rltoken/zeZrN58D0iOt161zLNqUMw)- [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention](/rltoken/01S-rlsZr6WfdsqMC2BEYQ)- [(Transformer) Attention Is All You Need | AISC Foundational](/rltoken/AEZqvJXaWBEZVdGKy57yQw)- [Transformer Models in NLP](/rltoken/vdCwNWQ6pM10Rc2g2Nwo0A)- [Transformer model for language understanding](/rltoken/SCgjN25SiFFEWzYLOCqRnw)- [Generative Modeling with Sparse Transformers](/rltoken/ukLu5CMMIoP_4Yt7MNY8UQ)- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](/rltoken/cXTOtTK9waaB-tM5C3EAMQ)- [(BERT) Pretranied Deep Bidirectional Transformers for Language Understanding (algorithm) | TDLS](/rltoken/FoCSA7ZVrzZyTGUGIgEtPg)**References:**- [Sequence to Sequence Learning with Neural Networks (2014)](/rltoken/v3oiayVt7mUWFVHp-LsSlw)- [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (2014)](/rltoken/cSywNMc0VkeDq4TZ4V8nEg)- [Neural Machine Translation by Jointly Learning to Align and Translate](/rltoken/YlDIODUFbkYQbRL3a5CwEQ)- [Attention Is All You Need (2017)](/rltoken/V29PxakOS66KNWeS016NAQ)- [tf.keras.layers.Embedding](/rltoken/XEpXcqIg5l9DoS2-2E7InQ)- [tf.keras.layers.LayerNormalization](/rltoken/-qb21N0gxX5UmRub_bLasg)- [Improving Language Understanding by Generative Pre-Training (2018)](/rltoken/fBJbaYK3mQliTYVmerk8Gw)- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](/rltoken/Xo7RMyK_8WiLXaerfTHoWw)- [SQuAD 2.0](/rltoken/hj1iAmCUDCnH72SGnTxsXA)- [Know What You Don’t Know: Unanswerable Questions for SQuAD (2018)](/rltoken/2gCjBpi3CDZ25VyRnwy3ow)- [GLUE Benchmark](/rltoken/XQMKBVpVRJ7PH_Fam-mYkw)- [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (2019)](/rltoken/gc0N-1a5GhwB6r8i4-PJ4A)**More recent papers in NLP:**- [Generating Long Sequences with Sparse Transformers (2019)](/rltoken/Vr6j7CGFKtuK7IetaIeMcw)- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)](/rltoken/kEkphkks_kGLj3XuXBMCAA)- [XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)](/rltoken/THgst72PaoA0cYrpfOQWDA)- [Language Models are Unsupervised Multitask Learners (GPT-2, 2019)](/rltoken/C2rjrR_0Dq0_XU8Bu9INbQ)- [Language Models are Few-Shot Learners (GPT-3, 2020)](/rltoken/2y-mtFWYnQ8QbtMij0PoWA)- [ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations (2020)](/rltoken/u6oVFipCalDn-X1QTwVsPA)To keep up with the newest papers and their code bases go to [paperswithcode.com](/rltoken/3VaoLjK3QGn7jJbnU6MpKQ). For example, check out the [raked list of state of the art models for Language Modelling on Penn Treebank](/rltoken/v3qkGtmWqnn_M3JgkqukYQ).## Learning Objectives
At the end of this project, you are expected to be able to  [explain to anyone](https://intranet.hbtn.io/rltoken/EE2g5_2rO8xiioY0mr_Nvg) 
 ,  without the help of Google :
### General
* What is the attention mechanism?
* How to apply attention to RNNs
* What is a transformer?
* How to create an encoder-decoder transformer model
* What is GPT? 
* What is BERT?
* What is self-supervised learning?
* How to use BERT for specific NLP tasks
* What is SQuAD? GLUE?
## Requirements
### General
* Allowed editors:  ` vi ` ,  ` vim ` ,  ` emacs ` 
* All your files will be interpreted/compiled on Ubuntu 16.04 LTS using  ` python3 `  (version 3.5)
* Your files will be executed with  ` numpy `  (version 1.16) and  ` tensorflow `  (version 1.15)
* All your files should end with a new line
* The first line of all your files should be exactly  ` #!/usr/bin/env python3 ` 
* All of your files must be executable
* A  ` README.md `  file, at the root of the folder of the project, is mandatory
* Your code should follow the  ` pycodestyle `  style (version 2.4)
* All your modules should have documentation ( ` python3 -c 'print(__import__("my_module").__doc__)' ` )
* All your classes should have documentation ( ` python3 -c 'print(__import__("my_module").MyClass.__doc__)' ` )
* All your functions (inside and outside a class) should have documentation ( ` python3 -c 'print(__import__("my_module").my_function.__doc__)' `  and  ` python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)' ` )
* Unless otherwise stated, you cannot import any module except  ` import tensorflow as tf ` 
## Update Tensorflow to 1.15
In order to complete the following tasks, you will need to update   ` tensorflow `   to version 1.15, which will also update   ` numpy `   to version 1.16
 ` pip install --user tensorflow==1.15
 ` ### Quiz questions
Great!          You've completed the quiz successfully! Keep going!          (Show quiz)#### 
        
        Question #0
    
 Quiz question Body What is the Attention mechanism?
 Quiz question Answers * An RNN

* A transformer

* A method for determining which terms are most important in a sequence

* None of the above

 Quiz question Tips #### 
        
        Question #1
    
 Quiz question Body A Transformer:
 Quiz question Answers * Is a novel neural network

* Utilizes the Attention mechanism

* Utilizes RNNs

* Utilizes Fully Connected Networks

* Utilizes CNNs

* Utilizes dropout

* Utilizes layer normalization

 Quiz question Tips #### 
        
        Question #2
    
 Quiz question Body BERT was novel because:
 Quiz question Answers * It used transformers for the first time

* It introduced self-supervised learning techniques

* It utilized layer normalization for the first time

* It can be fine tuned for various NLP tasks

 Quiz question Tips #### 
        
        Question #3
    
 Quiz question Body The database to use for Question-Answering is:
 Quiz question Answers * GLUE

* SQuAD

* Penn Treebank

* WikiText

 Quiz question Tips #### 
        
        Question #4
    
 Quiz question Body Layer Normalization is different from Batch Normalization because:
 Quiz question Answers * It normalizes the layer output for each example instead of across the batch

* It normalizes the layer output across the batch instead of for each example

* It learns the gamma and beta constants

* It does not need to learn the gamma and beta constants

 Quiz question Tips ## Tasks
### 0. RNN Encoder
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Resources:
* [Attention in RNN](https://intranet.hbtn.io/rltoken/qeb-zGzqaaIUfNEywi2sHg) 

Create a class   ` RNNEncoder `   that inherits from   ` tensorflow.keras.layers.Layer `   to encode for machine translation:
* Class constructor  ` def __init__(self, vocab, embedding, units, batch): ` *  ` vocab `  is an integer representing the size of the input vocabulary
*  ` embedding `  is an integer representing the dimensionality of the embedding vector
*  ` units `  is an integer representing the number of hidden units in the RNN cell
*  ` batch `  is an integer representing the batch size
* Sets the following public instance attributes:*  ` batch `  - the batch size
*  ` units `  - the number of hidden units in the RNN cell
*  ` embedding `  - a  ` keras `  Embedding layer that converts words from the vocabulary into an embedding vector
*  ` gru `  - a  ` keras `  GRU layer with  ` units `  units* Should return both the full sequence of outputs as well as the last hidden state
* Recurrent weights should be initialized with  ` glorot_uniform ` 



* Public instance method  ` def initialize_hidden_state(self): ` * Initializes the hidden states for the RNN cell to a tensor of zeros
* Returns: a tensor of shape  ` (batch, units) ` containing the initialized hidden states

*  Public instance method  ` def call(self, x, initial): ` *  ` x `  is a tensor of shape  ` (batch, input_seq_len) `  containing the input to the encoder layer as word indices within the vocabulary
*  ` initial `  is a tensor of shape  ` (batch, units) `  containing the initial hidden state
*  Returns:  ` outputs, hidden ` *  ` outputs `  is a tensor of shape  ` (batch, input_seq_len, units) ` containing the outputs of the encoder
*  ` hidden `  is a tensor of shape  ` (batch, units) `  containing the last hidden state of the encoder


```bash
$ cat 0-main.py
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
RNNEncoder = __import__('0-rnn_encoder').RNNEncoder

encoder = RNNEncoder(1024, 128, 256, 32)
print(encoder.batch)
print(encoder.units)
print(type(encoder.embedding))
print(type(encoder.gru))

initial = encoder.initialize_hidden_state()
print(initial)
x = tf.convert_to_tensor(np.random.choice(1024, 320).reshape((32, 10)))
outputs, hidden = encoder(x, initial)
print(outputs)
print(hidden)
$ ./0-main.py
32
256
<class 'tensorflow.python.keras.layers.embeddings.Embedding'>
<class 'tensorflow.python.keras.layers.recurrent.GRU'>
Tensor("zeros:0", shape=(32, 256), dtype=float32)
Tensor("rnn_encoder/gru/transpose_1:0", shape=(32, 10, 256), dtype=float32)
Tensor("rnn_encoder/gru/while/Exit_2:0", shape=(32, 256), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 0-rnn_encoder.py ` 
 Self-paced manual review  Panel footer - Controls 
### 1. Self Attention
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Create a class   ` SelfAttention `   that inherits from   ` tensorflow.keras.layers.Layer `   to calculate the attention for machine translation based on  [this paper](https://intranet.hbtn.io/rltoken/YlDIODUFbkYQbRL3a5CwEQ) 
 :
* Class constructor  ` def __init__(self, units): ` *  ` units `  is an integer representing the number of hidden units in the alignment model
*  Sets the following public instance attributes:*  ` W `  - a Dense layer with  ` units `  units, to be applied to the previous decoder hidden state
*  ` U `  - a Dense layer with  ` units `  units, to be applied to the encoder hidden states
*  ` V `  - a Dense layer with  ` 1 `  units, to be applied to the tanh of the sum of the outputs of  ` W `  and  ` U ` 


* Public instance method  ` def call(self, s_prev, hidden_states): ` *  ` s_prev `  is a tensor of shape  ` (batch, units) `  containing the previous decoder hidden state
*  ` hidden_states `  is a tensor of shape  ` (batch, input_seq_len, units) ` containing the outputs of the encoder
* Returns:  ` context, weights ` *  ` context `  is a tensor of shape  ` (batch, units) `  that contains the context vector for the decoder
*  ` weights `  is a tensor of shape  ` (batch, input_seq_len, 1) `  that contains the attention weights


```bash
$ cat 1-main.py
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
SelfAttention = __import__('1-self_attention').SelfAttention

attention = SelfAttention(256)
print(attention.W)
print(attention.U)
print(attention.V)
s_prev = tf.convert_to_tensor(np.random.uniform(size=(32, 256)), preferred_dtype='float32')
hidden_states = tf.convert_to_tensor(np.random.uniform(size=(32, 10, 256)), preferred_dtype='float32')
context, weights = attention(s_prev, hidden_states)
print(context)
print(weights)
$ ./1-main.py
<tensorflow.python.keras.layers.core.Dense object at 0x12309d3c8>
<tensorflow.python.keras.layers.core.Dense object at 0xb28536b38>
<tensorflow.python.keras.layers.core.Dense object at 0xb28536e48>
Tensor("self_attention/Sum:0", shape=(32, 256), dtype=float64)
Tensor("self_attention/transpose_1:0", shape=(32, 10, 1), dtype=float64)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 1-self_attention.py ` 
 Self-paced manual review  Panel footer - Controls 
### 2. RNN Decoder
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Create a class   ` RNNDecoder `   that inherits from   ` tensorflow.keras.layers.Layer `   to decode for machine translation:
* Class constructor  ` def __init__(self, vocab, embedding, units, batch): ` *  ` vocab `  is an integer representing the size of the output vocabulary
*  ` embedding `  is an integer representing the dimensionality of the embedding vector
*  ` units `  is an integer representing the number of hidden units in the RNN cell
*  ` batch `  is an integer representing the batch size
* Sets the following public instance attributes:*  ` embedding `  -  a  ` keras `  Embedding layer that converts words from the vocabulary into an embedding vector
*  ` gru `  - a  ` keras `  GRU layer with  ` units `  units* Should return both the full sequence of outputs as well as the last hidden state
* Recurrent weights should be initialized with  ` glorot_uniform ` 

*  ` F `  - a Dense layer with  ` vocab `  units


* Public instance method  ` def call(self, x, s_prev, hidden_states): ` *  ` x `  is a tensor of shape  ` (batch, 1) `  containing the previous word in the target sequence as an index of the target vocabulary
*  ` s_prev `  is a tensor of shape  ` (batch, units) `  containing the previous decoder hidden state
*  ` hidden_states `  is a tensor of shape  ` (batch, input_seq_len, units) ` containing the outputs of the encoder
* You should use  ` SelfAttention = __import__('1-self_attention').SelfAttention ` 
* You should concatenate the context vector with x in that order
* Returns:  ` y, s ` *  ` y `  is a tensor of shape  ` (batch, vocab) `  containing the output word as a one hot vector in the target vocabulary
*  ` s `  is a tensor of shape  ` (batch, units) `  containing the new decoder hidden state


```bash
$ cat 2-main.py
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
RNNDecoder = __import__('2-rnn_decoder').RNNDecoder

decoder = RNNDecoder(2048, 128, 256, 32)
print(decoder.embedding)
print(decoder.gru)
print(decoder.F)
x = tf.convert_to_tensor(np.random.choice(2048, 32).reshape((32, 1)))
s_prev = tf.convert_to_tensor(np.random.uniform(size=(32, 256)).astype('float32'))
hidden_states = tf.convert_to_tensor(np.random.uniform(size=(32, 10, 256)).astype('float32'))
y, s = decoder(x, s_prev, hidden_states)
print(y)
print(s)
$ ./2-main.py
<tensorflow.python.keras.layers.embeddings.Embedding object at 0x1321113c8>
<tensorflow.python.keras.layers.recurrent.GRU object at 0xb375aab00>
<tensorflow.python.keras.layers.core.Dense object at 0xb375d5128>
Tensor("rnn_decoder/dense/BiasAdd:0", shape=(32, 2048), dtype=float32)
Tensor("rnn_decoder/gru/while/Exit_2:0", shape=(32, 256), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 2-rnn_decoder.py ` 
 Self-paced manual review  Panel footer - Controls 
### 3. Positional Encoding
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Write the function   ` def positional_encoding(max_seq_len, dm): `   that calculates the positional encoding for a transformer:
*  ` max_seq_len `  is an integer representing the maximum sequence length
*  ` dm `  is the model depth
* Returns: a  ` numpy.ndarray `  of shape  ` (max_seq_len, dm) `  containing the positional encoding vectors
* You should use  ` import numpy as np ` 
```bash
$ cat 4-main.py
#!/usr/bin/env python3

import numpy as np
positional_encoding = __import__('4-positional_encoding').positional_encoding

PE = positional_encoding(30, 512)
print(PE.shape)
print(PE)
$ ./4-main.py
(30, 512)
[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00 ...  1.00000000e+00
   0.00000000e+00  1.00000000e+00]
 [ 8.41470985e-01  5.40302306e-01  8.21856190e-01 ...  9.99999994e-01
   1.03663293e-04  9.99999995e-01]
 [ 9.09297427e-01 -4.16146837e-01  9.36414739e-01 ...  9.99999977e-01
   2.07326584e-04  9.99999979e-01]
 ...
 [ 9.56375928e-01 -2.92138809e-01  7.91416314e-01 ...  9.99995791e-01
   2.79890525e-03  9.99996083e-01]
 [ 2.70905788e-01 -9.62605866e-01  9.53248145e-01 ...  9.99995473e-01
   2.90256812e-03  9.99995788e-01]
 [-6.63633884e-01 -7.48057530e-01  2.94705106e-01 ...  9.99995144e-01
   3.00623096e-03  9.99995481e-01]]
$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 4-positional_encoding.py ` 
 Self-paced manual review  Panel footer - Controls 
### 4. Scaled Dot Product Attention
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body  ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/7/8f5aadef511d9f646f5009756035b472073fe896.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220928%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220928T180030Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=5de9c978fe82f34403e7abd63b3354ca07fb3c8edea8ddb4f796083b41bc1f25) 

Write the function   ` def sdp_attention(Q, K, V, mask=None) `   that calculates the scaled dot product attention:
*  ` Q `  is a tensor with its last two dimensions as  ` (..., seq_len_q, dk) `   containing the query matrix
*  ` K `  is a tensor with its last two dimensions as  ` (..., seq_len_v, dk) `   containing the key matrix
*  ` V `  is a tensor with its last two dimensions as  ` (..., seq_len_v, dv) `   containing the value matrix
*  ` mask `  is a tensor that can be broadcast into  ` (..., seq_len_q, seq_len_v) `  containing the optional mask, or defaulted to  ` None ` * if  ` mask `  is not  ` None ` , multiply  ` -1e9 `  to the mask and add it to the scaled matrix multiplication 

* The preceding dimensions of  ` Q ` ,  ` K ` , and  ` V `  are the same
* Returns:  ` output, weights ` *  ` output ` a tensor with its last two dimensions as  ` (..., seq_len_q, dv) `  containing the scaled dot product attention
*  ` weights `  a tensor with its last two dimensions as  ` (..., seq_len_q, seq_len_v) `  containing the attention weights

```bash
$ cat 5-main.py
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
sdp_attention = __import__('5-sdp_attention').sdp_attention

np.random.seed(0)
Q = tf.convert_to_tensor(np.random.uniform(size=(50, 10, 256)).astype('float32'))
K = tf.convert_to_tensor(np.random.uniform(size=(50, 15, 256)).astype('float32'))
V = tf.convert_to_tensor(np.random.uniform(size=(50, 15, 512)).astype('float32'))
output, weights = sdp_attention(Q, K, V)
print(output)
print(weights)
$ ./5-main.py
Tensor("MatMul_1:0", shape=(50, 10, 512), dtype=float32)
Tensor("Softmax:0", shape=(50, 10, 15), dtype=float32)
$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 5-sdp_attention.py ` 
 Self-paced manual review  Panel footer - Controls 
### 5. Multi Head Attention
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body  ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/7/4a5aaa54ebdc32529b4f09a5f22789dc267e0796.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220928%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220928T180030Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=394861579350131dcc21a5fad4e056debe20632e083d343d346083ccdf809ae0) 

Read:
* [Why multi-head self attention works: math, intuitions and 10+1 hidden insights](https://intranet.hbtn.io/rltoken/qcVYSQLK9dekVG99SfVA8Q) 

Create a class   ` MultiHeadAttention `   that inherits from   ` tensorflow.keras.layers.Layer `   to perform multi head attention:
* Class constructor  ` def __init__(self, dm, h): ` *  ` dm `  is an integer representing the dimensionality of the model
*  ` h `  is an integer representing the number of heads
*  ` dm `  is divisible by  ` h ` 
* Sets the following public instance attributes:*  ` h `  - the number of heads
*  ` dm `  - the dimensionality of the model
*  ` depth `  - the depth of each attention head
*  ` Wq `  - a Dense layer with  ` dm `  units, used to generate the query matrix
*  ` Wk `  - a Dense layer with  ` dm `  units, used to generate the key matrix
*  ` Wv `  - a Dense layer with  ` dm `  units, used to generate the value matrix
*  ` linear `  - a Dense layer with  ` dm `  units, used to generate the attention output


* Public instance method  ` def call(self, Q, K, V, mask): ` *  ` Q `  is a tensor of shape  ` (batch, seq_len_q, dk) `  containing the input to generate the query matrix
*  ` K `  is a tensor of shape  ` (batch, seq_len_v, dk) `  containing the input to generate the key matrix
*  ` V `  is a tensor of shape  ` (batch, seq_len_v, dv) `  containing the input to generate the value matrix
*  ` mask `  is always  ` None ` 
* Returns:  ` output, weights ` *  ` output ` a tensor with its last two dimensions as  ` (..., seq_len_q, dm) `  containing the scaled dot product attention
*  ` weights `  a tensor with its last three dimensions as  ` (..., h, seq_len_q, seq_len_v) `  containing the attention weights


* You should use  ` sdp_attention = __import__('5-sdp_attention').sdp_attention ` 
```bash
$ cat 6-main.py
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
MultiHeadAttention = __import__('6-multihead_attention').MultiHeadAttention

mha = MultiHeadAttention(512, 8)
print(mha.dm)
print(mha.h)
print(mha.depth)
print(mha.Wq)
print(mha.Wk)
print(mha.Wv)
print(mha.linear)
Q = tf.convert_to_tensor(np.random.uniform(size=(50, 15, 256)).astype('float32'))
K = tf.convert_to_tensor(np.random.uniform(size=(50, 15, 256)).astype('float32'))
V = tf.convert_to_tensor(np.random.uniform(size=(50, 15, 256)).astype('float32'))
output, weights = mha(Q, K, V, None)
print(output)
print(weights)
$ ./6-main.py
512
8
64
<tensorflow.python.keras.layers.core.Dense object at 0xb2c585b38>
<tensorflow.python.keras.layers.core.Dense object at 0xb2c585e48>
<tensorflow.python.keras.layers.core.Dense object at 0xb2c5b1198>
<tensorflow.python.keras.layers.core.Dense object at 0xb2c5b14a8>
Tensor("multi_head_attention/dense_3/BiasAdd:0", shape=(50, 15, 512), dtype=float32)
Tensor("multi_head_attention/Softmax:0", shape=(50, 8, 15, 15), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 6-multihead_attention.py ` 
 Self-paced manual review  Panel footer - Controls 
### 6. Transformer Encoder Block
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body  ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2020/7/50a5309eae279760a5d6fc6031aa045eafd0e605.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220928%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220928T180030Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f7e3f416f32aba4f265d276e03f83e513de19644d3535047234ba5824b3ffbf3) 

Create a class   ` EncoderBlock `   that inherits from   ` tensorflow.keras.layers.Layer `   to create an encoder block for a transformer:
* Class constructor  ` def __init__(self, dm, h, hidden, drop_rate=0.1): ` *  ` dm `  - the dimensionality of the model
*  ` h `  - the number of heads
*  ` hidden `  - the number of hidden units in the fully connected layer
*  ` drop_rate `  - the dropout rate
* Sets the following public instance attributes:*  ` mha `  - a  ` MultiHeadAttention `  layer
*  ` dense_hidden `  - the hidden dense layer with  ` hidden `  units and  ` relu `  activation
*  ` dense_output `  - the output dense layer with  ` dm `  units
*  ` layernorm1 `  - the first layer norm layer, with  ` epsilon=1e-6 ` 
*  ` layernorm2 `  - the second layer norm layer, with  ` epsilon=1e-6 ` 
*  ` dropout1 `  - the first dropout layer
*  ` dropout2 `  - the second dropout layer


* Public instance method  ` call(self, x, training, mask=None): ` *  ` x `  - a tensor of shape  ` (batch, input_seq_len, dm) ` containing the input to the encoder block
*  ` training `  - a boolean to determine if the model is training
*  ` mask `  - the mask to be applied for multi head attention
* Returns: a tensor of shape  ` (batch, input_seq_len, dm) `  containing the block’s output

* You should use  ` MultiHeadAttention = __import__('6-multihead_attention').MultiHeadAttention ` 
```bash
$ cat 7-main.py
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
EncoderBlock = __import__('7-transformer_encoder_block').EncoderBlock

eblock = EncoderBlock(512, 8, 2048)
print(eblock.mha)
print(eblock.dense_hidden)
print(eblock.dense_output)
print(eblock.layernorm1)
print(eblock.layernorm2)
print(eblock.dropout1)
print(eblock.dropout2)
x = tf.random.uniform((32, 10, 512))
output = eblock(x, True, None)
print(output)
$ ./7-main.py
<6-multihead_attention.MultiHeadAttention object at 0x12c61b390>
<tensorflow.python.keras.layers.core.Dense object at 0xb31ae1860>
<tensorflow.python.keras.layers.core.Dense object at 0xb31ae1b70>
<tensorflow.python.keras.layers.normalization.LayerNormalization object at 0xb31ae1e80>
<tensorflow.python.keras.layers.normalization.LayerNormalization object at 0xb31aea128>
<tensorflow.python.keras.layers.core.Dropout object at 0xb31aea390>
<tensorflow.python.keras.layers.core.Dropout object at 0xb31aea518>
Tensor("encoder_block/layer_normalization_1/batchnorm/add_1:0", shape=(32, 10, 512), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 7-transformer_encoder_block.py ` 
 Self-paced manual review  Panel footer - Controls 
### 7. Transformer Decoder Block
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Create a class   ` DecoderBlock `   that inherits from   ` tensorflow.keras.layers.Layer `   to create an encoder block for a transformer:
* Class constructor  ` def __init__(self, dm, h, hidden, drop_rate=0.1): ` *  ` dm `  - the dimensionality of the model
*  ` h `  - the number of heads
*  ` hidden `  - the number of hidden units in the fully connected layer
*  ` drop_rate `  - the dropout rate
* Sets the following public instance attributes:*  ` mha1 `  - the first  ` MultiHeadAttention `  layer
*  ` mha2 `  - the second  ` MultiHeadAttention `  layer
*  ` dense_hidden `  - the hidden dense layer with  ` hidden `  units and  ` relu `  activation
*  ` dense_output `  - the output dense layer with  ` dm `  units
*  ` layernorm1 `  - the first layer norm layer, with  ` epsilon=1e-6 ` 
*  ` layernorm2 `  - the second layer norm layer, with  ` epsilon=1e-6 ` 
*  ` layernorm3 `  - the third layer norm layer, with  ` epsilon=1e-6 ` 
*  ` dropout1 `  - the first dropout layer
*  ` dropout2 `  - the second dropout layer
*  ` dropout3 `  - the third dropout layer


* Public instance method  ` def call(self, x, encoder_output, training, look_ahead_mask, padding_mask): ` *  ` x `  - a tensor of shape  ` (batch, target_seq_len, dm) ` containing the input to the decoder block
*  ` encoder_output `  - a tensor of shape  ` (batch, input_seq_len, dm) ` containing the output of the encoder
*  ` training `  - a boolean to determine if the model is training
*  ` look_ahead_mask `  - the mask to be applied to the first multi head attention layer
*  ` padding_mask `  - the mask to be applied to the second multi head attention layer
* Returns: a tensor of shape  ` (batch, target_seq_len, dm) `  containing the block’s output

* You should use  ` MultiHeadAttention = __import__('6-multihead_attention').MultiHeadAttention ` 
```bash
$ cat 8-main.py
#!/usr/bin/env python3

import tensorflow as tf
DecoderBlock = __import__('8-transformer_decoder_block').DecoderBlock

dblock = DecoderBlock(512, 8, 2048)
print(dblock.mha1)
print(dblock.mha2)
print(dblock.dense_hidden)
print(dblock.dense_output)
print(dblock.layernorm1)
print(dblock.layernorm2)
print(dblock.layernorm3)
print(dblock.dropout1)
print(dblock.dropout2)
print(dblock.dropout3)
x = tf.random.uniform((32, 15, 512))
hidden_states = tf.random.uniform((32, 10, 512))
output = dblock(x, hidden_states, False, None, None)
print(output)
$ ./8-main.py
<6-multihead_attention.MultiHeadAttention object at 0x1313f4400>
<6-multihead_attention.MultiHeadAttention object at 0xb368bc9b0>
<tensorflow.python.keras.layers.core.Dense object at 0xb368c37b8>
<tensorflow.python.keras.layers.core.Dense object at 0xb368c3ac8>
<tensorflow.python.keras.layers.normalization.LayerNormalization object at 0xb368c3dd8>
<tensorflow.python.keras.layers.normalization.LayerNormalization object at 0xb368cb080>
<tensorflow.python.keras.layers.normalization.LayerNormalization object at 0xb368cb2e8>
<tensorflow.python.keras.layers.core.Dropout object at 0xb368cb550>
<tensorflow.python.keras.layers.core.Dropout object at 0xb368cb6d8>
<tensorflow.python.keras.layers.core.Dropout object at 0xb368cb828>
Tensor("decoder_block/layer_normalization_2/batchnorm/add_1:0", shape=(32, 15, 512), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 8-transformer_decoder_block.py ` 
 Self-paced manual review  Panel footer - Controls 
### 8. Transformer Encoder
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Create a class   ` Encoder `   that inherits from   ` tensorflow.keras.layers.Layer `   to create the encoder for a transformer:
* Class constructor  ` def __init__(self, N, dm, h, hidden, input_vocab, max_seq_len, drop_rate=0.1): ` *  ` N `  - the number of blocks in the encoder
*  ` dm `  - the dimensionality of the model
*  ` h `  - the number of heads
*  ` hidden `  - the number of hidden units in the fully connected layer
*  ` input_vocab `  - the size of the input vocabulary
*  ` max_seq_len `  - the maximum sequence length possible
*  ` drop_rate `  - the dropout rate
* Sets the following public instance attributes:*  ` N `  - the number of blocks in the encoder
*  ` dm `  - the dimensionality of the model
*  ` embedding `  - the embedding layer for the inputs
*  ` positional_encoding `  - a  ` numpy.ndarray `  of shape  ` (max_seq_len, dm) `  containing the positional encodings
*  ` blocks `  - a list of length  ` N `  containing all of the  ` EncoderBlock ` ‘s
*  ` dropout `  - the dropout layer, to be applied to the positional encodings


* Public instance method  ` call(self, x, training, mask): ` *  ` x `  - a tensor of shape  ` (batch, input_seq_len, dm) ` containing the input to the encoder
*  ` training `  - a boolean to determine if the model is training
*  ` mask `  - the mask to be applied for multi head attention
* Returns: a tensor of shape  ` (batch, input_seq_len, dm) `  containing the encoder output

* You should use  ` positional_encoding = __import__('4-positional_encoding').positional_encoding `  and  ` EncoderBlock = __import__('7-transformer_encoder_block').EncoderBlock ` 
```bash
$ cat 9-main.py
#!/usr/bin/env python3

import tensorflow as tf
Encoder = __import__('9-transformer_encoder').Encoder

encoder = Encoder(6, 512, 8, 2048, 10000, 1000)
print(encoder.dm)
print(encoder.N)
print(encoder.embedding)
print(encoder.positional_encoding)
print(encoder.blocks)
print(encoder.dropout)
x = tf.random.uniform((32, 10))
output = encoder(x, True, None)
print(output)
$ ./9-main.py
512
6
<tensorflow.python.keras.layers.embeddings.Embedding object at 0xb2981acc0>
[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00 ...  1.00000000e+00
   0.00000000e+00  1.00000000e+00]
 [ 8.41470985e-01  5.40302306e-01  8.21856190e-01 ...  9.99999994e-01
   1.03663293e-04  9.99999995e-01]
 [ 9.09297427e-01 -4.16146837e-01  9.36414739e-01 ...  9.99999977e-01
   2.07326584e-04  9.99999979e-01]
 ...
 [-8.97967480e-01 -4.40061818e-01  4.26195541e-01 ...  9.94266169e-01
   1.03168405e-01  9.94663903e-01]
 [-8.55473152e-01  5.17847165e-01  9.86278111e-01 ...  9.94254673e-01
   1.03271514e-01  9.94653203e-01]
 [-2.64607527e-02  9.99649853e-01  6.97559894e-01 ...  9.94243164e-01
   1.03374623e-01  9.94642492e-01]]
ListWrapper([<7-transformer_encoder_block.EncoderBlock object at 0xb2981aef0>, <7-transformer_encoder_block.EncoderBlock object at 0xb29850ba8>, <7-transformer_encoder_block.EncoderBlock object at 0xb298647b8>, <7-transformer_encoder_block.EncoderBlock object at 0xb29e502e8>, <7-transformer_encoder_block.EncoderBlock object at 0xb29e5add8>, <7-transformer_encoder_block.EncoderBlock object at 0xb29e6c908>])
<tensorflow.python.keras.layers.core.Dropout object at 0xb29e7c470>
Tensor("encoder/encoder_block_5/layer_normalization_11/batchnorm/add_1:0", shape=(32, 10, 512), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 9-transformer_encoder.py ` 
 Self-paced manual review  Panel footer - Controls 
### 9. Transformer Decoder
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Create a class   ` Decoder `   that inherits from   ` tensorflow.keras.layers.Layer `   to create the decoder for a transformer:
* Class constructor  ` def __init__(self, N, dm, h, hidden, target_vocab, max_seq_len, drop_rate=0.1): ` 
*  ` N `  - the number of blocks in the encoder*  ` dm `  - the dimensionality of the model
*  ` h `  - the number of heads
*  ` hidden `  - the number of hidden units in the fully connected layer
*  ` target_vocab `  - the size of the target vocabulary
*  ` max_seq_len `  - the maximum sequence length possible
*  ` drop_rate `  - the dropout rate
* Sets the following public instance attributes:*  ` N `  - the number of blocks in the encoder
*  ` dm `  - the dimensionality of the model
*  ` embedding `  - the embedding layer for the targets
*  ` positional_encoding `  - a  ` numpy.ndarray `  of shape  ` (max_seq_len, dm) `  containing the positional encodings
*  ` blocks `  - a list of length  ` N `  containing all of the  ` DecoderBlock ` ‘s
*  ` dropout `  - the dropout layer, to be applied to the positional encodings


* Public instance method  ` def call(self, x, encoder_output, training, look_ahead_mask, padding_mask): ` *  ` x `  - a tensor of shape  ` (batch, target_seq_len, dm) ` containing the input to the decoder
*  ` encoder_output `  - a tensor of shape  ` (batch, input_seq_len, dm) ` containing the output of the encoder
*  ` training `  - a boolean to determine if the model is training
*  ` look_ahead_mask `  - the mask to be applied to the first multi head attention layer
*  ` padding_mask `  - the mask to be applied to the second multi head attention layer
* Returns: a tensor of shape  ` (batch, target_seq_len, dm) `  containing the decoder output

* You should use  ` positional_encoding = __import__('4-positional_encoding').positional_encoding `  and  ` DecoderBlock = __import__('8-transformer_decoder_block').DecoderBlock ` 
```bash
$ cat 10-main.py
#!/usr/bin/env python3

import tensorflow as tf
Decoder = __import__('10-transformer_decoder').Decoder

decoder = Decoder(6, 512, 8, 2048, 12000, 1500)
print(decoder.dm)
print(decoder.N)
print(decoder.embedding)
print(decoder.positional_encoding)
print(decoder.blocks)
print(decoder.dropout)
x = tf.random.uniform((32, 15))
hidden_states = tf.random.uniform((32, 10, 512))
output = decoder(x, hidden_states, True, None, None)
print(output)
$ ./10-main.py
512
6
<tensorflow.python.keras.layers.embeddings.Embedding object at 0xb2cdede48>
[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00 ...  1.00000000e+00
   0.00000000e+00  1.00000000e+00]
 [ 8.41470985e-01  5.40302306e-01  8.21856190e-01 ...  9.99999994e-01
   1.03663293e-04  9.99999995e-01]
 [ 9.09297427e-01 -4.16146837e-01  9.36414739e-01 ...  9.99999977e-01
   2.07326584e-04  9.99999979e-01]
 ...
 [ 9.99516416e-01 -3.10955511e-02 -8.59441209e-01 ...  9.87088496e-01
   1.54561841e-01  9.87983116e-01]
 [ 5.13875021e-01 -8.57865061e-01 -6.94580536e-02 ...  9.87071278e-01
   1.54664258e-01  9.87967088e-01]
 [-4.44220699e-01 -8.95917390e-01  7.80301396e-01 ...  9.87054048e-01
   1.54766673e-01  9.87951050e-01]]
ListWrapper([<8-transformer_decoder_block.DecoderBlock object at 0xb2ce0f0b8>, <8-transformer_decoder_block.DecoderBlock object at 0xb2ce29ef0>, <8-transformer_decoder_block.DecoderBlock object at 0xb2d711b00>, <8-transformer_decoder_block.DecoderBlock object at 0xb2d72c710>, <8-transformer_decoder_block.DecoderBlock object at 0xb2d744320>, <8-transformer_decoder_block.DecoderBlock object at 0xb2d755ef0>])
<tensorflow.python.keras.layers.core.Dropout object at 0xb2d76db38>
Tensor("decoder/decoder_block_5/layer_normalization_17/batchnorm/add_1:0", shape=(32, 15, 512), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 10-transformer_decoder.py ` 
 Self-paced manual review  Panel footer - Controls 
### 10. Transformer Network
          mandatory         Progress vs Score           Score: 0.00% (Checks completed: 0.00%)         Task Body Create a class   ` Transformer `   that inherits from   ` tensorflow.keras.Model `   to create a transformer network:
* Class constructor ```bash
def __init__(self, N, dm, h, hidden, input_vocab, target_vocab, max_seq_input, max_seq_target, drop_rate=0.1):
```
*  ` N `  - the number of blocks in the encoder and decoder
*  ` dm `  - the dimensionality of the model
*  ` h `  - the number of heads
*  ` hidden `  - the number of hidden units in the fully connected layers
*  ` input_vocab `  - the size of the input vocabulary
*  ` target_vocab `  - the size of the target vocabulary
*  ` max_seq_input `  - the maximum sequence length possible for the input
*  ` max_seq_target `  - the maximum sequence length possible for the target
*  ` drop_rate `  - the dropout rate
* Sets the following public instance attributes:*  ` encoder `  - the encoder layer
*  ` decoder `  - the decoder layer
*  ` linear `  - a final Dense layer with  ` target_vocab `  units


* Public instance method  ` def call(self, inputs, target, training, encoder_mask, look_ahead_mask, decoder_mask): ` *  ` inputs `  - a tensor of shape  ` (batch, input_seq_len) ` containing the inputs
*  ` target `  - a tensor of shape  ` (batch, target_seq_len) ` containing the target
*  ` training `  - a boolean to determine if the model is training
*  ` encoder_mask `  - the padding mask to be applied to the encoder
*  ` look_ahead_mask `  - the look ahead mask to be applied to the decoder
*  ` decoder_mask `  - the padding mask to be applied to the decoder
* Returns: a tensor of shape  ` (batch, target_seq_len, target_vocab) `  containing the transformer output

* You should use  ` Encoder = __import__('9-transformer_encoder').Encoder `  and  ` Decoder = __import__('10-transformer_decoder').Decoder ` 
```bash
$ cat 11-main.py
#!/usr/bin/env python3

import tensorflow as tf
Transformer = __import__('11-transformer').Transformer

transformer = Transformer(6, 512, 8, 2048, 10000, 12000, 1000, 1500)
print(transformer.encoder)
print(transformer.decoder)
print(transformer.linear)
x = tf.random.uniform((32, 10))
y = tf.random.uniform((32, 15))
output = transformer(x, y, True, None, None, None)
print(output)
$ ./11-main.py
<9-transformer_encoder.Encoder object at 0xb2edc5128>
<10-transformer_decoder.Decoder object at 0xb2f412b38>
<tensorflow.python.keras.layers.core.Dense object at 0xb2fd68898>
Tensor("transformer/dense_96/BiasAdd:0", shape=(32, 15, 12000), dtype=float32)
$

```
Ignore the Warning messages in the output
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x11-attention ` 
* File:  ` 11-transformer.py ` 
 Self-paced manual review  Panel footer - Controls 
