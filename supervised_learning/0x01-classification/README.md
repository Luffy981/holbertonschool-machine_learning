# 0x01. Classification
## Details
      By Alexa Orrico, Software Engineer at Holberton School          Weight: 3                Ongoing project - started May 2, 2022 , must end by May 11, 2022           - you're done with 0% of tasks.              Checker was released at May 6, 2022 12:00 PMManual QA review must be done          (request it when you are done with the project)              An auto review will be launched at the deadline       ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/83672a47323d70a88c5e.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ff863cd140e91b0f80deb727bf2f180d6d05583e54006c92ba23c207db903105) 

## Background Context
At the end of this project, you should be able to build your own binary image classifier from scratch using   ` numpy `  . 
Good luck and have fun!
## Resources (same as previous project)
Read or watch :
* [Supervised vs. Unsupervised Machine Learning](https://intranet.hbtn.io/rltoken/16mL_gwlZqRa3NAv325YiQ) 

* [How would you explain neural networks to someone who knows very little about AI or neurology?](https://intranet.hbtn.io/rltoken/A1B1qLJgpKvA4RySpI6r7g) 

* [Using Neural Nets to Recognize Handwritten Digits](https://intranet.hbtn.io/rltoken/EjjENEVXJKiAZsqSqWMl-w) 
 (until “A simple network to classify handwritten digits” (excluded))
* [Forward propagation](https://intranet.hbtn.io/rltoken/da0lo6JbjEDxCbffjRHc7g) 

* [Understanding Activation Functions in Neural Networks](https://intranet.hbtn.io/rltoken/hz77ChKoiSjMFzi7mgMzBA) 

* [Loss function](https://intranet.hbtn.io/rltoken/KgRV0-l2LBdQciUXGIeCqQ) 

* [Gradient descent](https://intranet.hbtn.io/rltoken/7iSJelYELwy7C8cCsGk5hw) 

* [Calculus on Computational Graphs: Backpropagation](https://intranet.hbtn.io/rltoken/BONZS65eZnIMjngFhr7dPA) 

* [Backpropagation calculus](https://intranet.hbtn.io/rltoken/Arpa6EFk9q5gD9aJ4Wl5qA) 

* [What is a Neural Network?](https://intranet.hbtn.io/rltoken/EkncpxTwCUJztJsYI0wciA) 

* [Supervised Learning with a Neural Network](https://intranet.hbtn.io/rltoken/LoWxJZN-JA0VkV13QQrw1g) 

* [Binary Classification](https://intranet.hbtn.io/rltoken/cFuQ0hUHg_SpVCHvKrXMzg) 

* [Logistic Regression](https://intranet.hbtn.io/rltoken/sGIlY030fFNX4nNQidq5fw) 

* [Logistic Regression Cost Function](https://intranet.hbtn.io/rltoken/xZTVYTU5pSnSK3a7o3OuIQ) 

* [Gradient Descent](https://intranet.hbtn.io/rltoken/M3YbEr_BqYcILJNz7YzLaQ) 

* [Computation Graph](https://intranet.hbtn.io/rltoken/X5CelY1ajZt3wHrjtls6Fg) 

* [Logistic Regression Gradient Descent](https://intranet.hbtn.io/rltoken/HLxYo6tgVumVNRysPUxKNA) 

* [Vectorization](https://intranet.hbtn.io/rltoken/Zxdbe_-GWZRfXKfs5JM9ig) 

* [Vectorizing Logistic Regression](https://intranet.hbtn.io/rltoken/HQ9VuO9c4XPJgIm6Nxyn5Q) 

* [Vectorizing Logistic Regression’s Gradient Computation](https://intranet.hbtn.io/rltoken/RaswXJ2G9LHswV0CypjA0A) 

* [A Note on Python/Numpy Vectors](https://intranet.hbtn.io/rltoken/wKRb7J-yeA92EF5aEJd3oA) 

* [Neural Network Representations](https://intranet.hbtn.io/rltoken/JyhRr98YlhACYERu0GncNQ) 

* [Computing Neural Network Output](https://intranet.hbtn.io/rltoken/Lpcj3uH_6hh8Fp1dXzKkCw) 

* [Vectorizing Across Multiple Examples](https://intranet.hbtn.io/rltoken/uWY4JFKkT58mrHSBig_f5A) 

* [Gradient Descent For Neural Networks](https://intranet.hbtn.io/rltoken/Q583jTfE8BfU5hPW1xlDiQ) 

* [Random Initialization](https://intranet.hbtn.io/rltoken/2uZWU7WaWSQfwGNKlcgZig) 

* [Deep L-Layer Neural Network](https://intranet.hbtn.io/rltoken/iMyExMqGZGcawyK51FcdzQ) 

* [Train/Dev/Test Sets](https://intranet.hbtn.io/rltoken/varxWT03Dy39WlyrZBGAVQ) 

* [Random Initialization For Neural Networks : A Thing Of The Past](https://intranet.hbtn.io/rltoken/HE-b5ClTF0pfEr0wbqqRcQ) 

* [Initialization of deep networks](https://intranet.hbtn.io/rltoken/ymVrn0IFwxnzoo3WwEZzcQ) 

* [Multiclass classification](https://intranet.hbtn.io/rltoken/F5ut0f8cYuUFY2IYBmQzww) 

* [Derivation: Derivatives for Common Neural Network Activation Functions](https://intranet.hbtn.io/rltoken/5Ao7YcSy4GlRPELdsfbVQg) 

* [What is One Hot Encoding? Why And When do you have to use it?](https://intranet.hbtn.io/rltoken/KziHGcOJPBNCkvAhp6SlUw) 

* [Softmax function](https://intranet.hbtn.io/rltoken/WvCQOcguT01A7ufgxH9Tlw) 

* [What is the intuition behind SoftMax function?](https://intranet.hbtn.io/rltoken/21nEj2eMnM3-Hp5j-WMmLg) 

* [Cross entropy](https://intranet.hbtn.io/rltoken/cyzEi2UxcP9w7JzKVU6LjA) 

* [Loss Functions: Cross-Entropy](https://intranet.hbtn.io/rltoken/BYpstsQ1_MbbWhbG8h7FMg) 

* [Softmax Regression](https://intranet.hbtn.io/rltoken/q95bk22oFJJXRdbbgjnBHg) 
 (Note: I suggest watching this video at 1.5x - 2x speed)
* [Training Softmax Classifier](https://intranet.hbtn.io/rltoken/Tbv6I5BYqKJPXlKboc_n2w) 
 (Note: I suggest watching this video at 1.5x - 2x speed)
* [numpy.zeros](https://intranet.hbtn.io/rltoken/Ho3q2XEingriAApbvjTy6w) 

* [numpy.random.randn](https://intranet.hbtn.io/rltoken/JFRl_j_ebMhXasgQ_geUug) 

* [numpy.exp](https://intranet.hbtn.io/rltoken/AjRCO7Yh0JW4lC0xuGmbOg) 

* [numpy.log](https://intranet.hbtn.io/rltoken/AtD0YS-8Q3Oc0TN1MQ6Wuw) 

* [numpy.sqrt](https://intranet.hbtn.io/rltoken/Xb26cP7K6Bqtoc_NhQXosg) 

* [numpy.where](https://intranet.hbtn.io/rltoken/6owcKbc6MWk7JlnlM0AaWA) 

* [numpy.max](https://intranet.hbtn.io/rltoken/XnCzzo75KyDx6l4zweUuYQ) 

* [numpy.sum](https://intranet.hbtn.io/rltoken/8VwoA9sM5hdWXZHSSjc73w) 

* [numpy.argmax](https://intranet.hbtn.io/rltoken/SzHZoLQXMe15-4-Lk92sQA) 

* [What is Pickle in python?](https://intranet.hbtn.io/rltoken/5cQhUAvtKB9AaBzVsDTOAA) 

* [pickle](https://intranet.hbtn.io/rltoken/uDynwTm7iC058ggs7HAOrQ) 

* [pickle.dump](https://intranet.hbtn.io/rltoken/W1gfWgy0dv32-M9mZnl6ww) 

* [pickle.load](https://intranet.hbtn.io/rltoken/CrTd_iMHnfImuVxYJQdUYA) 

Optional :
* [Predictive analytics](https://intranet.hbtn.io/rltoken/Bp_21TFndH5cSWLBObZuzQ) 

* [Maximum Likelihood Estimation](https://intranet.hbtn.io/rltoken/ZI9NXBApyS_bmk87ZxXerA) 

## Learning Objectives
At the end of this project, you are expected to be able to  [explain to anyone](https://intranet.hbtn.io/rltoken/JwVPSpHQajVf_SxzkR4LaQ) 
 ,  without the help of Google :
### General
* What is a model?
* What is supervised learning?
* What is a prediction?
* What is a node?
* What is a weight?
* What is a bias?
* What are activation functions?* Sigmoid?
* Tanh?
* Relu?
* Softmax?

* What is a layer?
* What is a hidden layer?
* What is Logistic Regression?
* What is a loss function?
* What is a cost function?
* What is forward propagation?
* What is Gradient Descent?
* What is back propagation?
* What is a Computation Graph?
* How to initialize weights/biases
* The importance of vectorization
* How to split up your data
* What is multiclass classification?
* What is a one-hot vector?
* How to encode/decode one-hot vectors
* What is the softmax function and when do you use it?
* What is cross-entropy loss?
* What is pickling in Python?
## Requirements
### General
* Allowed editors:  ` vi ` ,  ` vim ` ,  ` emacs ` 
* All your files will be interpreted/compiled on Ubuntu 16.04 LTS using  ` python3 `  (version 3.5)
* Your files will be executed with  ` numpy `  (version 1.15)
* All your files should end with a new line
* The first line of all your files should be exactly  ` #!/usr/bin/env python3 ` 
* A  ` README.md `  file, at the root of the folder of the project, is mandatory
* Your code should use the  ` pycodestyle `  style (version 2.4)
* All your modules should have documentation ( ` python3 -c 'print(__import__("my_module").__doc__)' ` )
* All your classes should have documentation ( ` python3 -c 'print(__import__("my_module").MyClass.__doc__)' ` )
* All your functions (inside and outside a class) should have documentation ( ` python3 -c 'print(__import__("my_module").my_function.__doc__)' `  and  ` python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)' ` )
* Unless otherwise noted, you are not allowed to import any module except  ` import numpy as np ` 
* Unless otherwise noted, you are not allowed to use any loops ( ` for ` ,  ` while ` , etc.)
* All your files must be executable
* The length of your files will be tested using  ` wc ` 
## More Info
### Matrix Multiplications
For all matrix multiplications in the following tasks, please use  [numpy.matmul](https://intranet.hbtn.io/rltoken/Ox8bY8ogmUftzjR96IrMDw) 

### Testing your code
In order to test your code, you’ll need DATA! Please download these datasets ( [Binary_Train.npz](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/Binary_Train.npz) 
 ,  [Binary_Dev.npz](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/Binary_Dev.npz) 
 ,  [MNIST.npz](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/MNIST.npz) 
 ) to go along with all of the following main files. You  do not  need to upload these files to GitHub. Your code will not necessarily be tested with these datasets. All of the following code assumes that you have stored all of your datasets in a separate   ` data `   directory.
```bash
alexa@ubuntu-xenial:$ cat show_data.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']

fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_3D[i])
    plt.title(Y[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./show_data.py
alexa@ubuntu-xenial:$
alexa@ubuntu-xenial:$ cat show_multi_data.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

lib = np.load('../data/MNIST.npz')
print(lib.files)
X_train_3D = lib['X_train']
Y_train = lib['Y_train']

fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_train_3D[i])
    plt.title(str(Y_train[i]))
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./show_multi_data.py
['Y_test', 'X_test', 'X_train', 'Y_train', 'X_valid', 'Y_valid']

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/9bf500103f10b830474e.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=7eebf0eaed0c076f8aed56e59399ca26b4f5977d9398f188e7b3d1b0522306b2) 

## Tasks
### 0. Neuron
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification:
* class constructor:  ` def __init__(self, nx): ` *  ` nx `  is the number of input features to the neuron* If  ` nx `  is not an integer, raise a  ` TypeError `  with the exception:  ` nx must be an integer ` 
* If  ` nx `  is less than 1, raise a  ` ValueError `  with the exception:  ` nx must be a positive integer ` 

* All exceptions should be raised in the order listed above

* Public instance attributes:*  ` W ` : The weights vector for the neuron. Upon instantiation, it should be initialized using a random normal distribution.
*  ` b ` : The bias for the neuron. Upon instantiation, it should be initialized to 0.
*  ` A ` : The activated output of the neuron (prediction). Upon instantiation, it should be initialized to 0.

```bash
alexa@ubuntu-xenial:$ cat 0-main.py
#!/usr/bin/env python3

import numpy as np

Neuron = __import__('0-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X.shape[0])
print(neuron.W)
print(neuron.W.shape)
print(neuron.b)
print(neuron.A)
neuron.A = 10
print(neuron.A)
alexa@ubuntu-xenial:$ ./0-main.py
[[ 1.76405235e+00  4.00157208e-01  9.78737984e-01  2.24089320e+00
   1.86755799e+00 -9.77277880e-01  9.50088418e-01 -1.51357208e-01

...

  -5.85865511e-02 -3.17543094e-01 -1.63242330e+00 -6.71341546e-02
   1.48935596e+00  5.21303748e-01  6.11927193e-01 -1.34149673e+00]]
(1, 784)
0
0
10
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 0-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 1. Privatize Neuron
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 0-neuron.py `  ):
* class constructor:  ` def __init__(self, nx): ` *  ` nx `  is the number of input features to the neuron* If  ` nx `  is not an integer, raise a  ` TypeError `  with the exception:  ` nx must be a integer ` 
* If  ` nx `  is less than 1, raise a  ` ValueError `  with the exception:  ` nx must be positive ` 

* All exceptions should be raised in the order listed above

* Private instance attributes:*  ` __W ` : The weights vector for the neuron. Upon instantiation, it should be initialized using a random normal distribution.
*  ` __b ` : The bias for the neuron. Upon instantiation, it should be initialized to 0.
*  ` __A ` : The activated output of the neuron (prediction). Upon instantiation, it should be initialized to 0.
* Each private attribute should have a corresponding getter function (no setter function).

```bash
alexa@ubuntu-xenial:$ cat 1-main.py
#!/usr/bin/env python3

import numpy as np

Neuron = __import__('1-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X.shape[0])
print(neuron.W)
print(neuron.b)
print(neuron.A)
neuron.A = 10
print(neuron.A)
alexa@ubuntu-xenial:$ ./1-main.py
[[ 1.76405235e+00  4.00157208e-01  9.78737984e-01  2.24089320e+00
   1.86755799e+00 -9.77277880e-01  9.50088418e-01 -1.51357208e-01

...

  -5.85865511e-02 -3.17543094e-01 -1.63242330e+00 -6.71341546e-02
   1.48935596e+00  5.21303748e-01  6.11927193e-01 -1.34149673e+00]]
0
0
Traceback (most recent call last):
  File "./1-main.py", line 16, in <module>
    neuron.A = 10
AttributeError: can't set attribute
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 1-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 2. Neuron Forward Propagation
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 1-neuron.py `  ):
* Add the public method  ` def forward_prop(self, X): ` * Calculates the forward propagation of the neuron
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

* Updates the private attribute  ` __A ` 
* The neuron should use a sigmoid activation function
* Returns the private attribute  ` __A ` 

```bash
alexa@ubuntu-xenial:$ cat 2-main.py
#!/usr/bin/env python3

import numpy as np

Neuron = __import__('2-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X.shape[0])
neuron._Neuron__b = 1
A = neuron.forward_prop(X)
if (A is neuron.A):
        print(A)
vagrant@ubuntu-xe
alexa@ubuntu-xenial:$ ./2-main.py
[[5.34775247e-10 7.24627778e-04 4.52416436e-07 ... 8.75691930e-05
  1.13141966e-06 6.55799932e-01]]
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 2-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 3. Neuron Cost
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 2-neuron.py `  ):
* Add the public method  ` def cost(self, Y, A): ` * Calculates the cost of the model using logistic regression
*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` A `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the activated output of the neuron for each example
* To avoid division by zero errors, please use  ` 1.0000001 - A `  instead of  ` 1 - A ` 
* Returns the cost

```bash
alexa@ubuntu-xenial:$ cat 3-main.py
#!/usr/bin/env python3

import numpy as np

Neuron = __import__('3-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X.shape[0])
A = neuron.forward_prop(X)
cost = neuron.cost(Y, A)
print(cost)
alexa@ubuntu-xenial:$ ./3-main.py
4.365104944262272
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 3-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 4. Evaluate Neuron
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 3-neuron.py `  ):
* Add the public method  ` def evaluate(self, X, Y): ` * Evaluates the neuron’s predictions
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
* Returns the neuron’s prediction and the cost of the network, respectively* The prediction should be a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the predicted labels for each example
* The label values should be 1 if the output of the network is >= 0.5 and 0 otherwise


```bash
alexa@ubuntu-xenial:$ cat 4-main.py
#!/usr/bin/env python3

import numpy as np

Neuron = __import__('4-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X.shape[0])
A, cost = neuron.evaluate(X, Y)
print(A)
print(cost)
alexa@ubuntu-xenial:$ ./4-main.py
[[0 0 0 ... 0 0 0]]
4.365104944262272
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 4-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 5. Neuron Gradient Descent
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 4-neuron.py `  ):
* Add the public method  ` def gradient_descent(self, X, Y, A, alpha=0.05): ` * Calculates one pass of gradient descent on the neuron
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` A `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the activated output of the neuron for each example
*  ` alpha `  is the learning rate
* Updates the private attributes  ` __W `  and  ` __b ` 

```bash
alexa@ubuntu-xenial:$ cat 5-main.py
#!/usr/bin/env python3

import numpy as np

Neuron = __import__('5-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X.shape[0])
A = neuron.forward_prop(X)
neuron.gradient_descent(X, Y, A, 0.5)
print(neuron.W)
print(neuron.b)
alexa@ubuntu-xenial:$ ./5-main.py
[[ 1.76405235e+00  4.00157208e-01  9.78737984e-01  2.24089320e+00
   1.86755799e+00 -9.77277880e-01  9.50088418e-01 -1.51357208e-01

...

  -5.85865511e-02 -3.17543094e-01 -1.63242330e+00 -6.71341546e-02
   1.48935596e+00  5.21303748e-01  6.11927193e-01 -1.34149673e+00]]
0.2579495783615682
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 5-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 6. Train Neuron
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 5-neuron.py `  ):
* Add the public method  ` def train(self, X, Y, iterations=5000, alpha=0.05): ` * Trains the neuron
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` iterations `  is the number of iterations to train over* if  ` iterations `  is not an integer, raise a  ` TypeError `  with the exception  ` iterations must be an integer ` 
* if  ` iterations `  is not positive, raise a  ` ValueError `  with the exception  ` iterations must be a positive integer ` 

*  ` alpha `  is the learning rate* if  ` alpha `  is not a float, raise a  ` TypeError `  with the exception  ` alpha must be a float ` 
* if  ` alpha `  is not positive, raise a  ` ValueError `  with the exception  ` alpha must be positive ` 

* All exceptions should be raised in the order listed above
* Updates the private attributes  ` __W ` ,  ` __b ` , and  ` __A ` 
* You are allowed to use one loop
* Returns the evaluation of the training data after  ` iterations `  of training have occurred

```bash
alexa@ubuntu-xenial:$ cat 6-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

Neuron = __import__('6-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
lib_dev = np.load('../data/Binary_Dev.npz')
X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']
X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X_train.shape[0])
A, cost = neuron.train(X_train, Y_train, iterations=10)
accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100
print("Train cost:", np.round(cost, decimals=10))
print("Train accuracy: {}%".format(np.round(accuracy, decimals=10)))
print("Train data:", np.round(A, decimals=10))
print("Train Neuron A:", np.round(neuron.A, decimals=10))

A, cost = neuron.evaluate(X_dev, Y_dev)
accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100
print("Dev cost:", np.round(cost, decimals=10))
print("Dev accuracy: {}%".format(np.round(accuracy, decimals=10)))
print("Dev data:", np.round(A, decimals=10))
print("Dev Neuron A:", np.round(neuron.A, decimals=10))

fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_dev_3D[i])
    plt.title(A[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()

alexa@ubuntu-xenial:$ ./6-main.py
Train cost: 1.3805076999
Train accuracy: 64.737465456%
Train data: [[0 0 0 ... 0 0 1]]
Train Neuron A: [[2.70000000e-08 2.18229559e-01 1.63492900e-04 ... 4.66530830e-03
  6.06518000e-05 9.73817942e-01]]
Dev cost: 1.4096194345
Dev accuracy: 64.4917257683%
Dev data: [[1 0 0 ... 0 0 1]]
Dev Neuron A: [[0.85021134 0.         0.3526692  ... 0.10140937 0.         0.99555018]]

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/7251476d2c799bd66b1b.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ace64e05b7fa21d2601c6a8b1e653907f696ebc9a042b9ff25f552cae8281133) 

Not that great… Let’s get more data!
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 6-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 7. Upgrade Train Neuron
          mandatory         Progress vs Score  Task Body Write a class   ` Neuron `   that defines a single neuron performing binary classification (Based on   ` 6-neuron.py `  ):
* Update the public method  ` train `  to  ` def train(self, X, Y, iterations=5000, alpha=0.05, verbose=True, graph=True, step=100): ` * Trains the neuron by updating the private attributes  ` __W ` ,  ` __b ` , and  ` __A ` 
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` iterations `  is the number of iterations to train over* if  ` iterations `  is not an integer, raise a  ` TypeError `  with the exception  ` iterations must be an integer ` 
* if  ` iterations `  is not positive, raise a  ` ValueError `  with the exception  ` iterations must be a positive integer ` 

*  ` alpha `  is the learning rate* if  ` alpha `  is not a float, raise a  ` TypeError `  with the exception  ` alpha must be a float ` 
* if  ` alpha `  is not positive, raise a  ` ValueError `  with the exception  ` alpha must be positive ` 

*  ` verbose `  is a boolean that defines whether or not to print information about the training. If  ` True ` , print  ` Cost after {iteration} iterations: {cost} `  every  ` step `  iterations:*  Include data from the 0th and last iteration

*  ` graph `  is a boolean that defines whether or not to graph information about the training once the training has completed. If  ` True ` :*  Plot the training data every  ` step `  iterations as a blue line
*  Label the x-axis as  ` iteration ` 
*  Label the y-axis as  ` cost ` 
*  Title the plot  ` Training Cost ` 
*  Include data from the 0th and last iteration

*  Only if either  ` verbose `  or  ` graph `  are  ` True ` :* if  ` step `  is not an integer, raise a  ` TypeError `  with the exception  ` step must be an integer ` 
* if  ` step `  is not positive or is greater than  ` iterations ` , raise a  ` ValueError `  with the exception  ` step must be positive and <= iterations ` 

* All exceptions should be raised in the order listed above
*  The 0th iteration should represent the state of the neuron before any training has occurred
*  You are allowed to use one loop
*  You can use  ` import matplotlib.pyplot as plt ` 
* Returns: the evaluation of the training data after  ` iterations `  of training have occurred

```bash
alexa@ubuntu-xenial:$ cat 7-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

Neuron = __import__('7-neuron').Neuron

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
lib_dev = np.load('../data/Binary_Dev.npz')
X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']
X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T

np.random.seed(0)
neuron = Neuron(X_train.shape[0])
A, cost = neuron.train(X_train, Y_train, iterations=3000)
accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100
print("Train cost:", cost)
print("Train accuracy: {}%".format(accuracy))
A, cost = neuron.evaluate(X_dev, Y_dev)
accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100
print("Dev cost:", cost)
print("Dev accuracy: {}%".format(accuracy))
fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_dev_3D[i])
    plt.title(A[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./7-main.py
Cost after 0 iterations: 4.365104944262272
Cost after 100 iterations: 0.11955134491351888

...

Cost after 3000 iterations: 0.013386353289868338

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/23a1fc5f90d79c63bd78.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=087c1bfaa0ee6b888b40defe30c26e831cc7e5ff51513c60a3b42ab8e118bac6) 

```bash
Train cost: 0.013386353289868338
Train accuracy: 99.66837741808132%
Dev cost: 0.010803484515167197
Dev accuracy: 99.81087470449172%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/6ffa61a3b13fb602f400.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=bf3bd9fbefd68c43be98a47530d34caceda91153932258c00e0f39e51c36c48d) 

 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 7-neuron.py ` 
 Self-paced manual review  Panel footer - Controls 
### 8. NeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification:
* class constructor:  ` def __init__(self, nx, nodes): ` *  ` nx `  is the number of input features* If  ` nx `  is not an integer, raise a  ` TypeError `  with the exception:  ` nx must be an integer ` 
* If  ` nx `  is less than 1, raise a  ` ValueError `  with the exception:  ` nx must be a positive integer ` 

*  ` nodes `  is the number of nodes found in the hidden layer* If  ` nodes `  is not an integer, raise a  ` TypeError `  with the exception:  ` nodes must be an integer ` 
* If  ` nodes `  is less than 1, raise a  ` ValueError `  with the exception:  ` nodes must be a positive integer ` 

* All exceptions should be raised in the order listed above

* Public instance attributes:*  ` W1 ` : The weights vector for the hidden layer. Upon instantiation, it should be initialized using a random normal distribution.
*  ` b1 ` : The bias for the hidden layer. Upon instantiation, it should be initialized with 0’s.
*  ` A1 ` : The activated output for the hidden layer. Upon instantiation, it should be initialized to 0.
*  ` W2 ` : The weights vector for the output neuron. Upon instantiation, it should be initialized using a random normal distribution.
*  ` b2 ` : The bias for the output neuron. Upon instantiation, it should be initialized to 0.
*  ` A2 ` : The activated output for the output neuron (prediction). Upon instantiation, it should be initialized to 0.

```bash
alexa@ubuntu-xenial:$ cat 8-main.py
#!/usr/bin/env python3

import numpy as np

NN = __import__('8-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X.shape[0], 3)
print(nn.W1)
print(nn.W1.shape)
print(nn.b1)
print(nn.W2)
print(nn.W2.shape)
print(nn.b2)
print(nn.A1)
print(nn.A2)
nn.A1 = 10
print(nn.A1)
alexa@ubuntu-xenial:$ ./8-main.py
[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719
  -1.34149673]
 [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133
   0.07912172]
 [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445
  -1.07836109]]
(3, 784)
[[0.]
 [0.]
 [0.]]
[[ 1.06160017 -1.18488744 -1.80525169]]
(1, 3)
0
0
0
10
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 8-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 9. Privatize NeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 8-neural_network.py `  ):
* class constructor:  ` def __init__(self, nx, nodes): ` *  ` nx `  is the number of input features* If  ` nx `  is not an integer, raise a  ` TypeError `  with the exception:  ` nx must be an integer ` 
* If  ` nx `  is less than 1, raise a  ` ValueError `  with the exception:  ` nx must be a positive integer ` 

*  ` nodes `  is the number of nodes found in the hidden layer* If  ` nodes `  is not an integer, raise a  ` TypeError `  with the exception:  ` nodes must be an integer ` 
* If  ` nodes `  is less than 1, raise a  ` ValueError `  with the exception:  ` nodes must be a positive integer ` 

* All exceptions should be raised in the order listed above

* Private instance attributes:*  ` W1 ` : The weights vector for the hidden layer. Upon instantiation, it should be initialized using a random normal distribution.
*  ` b1 ` : The bias for the hidden layer. Upon instantiation, it should be initialized with 0’s.
*  ` A1 ` : The activated output for the hidden layer. Upon instantiation, it should be initialized to 0.
*  ` W2 ` : The weights vector for the output neuron. Upon instantiation, it should be initialized using a random normal distribution.
*  ` b2 ` : The bias for the output neuron. Upon instantiation, it should be initialized to 0.
*  ` A2 ` : The activated output for the output neuron (prediction). Upon instantiation, it should be initialized to 0.
* Each private attribute should have a corresponding getter function (no setter function).

```bash
alexa@ubuntu-xenial:$ cat 9-main.py
#!/usr/bin/env python3

import numpy as np

NN = __import__('9-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X.shape[0], 3)
print(nn.W1)
print(nn.b1)
print(nn.W2)
print(nn.b2)
print(nn.A1)
print(nn.A2)
nn.A1 = 10
print(nn.A1)
alexa@ubuntu-xenial:$ ./9-main.py
[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719
  -1.34149673]
 [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133
   0.07912172]
 [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445
  -1.07836109]]
[[0.]
 [0.]
 [0.]]
[[ 1.06160017 -1.18488744 -1.80525169]]
0
0
0
Traceback (most recent call last):
  File "./9-main.py", line 19, in <module>
    nn.A1 = 10
AttributeError: can't set attribute
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 9-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 10. NeuralNetwork Forward Propagation
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 9-neural_network.py `  ):
* Add the public method  ` def forward_prop(self, X): ` * Calculates the forward propagation of the neural network
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

* Updates the private attributes  ` __A1 `  and  ` __A2 ` 
* The neurons should use a sigmoid activation function
* Returns the private attributes  ` __A1 `  and  ` __A2 ` , respectively

```bash
alexa@ubuntu-xenial:$ cat 10-main.py
#!/usr/bin/env python3

import numpy as np

NN = __import__('10-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X.shape[0], 3)
nn._NeuralNetwork__b1 = np.ones((3, 1))
nn._NeuralNetwork__b2 = 1
A1, A2 = nn.forward_prop(X)
if A1 is nn.A1:
        print(A1)
if A2 is nn.A2:
        print(A2)
alexa@ubuntu-xenial:$ ./10-main.py
[[5.34775247e-10 7.24627778e-04 4.52416436e-07 ... 8.75691930e-05
  1.13141966e-06 6.55799932e-01]
 [9.99652394e-01 9.99999995e-01 6.77919152e-01 ... 1.00000000e+00
  9.99662771e-01 9.99990554e-01]
 [5.57969669e-01 2.51645047e-02 4.04250047e-04 ... 1.57024117e-01
  9.97325173e-01 7.41310459e-02]]
[[0.23294587 0.44286405 0.54884691 ... 0.38502756 0.12079644 0.593269  ]]
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 10-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 11. NeuralNetwork Cost
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 10-neural_network.py `  ):
* Add the public method  ` def cost(self, Y, A): ` * Calculates the cost of the model using logistic regression
*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` A `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the activated output of the neuron for each example
* To avoid division by zero errors, please use  ` 1.0000001 - A `  instead of  ` 1 - A ` 
* Returns the cost

```bash
alexa@ubuntu-xenial:$ cat 11-main.py
#!/usr/bin/env python3

import numpy as np

NN = __import__('11-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X.shape[0], 3)
_, A = nn.forward_prop(X)
cost = nn.cost(Y, A)
print(cost)
alexa@ubuntu-xenial:$ ./11-main.py
0.7917984405648548
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 11-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 12. Evaluate NeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 11-neural_network.py `  ):
* Add the public method  ` def evaluate(self, X, Y): ` * Evaluates the neural network’s predictions
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
* Returns the neuron’s prediction and the cost of the network, respectively* The prediction should be a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the predicted labels for each example
* The label values should be 1 if the output of the network is >= 0.5 and 0 otherwise


```bash
alexa@ubuntu-xenial:$ cat 12-main.py
#!/usr/bin/env python3

import numpy as np

NN = __import__('12-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X.shape[0], 3)
A, cost = nn.evaluate(X, Y)
print(A)
print(cost)
alexa@ubuntu-xenial:$ ./12-main.py
[[0 0 0 ... 0 0 0]]
0.7917984405648548
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 12-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 13. NeuralNetwork Gradient Descent
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 12-neural_network.py `  ):
* Add the public method  ` def gradient_descent(self, X, Y, A1, A2, alpha=0.05): ` * Calculates one pass of gradient descent on the neural network
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` A1 `  is the output of the hidden layer
*  ` A2 `  is the predicted output
*  ` alpha `  is the learning rate
* Updates the private attributes  ` __W1 ` ,  ` __b1 ` ,  ` __W2 ` , and  ` __b2 ` 

```bash
alexa@ubuntu-xenial:$ cat 13-main.py
#!/usr/bin/env python3

import numpy as np

NN = __import__('13-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X.shape[0], 3)
A1, A2 = nn.forward_prop(X)
nn.gradient_descent(X, Y, A1, A2, 0.5)
print(nn.W1)
print(nn.b1)
print(nn.W2)
print(nn.b2)
alexa@ubuntu-xenial:$ ./13-main.py
[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719
  -1.34149673]
 [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133
   0.07912172]
 [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445
  -1.07836109]]
[[ 0.003193  ]
 [-0.01080922]
 [-0.01045412]]
[[ 1.06583858 -1.06149724 -1.79864091]]
[[0.15552509]]
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 13-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 14. Train NeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 13-neural_network.py `  ):
* Add the public method  ` def train(self, X, Y, iterations=5000, alpha=0.05): ` * Trains the neural network
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` iterations `  is the number of iterations to train over* if  ` iterations `  is not an integer, raise a  ` TypeError `  with the exception  ` iterations must be an integer ` 
* if  ` iterations `  is not positive, raise a  ` ValueError `  with the exception  ` iterations must be a positive integer ` 

*  ` alpha `  is the learning rate* if  ` alpha `  is not a float, raise a  ` TypeError `  with the exception  ` alpha must be a float ` 
* if  ` alpha `  is not positive, raise a  ` ValueError `  with the exception  ` alpha must be positive ` 

* All exceptions should be raised in the order listed above
* Updates the private attributes  ` __W1 ` ,  ` __b1 ` ,   ` __A1 ` ,  ` __W2 ` ,  ` __b2 ` , and  ` __A2 ` 
* You are allowed to use one loop
* Returns the evaluation of the training data after  ` iterations `  of training have occurred

```bash
alexa@ubuntu-xenial:$ cat 14-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

NN = __import__('14-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
lib_dev = np.load('../data/Binary_Dev.npz')
X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']
X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X_train.shape[0], 3)
A, cost = nn.train(X_train, Y_train, iterations=100)
accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100
print("Train cost:", cost)
print("Train accuracy: {}%".format(accuracy))
A, cost = nn.evaluate(X_dev, Y_dev)
accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100
print("Dev cost:", cost)
print("Dev accuracy: {}%".format(accuracy))
fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_dev_3D[i])
    plt.title(A[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./14-main.py
Train cost: 0.4680930945144984
Train accuracy: 84.69009080142123%
Dev cost: 0.45985938789496067
Dev accuracy: 86.52482269503547%
alexa@ubuntu-xenial:$

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/c744ae1dae04204a56ab.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=7a4e3a0e7e120b23bd6aa296a61bb4bf4c36138d2ab822e5dbf878628664d3b7) 

Pretty good… but there are still some incorrect labels. We need more data to see why…
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 14-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 15. Upgrade Train NeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` NeuralNetwork `   that defines a neural network with one hidden layer performing binary classification (based on   ` 14-neural_network.py `  ):
* Update the public method  ` train `  to  ` def train(self, X, Y, iterations=5000, alpha=0.05, verbose=True, graph=True, step=100): ` * Trains the neural network
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` iterations `  is the number of iterations to train over* if  ` iterations `  is not an integer, raise a  ` TypeError `  with the exception  ` iterations must be an integer ` 
* if  ` iterations `  is not positive, raise a  ` ValueError `  with the exception  ` iterations must be a positive integer ` 

*  ` alpha `  is the learning rate* if  ` alpha `  is not a float, raise a  ` TypeError `  with the exception  ` alpha must be a float ` 
* if  ` alpha `  is not positive, raise a  ` ValueError `  with the exception  ` alpha must be positive ` 

* Updates the private attributes  ` __W1 ` ,  ` __b1 ` ,  ` __A1 ` ,  ` __W2 ` ,  ` __b2 ` , and  ` __A2 ` 
*  ` verbose `  is a boolean that defines whether or not to print information about the training. If  ` True ` , print  ` Cost after {iteration} iterations: {cost} `  every  ` step `  iterations:*  Include data from the 0th and last iteration

*  ` graph `  is a boolean that defines whether or not to graph information about the training once the training has completed. If  ` True ` :*  Plot the training data every  ` step `  iterations as a blue line
*  Label the x-axis as  ` iteration ` 
*  Label the y-axis as  ` cost ` 
*  Title the plot  ` Training Cost ` 
*  Include data from the 0th and last iteration

*  Only if either  ` verbose `  or  ` graph `  are  ` True ` :* if  ` step `  is not an integer, raise a  ` TypeError `  with the exception  ` step must be an integer ` 
* if  ` step `  is not positive and less than or equal to  ` iterations ` , raise a  ` ValueError `  with the exception  ` step must be positive and <= iterations ` 

* All exceptions should be raised in the order listed above
*  The 0th iteration should represent the state of the neuron before any training has occurred
*  You are allowed to use one loop
* Returns the evaluation of the training data after  ` iterations `  of training have occurred

```bash
alexa@ubuntu-xenial:$ cat 15-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

NN = __import__('15-neural_network').NeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
lib_dev = np.load('../data/Binary_Dev.npz')
X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']
X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T

np.random.seed(0)
nn = NN(X_train.shape[0], 3)
A, cost = nn.train(X_train, Y_train)
accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100
print("Train cost:", cost)
print("Train accuracy: {}%".format(accuracy))
A, cost = nn.evaluate(X_dev, Y_dev)
accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100
print("Dev cost:", cost)
print("Dev accuracy: {}%".format(accuracy))
fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_dev_3D[i])
    plt.title(A[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./15-main.py
Cost after 0 iterations: 0.7917984405648547
Cost after 100 iterations: 0.4680930945144984

...

Cost after 5000 iterations: 0.024369225667283875

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/eab7b0b266e6b2b671f9.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=c63aa0ffbcfa843d1260d72271587ac65b6ef227682c791bdd1e62b4f10cf7c1) 

```bash
Train cost: 0.024369225667283875
Train accuracy: 99.3999210422424%
Dev cost: 0.020330639788072768
Dev accuracy: 99.57446808510639%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/af979f502eec55142860.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180431Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=79125a164b61b7b0cd934e365ef32682c59528829bf991df9c96cb2064856e78) 

 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 15-neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 16. DeepNeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification:
* class constructor:  ` def __init__(self, nx, layers): ` *  ` nx `  is the number of input features* If  ` nx `  is not an integer, raise a  ` TypeError `  with the exception:  ` nx must be an integer ` 
* If  ` nx `  is less than 1, raise a  ` ValueError `  with the exception:  ` nx must be a positive integer ` 

*  ` layers `  is a list representing the number of nodes in each layer of the network* If  ` layers `  is not a list or an empty list, raise a  ` TypeError `  with the exception:  ` layers must be a list of positive integers ` 
* The first value in  ` layers `  represents the number of nodes in the first layer, …
* If the elements in  ` layers `  are not all positive integers, raise a  ` TypeError `  with the exception  ` layers must be a list of positive integers ` 

* All exceptions should be raised in the order listed above
* Sets the public instance attributes:*  ` L ` : The number of layers in the neural network.
*  ` cache ` : A dictionary to hold all intermediary values of the network. Upon instantiation, it should be set to an empty dictionary.
*  ` weights ` : A dictionary to hold all weights and biased of the network. Upon instantiation:* The weights of the network should be initialized using the  ` He et al. `  method and saved in the  ` weights `  dictionary using the key  ` W{l} `  where  ` {l} `  is the hidden layer the weight belongs to
* The biases of the network should be initialized to 0’s and saved in the  ` weights `  dictionary using the key  ` b{l} `  where  ` {l} `  is the hidden layer the bias belongs to


* You are allowed to use one loop

```bash
alexa@ubuntu-xenial:$ cat 16-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('16-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X.shape[0], [5, 3, 1])
print(deep.cache)
print(deep.weights)
print(deep.L)
deep.L = 10
print(deep.L)
alexa@ubuntu-xenial:$ ./16-main.py
{}
{'b3': array([[0.]]), 'W2': array([[ 0.4609219 ,  0.56004008, -1.2250799 , -0.09454199,  0.57799141],
       [-0.16310703,  0.06882082, -0.94578088, -0.30359994,  1.15661914],
       [-0.49841799, -0.9111359 ,  0.09453424,  0.49877298,  0.75503205]]), 'W3': array([[-0.42271877,  0.18165055,  0.4444639 ]]), 'b2': array([[0.],
       [0.],
       [0.]]), 'W1': array([[ 0.0890981 ,  0.02021099,  0.04943373, ...,  0.02632982,
         0.03090699, -0.06775582],
       [ 0.02408701,  0.00749784,  0.02672082, ...,  0.00484894,
        -0.00227857,  0.00399625],
       [ 0.04295829, -0.04238217, -0.05110231, ..., -0.00364861,
         0.01571416, -0.05446546],
       [ 0.05361891, -0.05984585, -0.09117898, ..., -0.03094292,
        -0.01925805, -0.06308145],
       [-0.01667953, -0.04216413,  0.06239623, ..., -0.02024521,
        -0.05159656, -0.02373981]]), 'b1': array([[0.],
       [0.],
       [0.],
       [0.],
       [0.]])}
3
10
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 16-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 17. Privatize DeepNeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 16-deep_neural_network.py `  ):
* class constructor:  ` def __init__(self, nx, layers): ` *  ` nx `  is the number of input features* If  ` nx `  is not an integer, raise a  ` TypeError `  with the exception:  ` nx must be an integer ` 
* If  ` nx `  is less than 1, raise a  ` ValueError `  with the exception:  ` nx must be a positive integer ` 

*  ` layers `  is a list representing the number of nodes in each layer of the network* If  ` layers `  is not a list, raise a  ` TypeError `  with the exception:  ` layers must be a list of positive integers ` 
* The first value in  ` layers `  represents the number of nodes in the first layer, …
* If the elements in  ` layers `  are not all positive integers, raise a TypeError with the exception  ` layers must be a list of positive integers ` 

* All exceptions should be raised in the order listed above
* Sets the private instance attributes:*  ` __L ` : The number of layers in the neural network.
*  ` __cache ` : A dictionary to hold all intermediary values of the network. Upon instantiation, it should be set to an empty dictionary.
*  ` __weights ` : A dictionary to hold all weights and biased of the network. Upon instantiation:* The weights of the network should be initialized using the  ` He et al. `  method and saved in the  ` __weights `  dictionary using the key  ` W{l} `  where  ` {l} `  is the hidden layer the weight belongs to
* The biases of the network should be initialized to  ` 0 ` ‘s and saved in the  ` __weights `  dictionary using the key  ` b{l} `  where  ` {l} `  is the hidden layer the bias belongs to

* Each private attribute should have a corresponding getter function (no setter function).

* You are allowed to use one loop

```bash
alexa@ubuntu-xenial:$ cat 17-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('17-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X.shape[0], [5, 3, 1])
print(deep.cache)
print(deep.weights)
print(deep.L)
deep.L = 10
print(deep.L)
alexa@ubuntu-xenial:$ ./17-main.py
{}
{'b1': array([[0.],
       [0.],
       [0.],
       [0.],
       [0.]]), 'b2': array([[0.],
       [0.],
       [0.]]), 'W2': array([[ 0.4609219 ,  0.56004008, -1.2250799 , -0.09454199,  0.57799141],
       [-0.16310703,  0.06882082, -0.94578088, -0.30359994,  1.15661914],
       [-0.49841799, -0.9111359 ,  0.09453424,  0.49877298,  0.75503205]]), 'W1': array([[ 0.0890981 ,  0.02021099,  0.04943373, ...,  0.02632982,
         0.03090699, -0.06775582],
       [ 0.02408701,  0.00749784,  0.02672082, ...,  0.00484894,
        -0.00227857,  0.00399625],
       [ 0.04295829, -0.04238217, -0.05110231, ..., -0.00364861,
         0.01571416, -0.05446546],
       [ 0.05361891, -0.05984585, -0.09117898, ..., -0.03094292,
        -0.01925805, -0.06308145],
       [-0.01667953, -0.04216413,  0.06239623, ..., -0.02024521,
        -0.05159656, -0.02373981]]), 'b3': array([[0.]]), 'W3': array([[-0.42271877,  0.18165055,  0.4444639 ]])}
3
Traceback (most recent call last):
  File "./17-main.py", line 16, in <module>
    deep.L = 10
AttributeError: can't set attribute
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 17-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 18. DeepNeuralNetwork Forward Propagation
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 17-deep_neural_network.py `  ):
* Add the public method  ` def forward_prop(self, X): ` * Calculates the forward propagation of the neural network
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

* Updates the private attribute  ` __cache ` :* The activated outputs of each layer should be saved in the  ` __cache `  dictionary using the key  ` A{l} `  where  ` {l} `  is the hidden layer the activated output belongs to
*  ` X `  should be saved to the  ` cache `  dictionary using the key  ` A0 ` 

* All neurons should use a sigmoid activation function
* You are allowed to use one loop
* Returns the output of the neural network and the cache, respectively

```bash
alexa@ubuntu-xenial:$ cat 18-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('18-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X.shape[0], [5, 3, 1])
deep._DeepNeuralNetwork__weights['b1'] = np.ones((5, 1))
deep._DeepNeuralNetwork__weights['b2'] = np.ones((3, 1))
deep._DeepNeuralNetwork__weights['b3'] = np.ones((1, 1))
A, cache = deep.forward_prop(X)
print(A)
print(cache)
print(cache is deep.cache)
print(A is cache['A3'])
alexa@ubuntu-xenial:$ ./18-main.py
[[0.75603476 0.7516025  0.75526716 ... 0.75228888 0.75522853 0.75217069]]
{'A1': array([[0.4678435 , 0.64207147, 0.55271425, ..., 0.61718097, 0.56412986,
        0.72751504],
       [0.79441392, 0.87140579, 0.72851107, ..., 0.8898201 , 0.79466389,
        0.82257068],
       [0.72337339, 0.68239373, 0.63526533, ..., 0.7036234 , 0.7770501 ,
        0.69465346],
       [0.65305735, 0.69829955, 0.58646313, ..., 0.73949722, 0.52054315,
        0.73151973],
       [0.67408798, 0.69624537, 0.73084352, ..., 0.70663173, 0.76204175,
        0.72705428]]), 'A3': array([[0.75603476, 0.7516025 , 0.75526716, ..., 0.75228888, 0.75522853,
        0.75217069]]), 'A0': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'A2': array([[0.75067742, 0.78319533, 0.77755571, ..., 0.77891002, 0.75847839,
        0.78517215],
       [0.70591081, 0.71159364, 0.7362214 , ..., 0.70845465, 0.72133875,
        0.71090691],
       [0.72032379, 0.69519095, 0.72414599, ..., 0.70067751, 0.71161433,
        0.70420437]])}
True
True
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 18-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 19. DeepNeuralNetwork Cost
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 18-deep_neural_network.py `  ):
* Add the public method  ` def cost(self, Y, A): ` * Calculates the cost of the model using logistic regression
*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` A `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the activated output of the neuron for each example
* To avoid division by zero errors, please use  ` 1.0000001 - A `  instead of  ` 1 - A ` 
* Returns the cost

```bash
alexa@ubuntu-xenial:$ cat 19-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('19-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X.shape[0], [5, 3, 1])
A, _ = deep.forward_prop(X)
cost = deep.cost(Y, A)
print(cost)
alexa@ubuntu-xenial:$ ./19-main.py
0.6958649419170609
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 19-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 20. Evaluate DeepNeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 19-deep_neural_network.py `  ):
* Add the public method  ` def evaluate(self, X, Y): ` * Evaluates the neural network’s predictions
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
* Returns the neuron’s prediction and the cost of the network, respectively* The prediction should be a  ` numpy.ndarray `  with shape (1,  ` m ` ) containing the predicted labels for each example
* The label values should be 1 if the output of the network is >= 0.5 and 0 otherwise


```bash
alexa@ubuntu-xenial:$ cat 20-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('20-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X.shape[0], [5, 3, 1])
A, cost = deep.evaluate(X, Y)
print(A)
print(cost)
alexa@ubuntu-xenial:$ ./20-main.py
[[1 1 1 ... 1 1 1]]
0.6958649419170609
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 20-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 21. DeepNeuralNetwork Gradient Descent
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 20-deep_neural_network.py `  ):
* Add the public method  ` def gradient_descent(self, Y, cache, alpha=0.05): ` * Calculates one pass of gradient descent on the neural network
*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` cache `  is a dictionary containing all the intermediary values of the network
*  ` alpha `  is the learning rate
* Updates the private attribute  ` __weights ` 
* You are allowed to use one loop

```bash
alexa@ubuntu-xenial:$ cat 21-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('21-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_3D, Y = lib_train['X'], lib_train['Y']
X = X_3D.reshape((X_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X.shape[0], [5, 3, 1])
A, cache = deep.forward_prop(X)
deep.gradient_descent(Y, cache, 0.5)
print(deep.weights)
alexa@ubuntu-xenial:$ ./21-main.py
{'b3': array([[0.00659936]]), 'b2': array([[-0.00055419],
       [ 0.00032369],
       [ 0.0007201 ]]), 'W2': array([[ 0.4586347 ,  0.55968571, -1.22435332, -0.09516874,  0.57668454],
       [-0.16209305,  0.06902405, -0.9460547 , -0.30329296,  1.15722071],
       [-0.49595566, -0.91068385,  0.09382566,  0.49948968,  0.75647764]]), 'b1': array([[-1.01835520e-03],
       [-1.22929756e-04],
       [ 9.25521878e-05],
       [ 1.07730873e-04],
       [ 2.29014796e-04]]), 'W3': array([[-0.41262664,  0.18889024,  0.44717929]]), 'W1': array([[ 0.0890981 ,  0.02021099,  0.04943373, ...,  0.02632982,
         0.03090699, -0.06775582],
       [ 0.02408701,  0.00749784,  0.02672082, ...,  0.00484894,
        -0.00227857,  0.00399625],
       [ 0.04295829, -0.04238217, -0.05110231, ..., -0.00364861,
         0.01571416, -0.05446546],
       [ 0.05361891, -0.05984585, -0.09117898, ..., -0.03094292,
        -0.01925805, -0.06308145],
       [-0.01667953, -0.04216413,  0.06239623, ..., -0.02024521,
        -0.05159656, -0.02373981]])}
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 21-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 22. Train DeepNeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 21-deep_neural_network.py `  ):
* Add the public method  ` def train(self, X, Y, iterations=5000, alpha=0.05): ` * Trains the deep neural network
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` iterations `  is the number of iterations to train over* if  ` iterations `  is not an integer, raise a  ` TypeError `  with the exception  ` iterations must be an integer ` 
* if  ` iterations `  is not positive, raise a  ` ValueError `  with the exception  ` iterations must be a positive integer ` 

*  ` alpha `  is the learning rate* if  ` alpha `  is not a float, raise a TypeError with the exception  ` alpha must be a float ` 
* if  ` alpha `  is not positive, raise a ValueError with the exception  ` alpha must be positive ` 

* All exceptions should be raised in the order listed above
* Updates the private attributes  ` __weights `  and  ` __cache ` 
* You are allowed to use one loop
* Returns the evaluation of the training data after  ` iterations `  of training have occurred

```bash
alexa@ubuntu-xenial:$ cat 22-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

Deep = __import__('22-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
lib_dev = np.load('../data/Binary_Dev.npz')
X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']
X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X_train.shape[0], [5, 3, 1])
A, cost = deep.train(X_train, Y_train, iterations=100)
accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100
print("Train cost:", cost)
print("Train accuracy: {}%".format(accuracy))
A, cost = deep.evaluate(X_dev, Y_dev)
accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100
print("Dev cost:", cost)
print("Dev accuracy: {}%".format(accuracy))
fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_dev_3D[i])
    plt.title(A[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./22-main.py
Train cost: 0.6444304786060048
Train accuracy: 56.241610738255034%
Dev cost: 0.6428913158565179
Dev accuracy: 57.730496453900706%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/a98aac58c29430f560be.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180432Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=3e770a189dc271ae39322883e96dd0e9857cb10181cba675bb03b61bb2c18a1b) 

Hmm… doesn’t seem like this worked very well. Could it be because of our architecture or that it wasn’t trained properly? We need to see more information…
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 22-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 23. Upgrade Train DeepNeuralNetwork
          mandatory         Progress vs Score  Task Body Write a class   ` DeepNeuralNetwork `   that defines a deep neural network performing binary classification (based on   ` 22-deep_neural_network.py `  ):
* Update the public method  ` train `  to  ` def train(self, X, Y, iterations=5000, alpha=0.05, verbose=True, graph=True, step=100): ` * Trains the deep neural network by updating the private attributes  ` __weights `  and  ` __cache ` 
*  ` X `  is a  ` numpy.ndarray `  with shape ( ` nx ` ,  ` m ` ) that contains the input data*  ` nx `  is the number of input features to the neuron
*  ` m `  is the number of examples

*  ` Y `  is a  ` numpy.ndarray `  with shape (1,  ` m ` ) that contains the correct labels for the input data
*  ` iterations `  is the number of iterations to train over* if  ` iterations `  is not an integer, raise a  ` TypeError `  with the exception  ` iterations must be an integer ` 
* if  ` iterations `  is not positive, raise a  ` ValueError `  with the exception  ` iterations must be a positive integer ` 

*  ` alpha `  is the learning rate* if  ` alpha `  is not a float, raise a  ` TypeError `  with the exception  ` alpha must be a float ` 
* if  ` alpha `  is not positive, raise a  ` ValueError `  with the exception  ` alpha must be positive ` 

*  ` verbose `  is a boolean that defines whether or not to print information about the training. If  ` True ` , print  ` Cost after {iteration} iterations: {cost} `  every  ` step `  iterations:*  Include data from the 0th and last iteration

*  ` graph `  is a boolean that defines whether or not to graph information about the training once the training has completed. If  ` True ` :*  Plot the training data every  ` step `  iterations as a blue line
*  Label the x-axis as  ` iteration ` 
*  Label the y-axis as  ` cost ` 
*  Title the plot  ` Training Cost ` 
*  Include data from the 0th and last iteration

*  Only if either  ` verbose `  or  ` graph `  are  ` True ` :* if  ` step `  is not an integer, raise a  ` TypeError `  with the exception  ` step must be an integer ` 
* if  ` step `  is not positive and less than or equal to  ` iterations ` , raise a  ` ValueError `  with the exception  ` step must be positive and <= iterations ` 

* All exceptions should be raised in the order listed above
*  The 0th iteration should represent the state of the neuron before any training has occurred
*  You are allowed to use one loop
* Returns the evaluation of the training data after  ` iterations `  of training have occurred

```bash
alexa@ubuntu-xenial:$ cat 23-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

Deep = __import__('23-deep_neural_network').DeepNeuralNetwork

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
lib_dev = np.load('../data/Binary_Dev.npz')
X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']
X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X_train.shape[0], [5, 3, 1])
A, cost = deep.train(X_train, Y_train)
accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100
print("Train cost:", cost)
print("Train accuracy: {}%".format(accuracy))
A, cost = deep.evaluate(X_dev, Y_dev)
accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100
print("Dev cost:", cost)
print("Dev accuracy: {}%".format(accuracy))
fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_dev_3D[i])
    plt.title(A[0, i])
    plt.axis('off')
plt.tight_layout()
plt.show()
alexa@ubuntu-xenial:$ ./23-main.py
Cost after 0 iterations: 0.6958649419170609
Cost after 100 iterations: 0.6444304786060048

...

Cost after 5000 iterations: 0.011671820326008168

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/aa9c72daf01c699ff3e3.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180432Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f00223d276f58feabe528666d14e8743ec89ab3ea939ac12136f5c58e8f9485a) 

```bash
Train cost: 0.011671820326008168
Train accuracy: 99.88945913936044%
Dev cost: 0.00924955213227925
Dev accuracy: 99.95271867612293%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/4e851fa02634579a9591.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180432Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=a80230802d74602854370a19ca659fca5827ff399516641ae7992e32bf7421c9) 

 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 23-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 24. One-Hot Encode
          mandatory         Progress vs Score  Task Body Write a function   ` def one_hot_encode(Y, classes): `   that converts a numeric label vector into a one-hot matrix:
*  ` Y `  is a  ` numpy.ndarray `  with shape ( ` m ` ,) containing numeric class labels*  ` m `  is the number of examples

*  ` classes `  is the maximum number of classes found in  ` Y ` 
* Returns: a one-hot encoding of  ` Y `  with shape ( ` classes ` ,  ` m ` ), or  ` None `  on failure
```bash
alexa@ubuntu-xenial:$ cat 24-main.py
#!/usr/bin/env python3

import numpy as np

oh_encode = __import__('24-one_hot_encode').one_hot_encode

lib = np.load('../data/MNIST.npz')
Y = lib['Y_train'][:10]

print(Y)
Y_one_hot = oh_encode(Y, 10)
print(Y_one_hot)
alexa@ubuntu-xenial:$ ./24-main.py
[5 0 4 1 9 2 1 3 1 4]
[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 24-one_hot_encode.py ` 
 Self-paced manual review  Panel footer - Controls 
### 25. One-Hot Decode
          mandatory         Progress vs Score  Task Body Write a function   ` def one_hot_decode(one_hot): `   that converts a one-hot matrix into a vector of labels:
*  ` one_hot `  is a one-hot encoded  ` numpy.ndarray `  with shape ( ` classes ` ,  ` m ` )*  ` classes `  is the maximum number of classes
*  ` m `  is the number of examples

* Returns: a  ` numpy.ndarray `  with shape ( ` m ` , ) containing the numeric labels for each example, or  ` None `  on failure
```bash
alexa@ubuntu-xenial:$ cat 25-main.py
#!/usr/bin/env python3

import numpy as np

oh_encode = __import__('24-one_hot_encode').one_hot_encode
oh_decode = __import__('25-one_hot_decode').one_hot_decode

lib = np.load('../data/MNIST.npz')
Y = lib['Y_train'][:10]

print(Y)
Y_one_hot = oh_encode(Y, 10)
Y_decoded = oh_decode(Y_one_hot)
print(Y_decoded)
alexa@ubuntu-xenial:$ ./25-main.py
[5 0 4 1 9 2 1 3 1 4]
[5 0 4 1 9 2 1 3 1 4]
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 25-one_hot_decode.py ` 
 Self-paced manual review  Panel footer - Controls 
### 26. Persistence is Key
          mandatory         Progress vs Score  Task Body Update the class   ` DeepNeuralNetwork `   (based on   ` 23-deep_neural_network.py `  ):
* Create the instance method   ` def save(self, filename): ` 
* Saves the instance object to a file in  ` pickle `  format
*  ` filename `  is the file to which the object should be saved
* If  ` filename `  does not have the extension  ` .pkl ` , add it

* Create the static method   ` def load(filename): ` 
* Loads a pickled  ` DeepNeuralNetwork `  object
*  ` filename `  is the file from which the object should be loaded
* Returns: the loaded object, or  ` None `  if  ` filename `  doesn’t exist

```bash
alexa@ubuntu-xenial:$ cat 26-main.py
#!/usr/bin/env python3

import numpy as np

Deep = __import__('26-deep_neural_network').DeepNeuralNetwork
one_hot_encode = __import__('24-one_hot_encode').one_hot_encode
one_hot_decode = __import__('25-one_hot_decode').one_hot_decode

lib_train = np.load('../data/Binary_Train.npz')
X_train_3D, Y_train = lib_train['X'], lib_train['Y']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T

np.random.seed(0)
deep = Deep(X_train.shape[0], [3, 1])
A, cost = deep.train(X_train, Y_train, iterations=500, graph=False)
deep.save('26-output')
del deep

saved = Deep.load('26-output.pkl')
A_saved, cost_saved = saved.evaluate(X_train, Y_train)

print(np.array_equal(A, A_saved) and cost == cost_saved)
alexa@ubuntu-xenial:$ ls 26-output*
ls: cannot access '26-output*': No such file or directory
alexa@ubuntu-xenial:$ ./26-main.py
Cost after 0 iterations: 0.7773240521521816
Cost after 100 iterations: 0.18751378071323066
Cost after 200 iterations: 0.12117095705345622
Cost after 300 iterations: 0.09031067302785326
Cost after 400 iterations: 0.07222364349190777
Cost after 500 iterations: 0.060335256947006956
True
alexa@ubuntu-xenial:$ ls 26-output*
26-output.pkl
alexa@ubuntu-xenial:$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 26-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 27. Update DeepNeuralNetwork
          mandatory         Progress vs Score  Task Body Update the class   ` DeepNeuralNetwork `   to perform multiclass classification (based on   ` 26-deep_neural_network.py `  ):
* You will need to update the instance methods  ` forward_prop ` ,  ` cost ` , and  ` evaluate ` 
*  ` Y `  is now a one-hot  ` numpy.ndarray `  of shape  ` (classes, m) ` 
Ideally, you should not have to change the  ` __init__ ` ,  ` gradient_descent ` , or  ` train `  instance methods
Because the training process takes such a long time, I have pretrained a model for you to load and finish training ( [27-saved.pkl](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/27-saved.pkl) 
 ). This model has already been trained for 2000 iterations.
The training process may take up to 5 minutes
```bash
alexa@ubuntu-xenial:$ cat 27-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

Deep = __import__('27-deep_neural_network').DeepNeuralNetwork
one_hot_encode = __import__('24-one_hot_encode').one_hot_encode
one_hot_decode = __import__('25-one_hot_decode').one_hot_decode

lib= np.load('../data/MNIST.npz')
X_train_3D = lib['X_train']
Y_train = lib['Y_train']
X_valid_3D = lib['X_valid']
Y_valid = lib['Y_valid']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
X_valid = X_valid_3D.reshape((X_valid_3D.shape[0], -1)).T
Y_train_one_hot = one_hot_encode(Y_train, 10)
Y_valid_one_hot = one_hot_encode(Y_valid, 10)

deep = Deep.load('27-saved.pkl')
A_one_hot, cost = deep.train(X_train, Y_train_one_hot, iterations=100,
                             step=10, graph=False)
A = one_hot_decode(A_one_hot)
accuracy = np.sum(Y_train == A) / Y_train.shape[0] * 100
print("Train cost:", cost)
print("Train accuracy: {}%".format(accuracy))

A_one_hot, cost = deep.evaluate(X_valid, Y_valid_one_hot)
A = one_hot_decode(A_one_hot)
accuracy = np.sum(Y_valid == A) / Y_valid.shape[0] * 100
print("Validation cost:", cost)
print("Validation accuracy: {}%".format(accuracy))

deep.save('27-output')

fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_valid_3D[i])
    plt.title(A[i])
    plt.axis('off')
plt.tight_layout()
plt.show()
ubuntu@alexa-ml:~$ ./27-main.py
Cost after 0 iterations: 0.4388904112857044
Cost after 10 iterations: 0.4377828804163359
Cost after 20 iterations: 0.43668839872612714
Cost after 30 iterations: 0.43560674736059446
Cost after 40 iterations: 0.43453771176806555
Cost after 50 iterations: 0.4334810815993252
Cost after 60 iterations: 0.43243665061046205
Cost after 70 iterations: 0.4314042165687683
Cost after 80 iterations: 0.4303835811615513
Cost after 90 iterations: 0.4293745499077264
Cost after 100 iterations: 0.42837693207206473
Train cost: 0.42837693207206473
Train accuracy: 88.442%
Validation cost: 0.39517557351173044
Validation accuracy: 89.64%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/56a3c67be6d6c24329c8.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180432Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e07d121d0f5d7e591dd72ac802d49946db27812c7c9e98dee2bb75fdfa45516a) 

As you can see, our training has become very slow and is beginning to plateau. Let’s alter the model a little and see if we get a better result
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 27-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 28. All the Activations
          mandatory         Progress vs Score  Task Body Update the class   ` DeepNeuralNetwork `   to allow different activation functions (based on   ` 27-deep_neural_network.py `  ):
* Update the  ` __init__ `  method to  ` def __init__(self, nx, layers, activation='sig'): ` *  ` activation `  represents the type of activation function used in the hidden layers*  ` sig `  represents a sigmoid activation
*  ` tanh `  represents a tanh activation
* if  ` activation `  is not  ` sig `  or  ` tanh ` , raise a  ` ValueError `  with the exception:  ` activation must be 'sig' or 'tanh' ` 

* Create the private attribute  ` __activation `  and set it to the value of  ` activation ` 
* Create a getter for the private attribute  ` __activation ` 

* Update the  ` forward_prop `  and  ` gradient_descent `  instance methods to use the  ` __activation `  function in the hidden layers
Because the training process takes such a long time, I have pre-trained a model for you to load and finish training ( [28-saved.pkl](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/28-saved.pkl) 
 ). This model has already been trained for 2000 iterations.
The training process may take up to 5 minutes
```bash
alexa@ubuntu-xenial:$ cat 28-main.py
#!/usr/bin/env python3

import matplotlib.pyplot as plt
import numpy as np

Deep27 = __import__('27-deep_neural_network').DeepNeuralNetwork
Deep28 = __import__('28-deep_neural_network').DeepNeuralNetwork
one_hot_encode = __import__('24-one_hot_encode').one_hot_encode
one_hot_decode = __import__('25-one_hot_decode').one_hot_decode

lib= np.load('../data/MNIST.npz')
X_train_3D = lib['X_train']
Y_train = lib['Y_train']
X_valid_3D = lib['X_valid']
Y_valid = lib['Y_valid']
X_test_3D = lib['X_test']
Y_test = lib['Y_test']
X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T
X_valid = X_valid_3D.reshape((X_valid_3D.shape[0], -1)).T
X_test = X_test_3D.reshape((X_test_3D.shape[0], -1)).T
Y_train_one_hot = one_hot_encode(Y_train, 10)
Y_valid_one_hot = one_hot_encode(Y_valid, 10)
Y_test_one_hot = one_hot_encode(Y_test, 10)

print('Sigmoid activation:')
deep27 = Deep27.load('27-output.pkl')
A_one_hot27, cost27 = deep27.evaluate(X_train, Y_train_one_hot)
A27 = one_hot_decode(A_one_hot27)
accuracy27 = np.sum(Y_train == A27) / Y_train.shape[0] * 100
print("Train cost:", cost27)
print("Train accuracy: {}%".format(accuracy27))
A_one_hot27, cost27 = deep27.evaluate(X_valid, Y_valid_one_hot)
A27 = one_hot_decode(A_one_hot27)
accuracy27 = np.sum(Y_valid == A27) / Y_valid.shape[0] * 100
print("Validation cost:", cost27)
print("Validation accuracy: {}%".format(accuracy27))
A_one_hot27, cost27 = deep27.evaluate(X_test, Y_test_one_hot)
A27 = one_hot_decode(A_one_hot27)
accuracy27 = np.sum(Y_test == A27) / Y_test.shape[0] * 100
print("Test cost:", cost27)
print("Test accuracy: {}%".format(accuracy27))

fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_test_3D[i])
    plt.title(A27[i])
    plt.axis('off')
plt.tight_layout()
plt.show()

print('\nTanh activaiton:')

deep28 = Deep28.load('28-saved.pkl')
A_one_hot28, cost28 = deep28.train(X_train, Y_train_one_hot, iterations=100,
                                step=10, graph=False)
A28 = one_hot_decode(A_one_hot28)
accuracy28 = np.sum(Y_train == A28) / Y_train.shape[0] * 100
print("Train cost:", cost28)
print("Train accuracy: {}%".format(accuracy28))
A_one_hot28, cost28 = deep28.evaluate(X_valid, Y_valid_one_hot)
A28 = one_hot_decode(A_one_hot28)
accuracy28 = np.sum(Y_valid == A28) / Y_valid.shape[0] * 100
print("Validation cost:", cost28)
print("Validation accuracy: {}%".format(accuracy28))
A_one_hot28, cost28 = deep28.evaluate(X_test, Y_test_one_hot)
A28 = one_hot_decode(A_one_hot28)
accuracy28 = np.sum(Y_test == A28) / Y_test.shape[0] * 100
print("Test cost:", cost28)
print("Test accuracy: {}%".format(accuracy28))
deep28.save('28-output')

fig = plt.figure(figsize=(10, 10))
for i in range(100):
    fig.add_subplot(10, 10, i + 1)
    plt.imshow(X_test_3D[i])
    plt.title(A28[i])
    plt.axis('off')
plt.tight_layout()
plt.show()

alexa@ubuntu-xenial:$ ./28-main.py
Sigmoid activation:
Train cost: 0.42837693207206456
Train accuracy: 88.442%
Validation cost: 0.39517557351173044
Validation accuracy: 89.64%
Test cost: 0.4074169894615401
Test accuracy: 89.0%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/3866daf330fc19803b64.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180432Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=61b5e5a98534090689e77a2c289e8e9674a8c9146644b62e4becfd61114fe8d4) 

```bash
Tanh activaiton:
Cost after 0 iterations: 0.1806181562229199
Cost after 10 iterations: 0.1801200954271858
Cost after 20 iterations: 0.1796242897834926
Cost after 30 iterations: 0.17913072860418564
Cost after 40 iterations: 0.1786394012066576
Cost after 50 iterations: 0.17815029691267442
Cost after 60 iterations: 0.1776634050478437
Cost after 70 iterations: 0.1771787149412177
Cost after 80 iterations: 0.1766962159250237
Cost after 90 iterations: 0.1762158973345138
Cost after 100 iterations: 0.1757377485079266
Train cost: 0.1757377485079266
Train accuracy: 95.006%
Validation cost: 0.17689309600397934
Validation accuracy: 95.13000000000001%
Test cost: 0.1809489808838737
Test accuracy: 94.77%

```
 ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/10/eda31c43c3c9df937b9f.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220506%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220506T180432Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=dd33c1cd3653b00d93dc810df429ab1458d39a9675e89294f07dc50d0a3ed3ba) 

The training of this model is also getting slow and plateauing after about 2000 iterations. However, just by changing the activation function, we have nearly halved the model’s cost and increased its accuracy by about 6%
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x01-classification ` 
* File:  ` 28-deep_neural_network.py ` 
 Self-paced manual review  Panel footer - Controls 
### 29. Blogpost
          #advanced         Progress vs Score  Task Body Write a blog post that explains the purpose of activation functions and compares and contrasts (at the minimum) the following functions:
* Binary
* Linear
* Sigmoid
* Tanh
* ReLU
* Softmax
Your posts should have examples and at least one picture, at the top. Publish your blog post on Medium or LinkedIn, and share it at least on LinkedIn.
When done, please add all URLs below (blog post, LinkedIn post, etc.)
Please, remember that these blogs must be written in English to further your technical ability in a variety of settings.
 Task URLs #### Add URLs here:
                Save               Github information  Self-paced manual review  Panel footer - Controls 
Ready for a  manual review
