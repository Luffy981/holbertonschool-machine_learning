# 0x06. Keras
## Details
      By Alexa Orrico, Software Engineer at Holberton School          Weight: 2                Ongoing project - started May 30, 2022 , must end by Jun 1, 2022           - you're done with 0% of tasks.              Checker will be released at May 31, 2022 12:00 AM        An auto review will be launched at the deadline       ![](https://holbertonintranet.s3.amazonaws.com/uploads/medias/2018/11/c48e37d9cda2293173b7.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARDDGGGOU5BHMTQX4%2F20220530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220530T203211Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=c4b8cb60ad00604fa7af6380de0034d96bdfb6447e66ad43340d961667c37d8b) 

## Resources
Read or watch :
* [TensorFlow 1 vs TensorFlow 2: Is the new TF better?](https://intranet.hbtn.io/rltoken/MaKLr6vMXwXfxPE76O9riw) 

* [Differences Between Tensorflow 1.x and Tensorflow 2.0](https://intranet.hbtn.io/rltoken/B0gsi2941Q20JT7M2gNpwQ) 

* [Keras Explained](https://intranet.hbtn.io/rltoken/Q6LXU1f1JwaOe7fakxQYMA) 
 (starting at 3:48)
* [Keras](https://intranet.hbtn.io/rltoken/aMY5GW_HGJwP9Q5wvSI-Pw) 

* [Keras vs. tf.keras: What’s the difference in TensorFlow 2.0?](https://intranet.hbtn.io/rltoken/5ydJAynv2tvxboAY6l0kyg) 

* [Hierarchical Data Format](https://intranet.hbtn.io/rltoken/9wXncIBTP1NMQZ-djKipUw) 

References :
* [tf.keras](https://intranet.hbtn.io/rltoken/8d4MOKiPD_uAQsZMESHAEg) 
* [tf.keras.models](https://intranet.hbtn.io/rltoken/kxyXWw_TGTZSU-zL-mOplg) 

* [tf.keras.activations](https://intranet.hbtn.io/rltoken/BcrfQjmluFYgg5mw-ZSb8g) 

* [tf.keras.callbacks](https://intranet.hbtn.io/rltoken/IGUsMn26TFvnsFxwohR-qQ) 

* [tf.keras.initializers](https://intranet.hbtn.io/rltoken/TzfWV3oblkTEB2nzJcQFPA) 

* [tf.keras.layers](https://intranet.hbtn.io/rltoken/Dd82ZLTHkpsfg99F72VKeQ) 

* [tf.keras.losses](https://intranet.hbtn.io/rltoken/O2dfF4zjNiTqs00kh7WGcw) 

* [tf.keras.metrics](https://intranet.hbtn.io/rltoken/Jy9tCP43xGUHaVHCRSPIwg) 

* [tf.keras.optimizers](https://intranet.hbtn.io/rltoken/Ajww3cwZk4Tn0onIMYlsqQ) 

* [tf.keras.regularizers](https://intranet.hbtn.io/rltoken/9R-gz17WUjuBZRDkb8FcSw) 

* [tf.keras.utils](https://intranet.hbtn.io/rltoken/LRh9ApNMBnOnry5ntgrMZw) 


## Learning Objectives
At the end of this project, you are expected to be able to  [explain to anyone](https://intranet.hbtn.io/rltoken/2F4gmNqSXJQlQhe63QlWtQ) 
 ,  without the help of Google :
### General
* What is Keras?
* What is a model?
* How to instantiate a model (2 ways)
* How to build a layer
* How to add regularization to a layer
* How to add dropout to a layer
* How to add batch normalization
* How to compile a model
* How to optimize a model
* How to fit a model
* How to use validation data
* How to perform early stopping
* How to measure accuracy
* How to evaluate a model
* How to make a prediction with a model
* How to access the weights/outputs of a model
* What is HDF5?
* How to save and load a model’s weights, a model’s configuration, and the entire model
## Requirements
### General
* Allowed editors:  ` vi ` ,  ` vim ` ,  ` emacs ` 
* All your files will be interpreted/compiled on Ubuntu 20.04 LTS using  ` python3 `  (version 3.8)
* Your files will be executed with  ` numpy `  (version 1.19.2) and  ` tensorflow `  (version 2.6)
* All your files should end with a new line
* The first line of all your files should be exactly  ` #!/usr/bin/env python3 ` 
* A  ` README.md `  file, at the root of the folder of the project, is mandatory
* Your code should use the  ` pycodestyle `  style (version 2.6.0)
* All your modules should have documentation ( ` python3 -c 'print(__import__("my_module").__doc__)' ` )
* All your classes should have documentation ( ` python3 -c 'print(__import__("my_module").MyClass.__doc__)' ` )
* All your functions (inside and outside a class) should have documentation ( ` python3 -c 'print(__import__("my_module").my_function.__doc__)' `  and  ` python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)' ` )
* Unless otherwise noted, you are not allowed to import any module except  ` import tensorflow.keras as K ` 
* All your files must be executable
* The length of your files will be tested using  ` wc ` 
## Videos
### Introduction
Video Player is loading.Play VideoPlayMuteCurrent Time 0:00/Duration 2:42Loaded: 1.85%0:00Stream Type LIVESeek to live, currently behind liveLIVERemaining Time -2:42 1xPlayback RateChapters* Chapters
Descriptions* descriptions off, selected
Captions* captions settings, opens captions settings dialog
* captions off, selected
Audio Track* default, selected
auto* Quality
* 0p
* 270p
* 360p
* 540p
* 720p
* 1080p
* Auto, selected
Picture-in-PictureFullscreenThis is a modal window.
Beginning of dialog window. Escape will cancel and close the window.
TextColorWhiteBlackRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentBackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentTransparentWindowColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyTransparentSemi-TransparentOpaqueFont Size50%75%100%125%150%175%200%300%400%Text Edge StyleNoneRaisedDepressedUniformDropshadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsReset restore all settings to the default valuesDoneClose Modal DialogEnd of dialog window.
Keras is an API designed for humain beings, not machines. Keras follows best practices for reducing cognitive load: it offers conscient & simple APIs, …
## Tasks
### 0. Sequential
          mandatory         Progress vs Score  Task Body Write a function   ` def build_model(nx, layers, activations, lambtha, keep_prob): `   that builds a neural network with the Keras library:
*  ` nx `  is the number of input features to the network
*  ` layers `  is a list containing the number of nodes in each layer of the network
*  ` activations `  is a list containing the activation functions used for each layer of the network
*  ` lambtha `  is the L2 regularization parameter
*  ` keep_prob `  is the probability that a node will be kept for dropout
* You are not allowed to use the  ` Input `  class
* Returns: the keras model
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 0-main.py 
#!/usr/bin/env python3

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

build_model = __import__('0-sequential').build_model

if __name__ == '__main__':
    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)
    network.summary()
    print(network.losses)
ubuntu@alexa-ml:~/0x06-keras$ ./0-main.py 
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 256)               200960    
_________________________________________________________________
dropout (Dropout)            (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               65792     
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
=================================================================
Total params: 269,322
Trainable params: 269,322
Non-trainable params: 0
_________________________________________________________________
[<tf.Tensor: shape=(), dtype=float32, numpy=0.5120259>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5118243>, <tf.Tensor: shape=(), dtype=float32, numpy=0.020181004>]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 0-sequential.py ` 
 Self-paced manual review  Panel footer - Controls 
### 1. Input
          mandatory         Progress vs Score  Task Body Write a function   ` def build_model(nx, layers, activations, lambtha, keep_prob): `   that builds a neural network with the Keras library:
*  ` nx `  is the number of input features to the network
*  ` layers `  is a list containing the number of nodes in each layer of the network
*  ` activations `  is a list containing the activation functions used for each layer of the network
*  ` lambtha `  is the L2 regularization parameter
*  ` keep_prob `  is the probability that a node will be kept for dropout
* You are not allowed to use the  ` Sequential `  class
* Returns: the keras model
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 1-main.py 
#!/usr/bin/env python3

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K
build_model = __import__('1-input').build_model

if __name__ == '__main__':
    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)
    network.summary()
    print(network.losses)
ubuntu@alexa-ml:~/0x06-keras$ ./1-main.py 
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 784)]             0         
_________________________________________________________________
dense (Dense)                (None, 256)               200960    
_________________________________________________________________
dropout (Dropout)            (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               65792     
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
=================================================================
Total params: 269,322
Trainable params: 269,322
Non-trainable params: 0
_________________________________________________________________
[<tf.Tensor: shape=(), dtype=float32, numpy=0.5120259>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5118243>, <tf.Tensor: shape=(), dtype=float32, numpy=0.020181004>]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 1-input.py ` 
 Self-paced manual review  Panel footer - Controls 
### 2. Optimize
          mandatory         Progress vs Score  Task Body Write a function   ` def optimize_model(network, alpha, beta1, beta2): `   that sets up Adam optimization for a keras model with categorical crossentropy loss and accuracy metrics:
*  ` network `  is the model to optimize
*  ` alpha `  is the learning rate
*  ` beta1 `  is the first Adam optimization parameter
*  ` beta2 `  is the second Adam optimization parameter
* Returns:  ` None ` 
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 2-main.py 
#!/usr/bin/env python3

import tensorflow as tf

build_model = __import__('1-input').build_model
optimize_model = __import__('2-optimize').optimize_model

if __name__ == '__main__':
    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)
    optimize_model(model, 0.01, 0.99, 0.9)
    print(model.loss)
    opt = model.optimizer
    print(opt.__class__)
    print(tuple(map(lambda x: x.numpy(),(opt.lr, opt.beta_1, opt.beta_2))))

ubuntu@alexa-ml:~/0x06-keras$ ./2-main.py 
categorical_crossentropy
<class 'keras.optimizer_v2.adam.Adam'>
(0.01, 0.99, 0.9)
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 2-optimize.py ` 
 Self-paced manual review  Panel footer - Controls 
### 3. One Hot
          mandatory         Progress vs Score  Task Body Write a function   ` def one_hot(labels, classes=None): `   that converts a label vector into a one-hot matrix:
* The last dimension of the one-hot matrix must be the number of classes
* Returns: the one-hot matrix
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 3-main.py 
#!/usr/bin/env python3

import numpy as np
one_hot = __import__('3-one_hot').one_hot

if __name__ == '__main__':
    labels = np.load('../data/MNIST.npz')['Y_train'][:10]
    print(labels)
    print(one_hot(labels))   
ubuntu@alexa-ml:~/0x06-keras$ ./3-main.py 
[5 0 4 1 9 2 1 3 1 4]
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 3-one_hot.py ` 
 Self-paced manual review  Panel footer - Controls 
### 4. Train
          mandatory         Progress vs Score  Task Body Write a function   ` def train_model(network, data, labels, batch_size, epochs, verbose=True, shuffle=False): `   that trains a model using mini-batch gradient descent:
*  ` network `  is the model to train
*  ` data `  is a  ` numpy.ndarray `  of shape  ` (m, nx) `  containing the input data
*  ` labels `  is a one-hot  ` numpy.ndarray `  of shape  ` (m, classes) `  containing the labels of  ` data ` 
*  ` batch_size `  is the size of the batch used for mini-batch gradient descent
*  ` epochs `  is the number of passes through  ` data `  for mini-batch gradient descent
*  ` verbose `  is a boolean that determines if output should be printed during training
*  ` shuffle `  is a boolean that determines whether to shuffle the batches every epoch. Normally, it is a good idea to shuffle, but for reproducibility, we have chosen to set the default to  ` False ` .
* Returns: the  ` History `  object generated after training the model
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 4-main.py
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
build_model = __import__('1-input').build_model
optimize_model = __import__('2-optimize').optimize_model
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('4-train').train_model


if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_train = datasets['X_train']
    X_train = X_train.reshape(X_train.shape[0], -1)
    Y_train = datasets['Y_train']
    Y_train_oh = one_hot(Y_train)

    lambtha = 0.0001
    keep_prob = 0.95
    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)
    alpha = 0.001
    beta1 = 0.9
    beta2 = 0.999
    optimize_model(network, alpha, beta1, beta2)
    batch_size = 64
    epochs = 5
    train_model(network, X_train, Y_train_oh, batch_size, epochs)
ubuntu@alexa-ml:~/0x06-keras$ ./4-main.py
Epoch 1/5
782/782 [==============================] - 5s 4ms/step - loss: 0.3536 - acc: 0.9205
Epoch 2/5
782/782 [==============================] - 3s 3ms/step - loss: 0.1948 - acc: 0.9654
Epoch 3/5
782/782 [==============================] - 3s 3ms/step - loss: 0.1568 - acc: 0.9752
Epoch 4/5
782/782 [==============================] - 3s 3ms/step - loss: 0.1365 - acc: 0.9803
Epoch 5/5
782/782 [==============================] - 3s 3ms/step - loss: 0.1269 - acc: 0.9829
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 4-train.py ` 
 Self-paced manual review  Panel footer - Controls 
### 5. Validate
          mandatory         Progress vs Score  Task Body Based on   ` 4-train.py `  , update the function  ```bash
def train_model(network, data, labels, batch_size, epochs, validation_data=None, verbose=True, shuffle=False):
```
  to also analyze validaiton data:
*  ` validation_data `  is the data to validate the model with, if not  ` None ` 
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 5-main.py 
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
build_model = __import__('1-input').build_model
optimize_model = __import__('2-optimize').optimize_model
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('5-train').train_model

if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_train = datasets['X_train']
    X_train = X_train.reshape(X_train.shape[0], -1)
    Y_train = datasets['Y_train']
    Y_train_oh = one_hot(Y_train)
    X_valid = datasets['X_valid']
    X_valid = X_valid.reshape(X_valid.shape[0], -1)
    Y_valid = datasets['Y_valid']
    Y_valid_oh = one_hot(Y_valid)

    lambtha = 0.0001
    keep_prob = 0.95
    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)
    alpha = 0.001
    beta1 = 0.9
    beta2 = 0.999
    optimize_model(network, alpha, beta1, beta2)
    batch_size = 64
    epochs = 5
    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))

ubuntu@alexa-ml:~/0x06-keras$ ./5-main.py 
Train on 50000 samples, validate on 10000 samples
Epoch 1/5
50000/50000 [==============================] - 7s 145us/step - loss: 0.3508 - acc: 0.9202 - val_loss: 0.2174 - val_acc: 0.9602
Epoch 2/5
50000/50000 [==============================] - 7s 135us/step - loss: 0.1964 - acc: 0.9660 - val_loss: 0.1772 - val_acc: 0.9702
Epoch 3/5
50000/50000 [==============================] - 7s 131us/step - loss: 0.1587 - acc: 0.9760 - val_loss: 0.1626 - val_acc: 0.9740
Epoch 4/5
50000/50000 [==============================] - 6s 129us/step - loss: 0.1374 - acc: 0.9810 - val_loss: 0.1783 - val_acc: 0.9703
Epoch 5/5
50000/50000 [==============================] - 7s 137us/step - loss: 0.1242 - acc: 0.9837 - val_loss: 0.1547 - val_acc: 0.9757
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 5-train.py ` 
 Self-paced manual review  Panel footer - Controls 
### 6. Early Stopping
          mandatory         Progress vs Score  Task Body Based on   ` 5-train.py `  , update the function  ```bash
def train_model(network, data, labels, batch_size, epochs, validation_data=None, early_stopping=False, patience=0, verbose=True, shuffle=False):
```
  to also train the model using early stopping:
*  ` early_stopping `  is a boolean that indicates whether early stopping should be used* early stopping should only be performed if  ` validation_data `  exists
* early stopping should be based on validation loss

*  ` patience `  is the patience used for early stopping 
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 6-main.py 
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
build_model = __import__('1-input').build_model
optimize_model = __import__('2-optimize').optimize_model
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('6-train').train_model


if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_train = datasets['X_train']
    X_train = X_train.reshape(X_train.shape[0], -1)
    Y_train = datasets['Y_train']
    Y_train_oh = one_hot(Y_train)
    X_valid = datasets['X_valid']
    X_valid = X_valid.reshape(X_valid.shape[0], -1)
    Y_valid = datasets['Y_valid']
    Y_valid_oh = one_hot(Y_valid)

    lambtha = 0.0001
    keep_prob = 0.95
    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)
    alpha = 0.001
    beta1 = 0.9
    beta2 = 0.999
    optimize_model(network, alpha, beta1, beta2)
    batch_size = 64
    epochs = 30
    train_model(network, X_train, Y_train_oh, batch_size, epochs,
                validation_data=(X_valid, Y_valid_oh), early_stopping=True,
                patience=3)

ubuntu@alexa-ml:~/0x06-keras$ ./6-main.py 
Epoch 1/30
782/782 [==============================] - 5s 4ms/step - loss: 0.3536 - acc: 0.9205 - val_loss: 0.2088 - val_acc: 0.9639
Epoch 2/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1949 - acc: 0.9658 - val_loss: 0.1681 - val_acc: 0.9726
Epoch 3/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1574 - acc: 0.9758 - val_loss: 0.1637 - val_acc: 0.9741
Epoch 4/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1369 - acc: 0.9804 - val_loss: 0.1562 - val_acc: 0.9760
Epoch 5/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1257 - acc: 0.9834 - val_loss: 0.1585 - val_acc: 0.9751
Epoch 6/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1151 - acc: 0.9857 - val_loss: 0.1503 - val_acc: 0.9773
Epoch 7/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1100 - acc: 0.9866 - val_loss: 0.1500 - val_acc: 0.9760
Epoch 8/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1071 - acc: 0.9875 - val_loss: 0.1395 - val_acc: 0.9790
Epoch 9/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1015 - acc: 0.9889 - val_loss: 0.1406 - val_acc: 0.9787
Epoch 10/30
782/782 [==============================] - 3s 3ms/step - loss: 0.1004 - acc: 0.9883 - val_loss: 0.1459 - val_acc: 0.9773
Epoch 11/30
782/782 [==============================] - 3s 3ms/step - loss: 0.0943 - acc: 0.9907 - val_loss: 0.1477 - val_acc: 0.9786
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 6-train.py ` 
 Self-paced manual review  Panel footer - Controls 
### 7. Learning Rate Decay
          mandatory         Progress vs Score  Task Body Based on   ` 6-train.py `  , update the function  ```bash
def train_model(network, data, labels, batch_size, epochs, validation_data=None, early_stopping=False, patience=0, learning_rate_decay=False, alpha=0.1, decay_rate=1, verbose=True, shuffle=False):
```
  to also train the model with learning rate decay:
*  ` learning_rate_decay `  is a boolean that indicates whether learning rate decay should be used* learning rate decay should only be performed if  ` validation_data `  exists
* the decay should be performed using inverse time decay
* the learning rate should decay in a stepwise fashion after each epoch
* each time the learning rate updates,  ` Keras `  should print a message

*  ` alpha `  is the initial learning rate
*  ` decay_rate `  is the decay rate
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 7-main.py 
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
build_model = __import__('1-input').build_model
optimize_model = __import__('2-optimize').optimize_model
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('7-train').train_model 

if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_train = datasets['X_train']
    X_train = X_train.reshape(X_train.shape[0], -1)
    Y_train = datasets['Y_train']
    Y_train_oh = one_hot(Y_train)
    X_valid = datasets['X_valid']
    X_valid = X_valid.reshape(X_valid.shape[0], -1)
    Y_valid = datasets['Y_valid']
    Y_valid_oh = one_hot(Y_valid)

    lambtha = 0.0001
    keep_prob = 0.95
    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)
    alpha = 0.001
    beta1 = 0.9
    beta2 = 0.999
    optimize_model(network, alpha, beta1, beta2)
    batch_size = 64
    epochs = 1000
    train_model(network, X_train, Y_train_oh, batch_size, epochs,
                validation_data=(X_valid, Y_valid_oh), early_stopping=True,
                patience=3, learning_rate_decay=True, alpha=alpha)

ubuntu@alexa-ml:~/0x06-keras$ ./7-main.py
Epoch 1/1000

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
782/782 [==============================] - 5s 4ms/step - loss: 0.3536 - acc: 0.9205 - val_loss: 0.2088 - val_acc: 0.9639
Epoch 2/1000

Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.
782/782 [==============================] - 3s 4ms/step - loss: 0.1803 - acc: 0.9703 - val_loss: 0.1642 - val_acc: 0.9744
Epoch 3/1000

Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.
782/782 [==============================] - 3s 4ms/step - loss: 0.1447 - acc: 0.9800 - val_loss: 0.1522 - val_acc: 0.9768
Epoch 4/1000

Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.
782/782 [==============================] - 3s 4ms/step - loss: 0.1262 - acc: 0.9844 - val_loss: 0.1449 - val_acc: 0.9790
Epoch 5/1000

...

Epoch 00043: LearningRateScheduler setting learning rate to 2.3255813953488374e-05.
782/782 [==============================] - 3s 3ms/step - loss: 0.0564 - acc: 0.9990 - val_loss: 0.1059 - val_acc: 0.9827
Epoch 44/1000

Epoch 00044: LearningRateScheduler setting learning rate to 2.272727272727273e-05.
782/782 [==============================] - 3s 3ms/step - loss: 0.0561 - acc: 0.9988 - val_loss: 0.1061 - val_acc: 0.9829
Epoch 45/1000

Epoch 00045: LearningRateScheduler setting learning rate to 2.2222222222222223e-05.
782/782 [==============================] - 3s 3ms/step - loss: 0.0559 - acc: 0.9988 - val_loss: 0.1063 - val_acc: 0.9828 
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 7-train.py ` 
 Self-paced manual review  Panel footer - Controls 
### 8. Save Only the Best
          mandatory         Progress vs Score  Task Body Based on   ` 7-train.py `  , update the function  ```bash
def train_model(network, data, labels, batch_size, epochs, validation_data=None, early_stopping=False, patience=0, learning_rate_decay=False, alpha=0.1, decay_rate=1, save_best=False, filepath=None, verbose=True, shuffle=False):
```
  to also save the best iteration of the model:
*  ` save_best `  is a boolean indicating whether to save the model after each epoch if it is the best* a model is considered the best if its validation loss is the lowest that the model has obtained

*  ` filepath `  is the file path where the model should be saved
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 8-main.py 
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
build_model = __import__('1-input').build_model
optimize_model = __import__('2-optimize').optimize_model
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('8-train').train_model 


if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_train = datasets['X_train']
    X_train = X_train.reshape(X_train.shape[0], -1)
    Y_train = datasets['Y_train']
    Y_train_oh = one_hot(Y_train)
    X_valid = datasets['X_valid']
    X_valid = X_valid.reshape(X_valid.shape[0], -1)
    Y_valid = datasets['Y_valid']
    Y_valid_oh = one_hot(Y_valid)

    lambtha = 0.0001
    keep_prob = 0.95
    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)
    alpha = 0.001
    beta1 = 0.9
    beta2 = 0.999
    optimize_model(network, alpha, beta1, beta2)
    batch_size = 64
    epochs = 1000
    train_model(network, X_train, Y_train_oh, batch_size, epochs,
                validation_data=(X_valid, Y_valid_oh), early_stopping=True,
                patience=3, learning_rate_decay=True, alpha=alpha,
                save_best=True, filepath='network1.h5')

ubuntu@alexa-ml:~/0x06-keras$ ./8-main.py 
Epoch 1/1000

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
782/782 [==============================] - 4s 4ms/step - loss: 0.3536 - acc: 0.9205 - val_loss: 0.2088 - val_acc: 0.9639
Epoch 2/1000

Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.
782/782 [==============================] - 3s 4ms/step - loss: 0.1803 - acc: 0.9703 - val_loss: 0.1642 - val_acc: 0.9744
Epoch 3/1000

Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.
782/782 [==============================] - 3s 4ms/step - loss: 0.1447 - acc: 0.9799 - val_loss: 0.1523 - val_acc: 0.9764
Epoch 4/1000

Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.
782/782 [==============================] - 3s 4ms/step - loss: 0.1263 - acc: 0.9843 - val_loss: 0.1448 - val_acc: 0.9791
Epoch 5/1000

...

Epoch 00056: LearningRateScheduler setting learning rate to 1.785714285714286e-05.
782/782 [==============================] - 3s 3ms/step - loss: 0.0527 - acc: 0.9992 - val_loss: 0.1030 - val_acc: 0.9827
Epoch 57/1000

Epoch 00057: LearningRateScheduler setting learning rate to 1.7543859649122806e-05.
782/782 [==============================] - 3s 3ms/step - loss: 0.0527 - acc: 0.9991 - val_loss: 0.1024 - val_acc: 0.9830
ubuntu@alexa-ml:~/0x06-keras$ ls network1.h5 
network1.h5
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 8-train.py ` 
 Self-paced manual review  Panel footer - Controls 
### 9. Save and Load Model
          mandatory         Progress vs Score  Task Body Write the following functions:
*  ` def save_model(network, filename): `  saves an entire model:*  ` network `  is the model to save
*  ` filename `  is the path of the file that the model should be saved to
* Returns:  ` None ` 

*  ` def load_model(filename): `  loads an entire model:*  ` filename `  is the path of the file that the model should be loaded from
* Returns: the loaded model

```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 9-main.py
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('8-train').train_model 
model = __import__('9-model')

if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_train = datasets['X_train']
    X_train = X_train.reshape(X_train.shape[0], -1)
    Y_train = datasets['Y_train']
    Y_train_oh = one_hot(Y_train)
    X_valid = datasets['X_valid']
    X_valid = X_valid.reshape(X_valid.shape[0], -1)
    Y_valid = datasets['Y_valid']
    Y_valid_oh = one_hot(Y_valid)

    network = model.load_model('network1.h5')
    batch_size = 32
    epochs = 1000
    train_model(network, X_train, Y_train_oh, batch_size, epochs,
                validation_data=(X_valid, Y_valid_oh), early_stopping=True,
                patience=2, learning_rate_decay=True, alpha=0.001)
    model.save_model(network, 'network2.h5')
    network.summary()
    print(network.get_weights())
    del network

    network2 = model.load_model('network2.h5')
    network2.summary()
    print(network2.get_weights())

ubuntu@alexa-ml:~/0x06-keras$ ./9-main.py
Epoch 1/1000

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
1563/1563 [==============================] - 8s 4ms/step - loss: 0.1841 - acc: 0.9631 - val_loss: 0.1633 - val_acc: 0.9717
Epoch 2/1000

Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.
1563/1563 [==============================] - 6s 4ms/step - loss: 0.1061 - acc: 0.9865 - val_loss: 0.1317 - val_acc: 0.9789
Epoch 3/1000

Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.
1563/1563 [==============================] - 7s 4ms/step - loss: 0.0848 - acc: 0.9916 - val_loss: 0.1219 - val_acc: 0.9805
Epoch 4/1000

Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.
1563/1563 [==============================] - 6s 4ms/step - loss: 0.0736 - acc: 0.9945 - val_loss: 0.1190 - val_acc: 0.9808
Epoch 5/1000

Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.
1563/1563 [==============================] - 6s 4ms/step - loss: 0.0673 - acc: 0.9957 - val_loss: 0.1169 - val_acc: 0.9819
Epoch 6/1000

Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.
1563/1563 [==============================] - 6s 4ms/step - loss: 0.0631 - acc: 0.9963 - val_loss: 0.1110 - val_acc: 0.9832
Epoch 7/1000

Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.
1563/1563 [==============================] - 6s 4ms/step - loss: 0.0586 - acc: 0.9977 - val_loss: 0.1107 - val_acc: 0.9825
Epoch 8/1000

Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0568 - acc: 0.9976 - val_loss: 0.1083 - val_acc: 0.9828
Epoch 9/1000

Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0545 - acc: 0.9983 - val_loss: 0.1070 - val_acc: 0.9829
Epoch 10/1000

Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0525 - acc: 0.9982 - val_loss: 0.1044 - val_acc: 0.9831
Epoch 11/1000

Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0513 - acc: 0.9985 - val_loss: 0.1036 - val_acc: 0.9828
Epoch 12/1000

Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0502 - acc: 0.9987 - val_loss: 0.1022 - val_acc: 0.9839
Epoch 13/1000

Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0489 - acc: 0.9989 - val_loss: 0.1026 - val_acc: 0.9831
Epoch 14/1000

Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0478 - acc: 0.9990 - val_loss: 0.0987 - val_acc: 0.9828
Epoch 15/1000

Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0471 - acc: 0.9988 - val_loss: 0.0989 - val_acc: 0.9836
Epoch 16/1000

Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0467 - acc: 0.9987 - val_loss: 0.0968 - val_acc: 0.9842
Epoch 17/1000

Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0459 - acc: 0.9989 - val_loss: 0.0957 - val_acc: 0.9840
Epoch 18/1000

Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0454 - acc: 0.9989 - val_loss: 0.0970 - val_acc: 0.9841
Epoch 19/1000

Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.
1563/1563 [==============================] - 5s 3ms/step - loss: 0.0448 - acc: 0.9991 - val_loss: 0.0972 - val_acc: 0.9834
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 784)]             0
_________________________________________________________________
dense (Dense)                (None, 256)               200960
_________________________________________________________________
dropout (Dropout)            (None, 256)               0
_________________________________________________________________
dense_1 (Dense)              (None, 256)               65792
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570
=================================================================
Total params: 269,322
Trainable params: 269,322
Non-trainable params: 0
_________________________________________________________________
[array([[-5.2766692e-34, -4.8469041e-33, -4.5029721e-32, ...,
         5.3849467e-32,  3.5366846e-32, -8.8805072e-33],
       [ 5.3237510e-32,  5.7210311e-33, -6.5247225e-33, ...,
        -1.4632824e-32, -4.9282095e-32,  4.7386293e-33],
       [-7.1492310e-33,  4.2906925e-32, -4.7658579e-32, ...,
         9.0034874e-33,  4.3518288e-32, -2.6758248e-32],
       ...,
       [ 1.2808156e-32,  4.4801863e-32, -5.6753514e-32, ...,
         7.7905256e-33,  1.4356277e-32, -2.7032432e-32],
       [ 4.7664503e-32,  8.5089334e-33,  2.6985013e-32, ...,
        -4.7109364e-32,  3.1981172e-32,  5.0471966e-33],
       [ 8.8359207e-33, -5.0076835e-32,  1.6378386e-32, ...,
         4.9723999e-33,  4.6501740e-32,  4.8748133e-32]], dtype=float32), array([ 2.42055748e-02,  4.97336127e-02, -1.31449163e-01, -3.48263904e-02,
        3.38328965e-02,  8.94753914e-03, -3.32898926e-03,  8.62065889e-03,
        1.13340266e-01,  1.13596939e-01, -3.45221981e-02, -8.94739106e-02,
        1.17520526e-01, -3.92694436e-02,  2.54333001e-02,  3.96549702e-02,
        3.62765752e-02,  1.03379473e-01,  5.12550324e-02, -1.53140957e-02,
        3.25467549e-02,  7.13156164e-02,  7.41765946e-02,  2.84082606e-03,
       -1.70718804e-02, -6.67905062e-02, -7.24370256e-02, -3.09881531e-02,
        4.12718989e-02,  4.31933999e-02,  6.86500371e-02, -2.52993274e-02,
       -8.67424719e-03,  4.20734212e-02,  2.75987908e-02,  4.17117178e-02,
        6.57536685e-02, -7.55800307e-02, -8.00409447e-03, -4.41815220e-02,
        1.06031306e-01, -6.16804883e-03, -5.61822802e-02, -1.34118628e-02,
       -7.21768430e-03, -1.67810090e-03,  3.80131602e-02, -2.56150216e-03,
       -2.15920210e-02, -3.67936231e-02, -1.57706484e-01, -4.93261144e-02,
        3.43423039e-02, -1.09447101e-02,  1.26106478e-02, -4.99122590e-02,
       -2.03058310e-02,  1.18195854e-01, -8.57081711e-02, -2.40898635e-02,
       -1.38498889e-02,  8.14961717e-02,  6.17014663e-03, -2.50044595e-02,
        5.88443540e-02, -6.71132579e-02,  3.02182473e-02, -3.57209035e-04,
        6.04537129e-03, -4.49447483e-02,  1.21123768e-01, -1.50266513e-02,
        2.98477411e-02,  1.56357229e-01,  7.42858127e-02, -1.37053197e-02,
       -1.92796178e-02, -6.30884722e-04, -9.14448500e-03,  7.78798535e-02,
        4.73481882e-03, -5.00369817e-02,  2.01115441e-02,  4.91318516e-02,
        6.89878613e-02, -7.41709897e-04,  4.10465077e-02,  5.66071868e-02,
       -2.91831605e-02, -1.24142794e-02,  1.59813855e-02, -5.82990013e-02,
       -1.46004912e-02, -7.04847230e-03, -1.53810484e-02, -3.53730768e-02,
       -2.62593385e-02, -3.42618972e-02, -6.40405994e-03,  2.41093263e-02,
        6.69784099e-02,  1.22419134e-01,  1.40971853e-03, -3.65416706e-02,
       -6.05292656e-02,  2.94122733e-02,  2.28075054e-03,  1.54874530e-02,
        2.44536772e-02,  4.10329774e-02, -6.52046949e-02,  2.84271035e-02,
        1.30639868e-02,  9.24794748e-02,  2.55401842e-02, -1.26035213e-02,
       -2.12160926e-02, -3.07128485e-02,  1.20450005e-01,  3.44891809e-02,
        9.42492764e-03, -4.62370738e-02, -4.61713448e-02, -5.96997105e-02,
        5.24189286e-02, -1.08038336e-01, -3.57228667e-02, -6.09593131e-02,
       -1.27689177e-02,  6.05372898e-02, -8.68709981e-02, -1.77335870e-02,
        1.10579222e-01,  3.58308130e-03,  6.44926131e-02,  2.26106066e-02,
       -1.23218082e-01,  1.50772229e-01,  3.68883461e-02,  1.78936440e-02,
       -1.62284579e-02,  2.71826703e-02, -1.41632140e-01, -2.41718553e-02,
       -1.47164920e-02, -1.56504184e-03, -8.02510604e-03, -5.70992380e-02,
        4.42996109e-03, -3.61215211e-02,  7.25488039e-03, -3.61059383e-02,
       -1.67860463e-02, -1.43339010e-02, -5.86435348e-02, -5.77094555e-02,
        6.61153495e-02, -1.86597425e-02, -5.48199890e-03, -2.26344187e-02,
        2.32470129e-02, -3.16553563e-02,  1.38850778e-01,  5.08232377e-02,
       -5.55736646e-02, -3.11112832e-02, -1.68804917e-02, -4.78236228e-02,
        8.85246415e-03,  3.73947397e-02, -6.33736998e-02, -3.13683078e-02,
       -5.55279292e-02, -1.91320153e-03, -7.63542280e-02,  3.06050871e-02,
        2.08393075e-02, -2.38516252e-03, -4.03844751e-03, -6.20031208e-02,
       -3.11615635e-02,  2.10694913e-02, -1.69931185e-02, -1.22885905e-01,
       -5.40405363e-02,  9.48599279e-02, -1.46141701e-05,  2.44354960e-02,
       -6.36112615e-02, -1.85025558e-02,  1.10179268e-01,  2.54274365e-02,
       -1.59756131e-02,  5.80502255e-03,  3.50507423e-02, -2.47005634e-02,
        2.37329677e-02,  1.61884520e-02, -6.23314828e-03, -5.45048378e-02,
       -1.45113319e-02,  3.74473259e-02,  4.51476574e-02,  4.61748466e-02,
        2.49474179e-02,  5.19658923e-02, -4.17965725e-02, -3.74812558e-02,
        2.69362181e-02,  7.13354498e-02,  1.19988434e-01, -7.17105940e-02,
        4.34395261e-02,  2.69286260e-02, -1.98554760e-03, -1.25212921e-02,
        4.57909796e-03,  1.14607428e-04,  4.33304757e-02,  7.02673495e-02,
        4.33294699e-02, -3.10730692e-02, -4.91303429e-02, -4.43950891e-02,
        8.53582248e-02,  6.49776459e-02, -2.55251769e-02, -4.71857861e-02,
       -1.97647642e-02, -1.07442155e-01, -3.75369266e-02,  5.64541370e-02,
        6.39467558e-04,  4.29542823e-04, -4.01261784e-02,  9.82200634e-03,
        1.86100341e-02, -5.52693531e-02,  5.80316670e-02,  1.01340197e-01,
        1.27654761e-01, -2.42472347e-02, -4.65300158e-02, -1.27472915e-02,
       -5.06722415e-03, -7.26248622e-02,  1.37946010e-01,  1.84302162e-02,
       -9.16350633e-02,  1.74229371e-03, -4.09353971e-02,  7.93040097e-02,
        9.29820910e-02,  2.79681641e-03, -1.35056628e-03, -2.48960941e-03],
      dtype=float32), array([[ 1.90781970e-02, -9.31159779e-03,  5.60704172e-02, ...,
        -1.14697311e-02,  7.66689423e-03,  7.87436310e-03],
       [ 5.11376187e-02, -1.64989606e-02,  1.52302729e-02, ...,
         3.87132987e-02, -1.69848874e-02,  6.08293787e-02],
       [ 2.03613075e-03, -7.25060105e-02,  1.29213445e-02, ...,
        -4.64127697e-02, -2.96940506e-02,  5.07990420e-02],
       ...,
       [ 6.87275231e-02,  4.87157056e-04,  3.81113612e-03, ...,
        -6.01525418e-03, -4.17754054e-02,  8.12370181e-02],
       [ 6.26001862e-28,  1.61327321e-12, -2.38097835e-22, ...,
         8.20580706e-25, -1.04949193e-19,  8.59037164e-22],
       [ 4.65337411e-02,  5.54108508e-02, -1.37382867e-02, ...,
         6.19266927e-02, -6.61780611e-02,  8.66155177e-02]], dtype=float32), array([-0.01868563,  0.1996879 , -0.00895025,  0.0173578 ,  0.10023709,
       -0.03503804, -0.04807546,  0.01893422,  0.14127974, -0.04424038,
        0.03609959,  0.10092543, -0.0115434 ,  0.04887414, -0.04527254,
       -0.01631377, -0.03616309,  0.04294399,  0.08644044,  0.03289045,
        0.14667207, -0.0580859 ,  0.0506365 ,  0.00165932,  0.03349096,
       -0.06461567,  0.04024672,  0.02451268,  0.03542558,  0.09147327,
        0.13787933, -0.01283798,  0.08955868, -0.05454201,  0.06415799,
        0.04712304,  0.00770633,  0.166734  ,  0.02794588,  0.15199189,
        0.11747128,  0.01486994,  0.02871312,  0.05449404, -0.03602317,
       -0.01333874,  0.08274704,  0.09028156,  0.00539852,  0.17305732,
        0.03388352,  0.02410749,  0.03252971,  0.0992787 , -0.0329654 ,
        0.10095967,  0.09207146,  0.18567972, -0.03686088,  0.08430403,
        0.00457946,  0.12816532,  0.07731209,  0.07737107,  0.06228883,
        0.00870382, -0.01480328,  0.00862901,  0.04710835,  0.12403201,
        0.19892661, -0.03729338,  0.04397079, -0.04526563,  0.0771606 ,
        0.0039387 , -0.04273853,  0.01281196, -0.00307695,  0.10373595,
        0.03555368,  0.04544834,  0.01787842,  0.02592109,  0.11883164,
        0.06086406,  0.01459295,  0.13227804,  0.11683677,  0.13622735,
        0.00235192, -0.02328737, -0.03794846,  0.0938736 , -0.06584721,
        0.13379525,  0.1438743 ,  0.1263114 ,  0.12829179, -0.08921356,
        0.02283293, -0.01118934, -0.00943715, -0.05311007, -0.03138095,
       -0.02292866,  0.0263631 ,  0.14976385,  0.00199215,  0.01065395,
        0.05893238, -0.00224998,  0.19775297, -0.05182957, -0.05962858,
       -0.02659106, -0.01557225, -0.05956717,  0.12984088, -0.05588398,
        0.01537142,  0.03161344,  0.09835552,  0.01060282, -0.00357315,
       -0.0316791 ,  0.12336725,  0.1072735 ,  0.10220348,  0.12345371,
        0.12902246, -0.00485598,  0.00633773,  0.13062255,  0.06457694,
        0.1543006 ,  0.09414724,  0.01904248,  0.1222837 ,  0.08334316,
        0.02941048,  0.04507583,  0.0428214 ,  0.1258853 , -0.04674396,
       -0.00865523,  0.01828937,  0.14067617, -0.00678011,  0.09859391,
        0.20188846, -0.03063613, -0.02241488,  0.10228955, -0.0091732 ,
        0.09561207, -0.04075339,  0.01657417,  0.01278578,  0.1749786 ,
        0.14774854,  0.07352936,  0.0938032 ,  0.10489926, -0.04541885,
        0.03370849,  0.10673981,  0.07275312,  0.00319357, -0.02043473,
       -0.03737572,  0.01174216,  0.07646789,  0.13456808,  0.10993656,
       -0.00711449,  0.05465424,  0.07678176, -0.01130569,  0.13136199,
        0.12488808,  0.12369888,  0.06577432,  0.13954146, -0.0369533 ,
        0.07225734,  0.12732355,  0.06783256,  0.10996079,  0.15201433,
        0.04795463, -0.02891155,  0.10057276, -0.01139818, -0.07155777,
       -0.02464746,  0.07380133, -0.07539964, -0.0370912 , -0.01562413,
       -0.02480589,  0.07312354,  0.02197213,  0.11165579,  0.12200221,
        0.0534094 , -0.01725672,  0.1233487 ,  0.15092307,  0.06231683,
        0.02700295,  0.00486124,  0.02487769, -0.07535981,  0.1105212 ,
        0.01117107,  0.01503461,  0.00755382, -0.05963881,  0.07816348,
        0.04380285, -0.09999963, -0.03154609,  0.02469472,  0.16588104,
        0.03116903,  0.11040416,  0.09537021,  0.16924927,  0.14196277,
       -0.08147729, -0.0310944 ,  0.01578016,  0.06446993, -0.00770771,
        0.04072778,  0.09360179,  0.1949345 , -0.08859233, -0.06656316,
       -0.04537171,  0.12998222,  0.1007067 ,  0.06708504,  0.04702223,
        0.05339693,  0.07188582, -0.00751081, -0.01630633,  0.1001667 ,
       -0.02479191,  0.13797905,  0.13946712, -0.0132787 ,  0.0645536 ,
        0.00065998], dtype=float32), array([[-0.11318333, -0.01394981, -0.08518942, ...,  0.17544281,
         0.01231085, -0.19481356],
       [-0.01007743, -0.26932693,  0.0530858 , ..., -0.15927719,
         0.2543153 ,  0.02261621],
       [ 0.14198484,  0.05500012, -0.15798448, ...,  0.30319262,
        -0.03104973,  0.02341664],
       ...,
       [-0.00364847,  0.09823479, -0.14419091, ..., -0.05911801,
        -0.16701627,  0.12995604],
       [ 0.24357872, -0.12123701,  0.08571428, ..., -0.07280003,
         0.03248561,  0.20008057],
       [-0.20092683, -0.08602812, -0.05572913, ...,  0.2714223 ,
        -0.06357553, -0.29062647]], dtype=float32), array([-0.01019929, -0.10848439,  0.01929379, -0.01769626, -0.00433212,
       -0.03878876, -0.02930045, -0.08742885,  0.1831188 ,  0.00656276],
      dtype=float32)]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 784)]             0
_________________________________________________________________
dense (Dense)                (None, 256)               200960
_________________________________________________________________
dropout (Dropout)            (None, 256)               0
_________________________________________________________________
dense_1 (Dense)              (None, 256)               65792
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570
=================================================================
Total params: 269,322
Trainable params: 269,322
Non-trainable params: 0
_________________________________________________________________
[array([[-5.2766692e-34, -4.8469041e-33, -4.5029721e-32, ...,
         5.3849467e-32,  3.5366846e-32, -8.8805072e-33],
       [ 5.3237510e-32,  5.7210311e-33, -6.5247225e-33, ...,
        -1.4632824e-32, -4.9282095e-32,  4.7386293e-33],
       [-7.1492310e-33,  4.2906925e-32, -4.7658579e-32, ...,
         9.0034874e-33,  4.3518288e-32, -2.6758248e-32],
       ...,
       [ 1.2808156e-32,  4.4801863e-32, -5.6753514e-32, ...,
         7.7905256e-33,  1.4356277e-32, -2.7032432e-32],
       [ 4.7664503e-32,  8.5089334e-33,  2.6985013e-32, ...,
        -4.7109364e-32,  3.1981172e-32,  5.0471966e-33],
       [ 8.8359207e-33, -5.0076835e-32,  1.6378386e-32, ...,
         4.9723999e-33,  4.6501740e-32,  4.8748133e-32]], dtype=float32), array([ 2.42055748e-02,  4.97336127e-02, -1.31449163e-01, -3.48263904e-02,
        3.38328965e-02,  8.94753914e-03, -3.32898926e-03,  8.62065889e-03,
        1.13340266e-01,  1.13596939e-01, -3.45221981e-02, -8.94739106e-02,
        1.17520526e-01, -3.92694436e-02,  2.54333001e-02,  3.96549702e-02,
        3.62765752e-02,  1.03379473e-01,  5.12550324e-02, -1.53140957e-02,
        3.25467549e-02,  7.13156164e-02,  7.41765946e-02,  2.84082606e-03,
       -1.70718804e-02, -6.67905062e-02, -7.24370256e-02, -3.09881531e-02,
        4.12718989e-02,  4.31933999e-02,  6.86500371e-02, -2.52993274e-02,
       -8.67424719e-03,  4.20734212e-02,  2.75987908e-02,  4.17117178e-02,
        6.57536685e-02, -7.55800307e-02, -8.00409447e-03, -4.41815220e-02,
        1.06031306e-01, -6.16804883e-03, -5.61822802e-02, -1.34118628e-02,
       -7.21768430e-03, -1.67810090e-03,  3.80131602e-02, -2.56150216e-03,
       -2.15920210e-02, -3.67936231e-02, -1.57706484e-01, -4.93261144e-02,
        3.43423039e-02, -1.09447101e-02,  1.26106478e-02, -4.99122590e-02,
       -2.03058310e-02,  1.18195854e-01, -8.57081711e-02, -2.40898635e-02,
       -1.38498889e-02,  8.14961717e-02,  6.17014663e-03, -2.50044595e-02,
        5.88443540e-02, -6.71132579e-02,  3.02182473e-02, -3.57209035e-04,
        6.04537129e-03, -4.49447483e-02,  1.21123768e-01, -1.50266513e-02,
        2.98477411e-02,  1.56357229e-01,  7.42858127e-02, -1.37053197e-02,
       -1.92796178e-02, -6.30884722e-04, -9.14448500e-03,  7.78798535e-02,
        4.73481882e-03, -5.00369817e-02,  2.01115441e-02,  4.91318516e-02,
        6.89878613e-02, -7.41709897e-04,  4.10465077e-02,  5.66071868e-02,
       -2.91831605e-02, -1.24142794e-02,  1.59813855e-02, -5.82990013e-02,
       -1.46004912e-02, -7.04847230e-03, -1.53810484e-02, -3.53730768e-02,
       -2.62593385e-02, -3.42618972e-02, -6.40405994e-03,  2.41093263e-02,
        6.69784099e-02,  1.22419134e-01,  1.40971853e-03, -3.65416706e-02,
       -6.05292656e-02,  2.94122733e-02,  2.28075054e-03,  1.54874530e-02,
        2.44536772e-02,  4.10329774e-02, -6.52046949e-02,  2.84271035e-02,
        1.30639868e-02,  9.24794748e-02,  2.55401842e-02, -1.26035213e-02,
       -2.12160926e-02, -3.07128485e-02,  1.20450005e-01,  3.44891809e-02,
        9.42492764e-03, -4.62370738e-02, -4.61713448e-02, -5.96997105e-02,
        5.24189286e-02, -1.08038336e-01, -3.57228667e-02, -6.09593131e-02,
       -1.27689177e-02,  6.05372898e-02, -8.68709981e-02, -1.77335870e-02,
        1.10579222e-01,  3.58308130e-03,  6.44926131e-02,  2.26106066e-02,
       -1.23218082e-01,  1.50772229e-01,  3.68883461e-02,  1.78936440e-02,
       -1.62284579e-02,  2.71826703e-02, -1.41632140e-01, -2.41718553e-02,
       -1.47164920e-02, -1.56504184e-03, -8.02510604e-03, -5.70992380e-02,
        4.42996109e-03, -3.61215211e-02,  7.25488039e-03, -3.61059383e-02,
       -1.67860463e-02, -1.43339010e-02, -5.86435348e-02, -5.77094555e-02,
        6.61153495e-02, -1.86597425e-02, -5.48199890e-03, -2.26344187e-02,
        2.32470129e-02, -3.16553563e-02,  1.38850778e-01,  5.08232377e-02,
       -5.55736646e-02, -3.11112832e-02, -1.68804917e-02, -4.78236228e-02,
        8.85246415e-03,  3.73947397e-02, -6.33736998e-02, -3.13683078e-02,
       -5.55279292e-02, -1.91320153e-03, -7.63542280e-02,  3.06050871e-02,
        2.08393075e-02, -2.38516252e-03, -4.03844751e-03, -6.20031208e-02,
       -3.11615635e-02,  2.10694913e-02, -1.69931185e-02, -1.22885905e-01,
       -5.40405363e-02,  9.48599279e-02, -1.46141701e-05,  2.44354960e-02,
       -6.36112615e-02, -1.85025558e-02,  1.10179268e-01,  2.54274365e-02,
       -1.59756131e-02,  5.80502255e-03,  3.50507423e-02, -2.47005634e-02,
        2.37329677e-02,  1.61884520e-02, -6.23314828e-03, -5.45048378e-02,
       -1.45113319e-02,  3.74473259e-02,  4.51476574e-02,  4.61748466e-02,
        2.49474179e-02,  5.19658923e-02, -4.17965725e-02, -3.74812558e-02,
        2.69362181e-02,  7.13354498e-02,  1.19988434e-01, -7.17105940e-02,
        4.34395261e-02,  2.69286260e-02, -1.98554760e-03, -1.25212921e-02,
        4.57909796e-03,  1.14607428e-04,  4.33304757e-02,  7.02673495e-02,
        4.33294699e-02, -3.10730692e-02, -4.91303429e-02, -4.43950891e-02,
        8.53582248e-02,  6.49776459e-02, -2.55251769e-02, -4.71857861e-02,
       -1.97647642e-02, -1.07442155e-01, -3.75369266e-02,  5.64541370e-02,
        6.39467558e-04,  4.29542823e-04, -4.01261784e-02,  9.82200634e-03,
        1.86100341e-02, -5.52693531e-02,  5.80316670e-02,  1.01340197e-01,
        1.27654761e-01, -2.42472347e-02, -4.65300158e-02, -1.27472915e-02,
       -5.06722415e-03, -7.26248622e-02,  1.37946010e-01,  1.84302162e-02,
       -9.16350633e-02,  1.74229371e-03, -4.09353971e-02,  7.93040097e-02,
        9.29820910e-02,  2.79681641e-03, -1.35056628e-03, -2.48960941e-03],
      dtype=float32), array([[ 1.90781970e-02, -9.31159779e-03,  5.60704172e-02, ...,
        -1.14697311e-02,  7.66689423e-03,  7.87436310e-03],
       [ 5.11376187e-02, -1.64989606e-02,  1.52302729e-02, ...,
         3.87132987e-02, -1.69848874e-02,  6.08293787e-02],
       [ 2.03613075e-03, -7.25060105e-02,  1.29213445e-02, ...,
        -4.64127697e-02, -2.96940506e-02,  5.07990420e-02],
       ...,
       [ 6.87275231e-02,  4.87157056e-04,  3.81113612e-03, ...,
        -6.01525418e-03, -4.17754054e-02,  8.12370181e-02],
       [ 6.26001862e-28,  1.61327321e-12, -2.38097835e-22, ...,
         8.20580706e-25, -1.04949193e-19,  8.59037164e-22],
       [ 4.65337411e-02,  5.54108508e-02, -1.37382867e-02, ...,
         6.19266927e-02, -6.61780611e-02,  8.66155177e-02]], dtype=float32), array([-0.01868563,  0.1996879 , -0.00895025,  0.0173578 ,  0.10023709,
       -0.03503804, -0.04807546,  0.01893422,  0.14127974, -0.04424038,
        0.03609959,  0.10092543, -0.0115434 ,  0.04887414, -0.04527254,
       -0.01631377, -0.03616309,  0.04294399,  0.08644044,  0.03289045,
        0.14667207, -0.0580859 ,  0.0506365 ,  0.00165932,  0.03349096,
       -0.06461567,  0.04024672,  0.02451268,  0.03542558,  0.09147327,
        0.13787933, -0.01283798,  0.08955868, -0.05454201,  0.06415799,
        0.04712304,  0.00770633,  0.166734  ,  0.02794588,  0.15199189,
        0.11747128,  0.01486994,  0.02871312,  0.05449404, -0.03602317,
       -0.01333874,  0.08274704,  0.09028156,  0.00539852,  0.17305732,
        0.03388352,  0.02410749,  0.03252971,  0.0992787 , -0.0329654 ,
        0.10095967,  0.09207146,  0.18567972, -0.03686088,  0.08430403,
        0.00457946,  0.12816532,  0.07731209,  0.07737107,  0.06228883,
        0.00870382, -0.01480328,  0.00862901,  0.04710835,  0.12403201,
        0.19892661, -0.03729338,  0.04397079, -0.04526563,  0.0771606 ,
        0.0039387 , -0.04273853,  0.01281196, -0.00307695,  0.10373595,
        0.03555368,  0.04544834,  0.01787842,  0.02592109,  0.11883164,
        0.06086406,  0.01459295,  0.13227804,  0.11683677,  0.13622735,
        0.00235192, -0.02328737, -0.03794846,  0.0938736 , -0.06584721,
        0.13379525,  0.1438743 ,  0.1263114 ,  0.12829179, -0.08921356,
        0.02283293, -0.01118934, -0.00943715, -0.05311007, -0.03138095,
       -0.02292866,  0.0263631 ,  0.14976385,  0.00199215,  0.01065395,
        0.05893238, -0.00224998,  0.19775297, -0.05182957, -0.05962858,
       -0.02659106, -0.01557225, -0.05956717,  0.12984088, -0.05588398,
        0.01537142,  0.03161344,  0.09835552,  0.01060282, -0.00357315,
       -0.0316791 ,  0.12336725,  0.1072735 ,  0.10220348,  0.12345371,
        0.12902246, -0.00485598,  0.00633773,  0.13062255,  0.06457694,
        0.1543006 ,  0.09414724,  0.01904248,  0.1222837 ,  0.08334316,
        0.02941048,  0.04507583,  0.0428214 ,  0.1258853 , -0.04674396,
       -0.00865523,  0.01828937,  0.14067617, -0.00678011,  0.09859391,
        0.20188846, -0.03063613, -0.02241488,  0.10228955, -0.0091732 ,
        0.09561207, -0.04075339,  0.01657417,  0.01278578,  0.1749786 ,
        0.14774854,  0.07352936,  0.0938032 ,  0.10489926, -0.04541885,
        0.03370849,  0.10673981,  0.07275312,  0.00319357, -0.02043473,
       -0.03737572,  0.01174216,  0.07646789,  0.13456808,  0.10993656,
       -0.00711449,  0.05465424,  0.07678176, -0.01130569,  0.13136199,
        0.12488808,  0.12369888,  0.06577432,  0.13954146, -0.0369533 ,
        0.07225734,  0.12732355,  0.06783256,  0.10996079,  0.15201433,
        0.04795463, -0.02891155,  0.10057276, -0.01139818, -0.07155777,
       -0.02464746,  0.07380133, -0.07539964, -0.0370912 , -0.01562413,
       -0.02480589,  0.07312354,  0.02197213,  0.11165579,  0.12200221,
        0.0534094 , -0.01725672,  0.1233487 ,  0.15092307,  0.06231683,
        0.02700295,  0.00486124,  0.02487769, -0.07535981,  0.1105212 ,
        0.01117107,  0.01503461,  0.00755382, -0.05963881,  0.07816348,
        0.04380285, -0.09999963, -0.03154609,  0.02469472,  0.16588104,
        0.03116903,  0.11040416,  0.09537021,  0.16924927,  0.14196277,
       -0.08147729, -0.0310944 ,  0.01578016,  0.06446993, -0.00770771,
        0.04072778,  0.09360179,  0.1949345 , -0.08859233, -0.06656316,
       -0.04537171,  0.12998222,  0.1007067 ,  0.06708504,  0.04702223,
        0.05339693,  0.07188582, -0.00751081, -0.01630633,  0.1001667 ,
       -0.02479191,  0.13797905,  0.13946712, -0.0132787 ,  0.0645536 ,
        0.00065998], dtype=float32), array([[-0.11318333, -0.01394981, -0.08518942, ...,  0.17544281,
         0.01231085, -0.19481356],
       [-0.01007743, -0.26932693,  0.0530858 , ..., -0.15927719,
         0.2543153 ,  0.02261621],
       [ 0.14198484,  0.05500012, -0.15798448, ...,  0.30319262,
        -0.03104973,  0.02341664],
       ...,
       [-0.00364847,  0.09823479, -0.14419091, ..., -0.05911801,
        -0.16701627,  0.12995604],
       [ 0.24357872, -0.12123701,  0.08571428, ..., -0.07280003,
         0.03248561,  0.20008057],
       [-0.20092683, -0.08602812, -0.05572913, ...,  0.2714223 ,
        -0.06357553, -0.29062647]], dtype=float32), array([-0.01019929, -0.10848439,  0.01929379, -0.01769626, -0.00433212,
       -0.03878876, -0.02930045, -0.08742885,  0.1831188 ,  0.00656276],
      dtype=float32)]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 9-model.py ` 
 Self-paced manual review  Panel footer - Controls 
### 10. Save and Load Weights
          mandatory         Progress vs Score  Task Body Write the following functions:
*  ` def save_weights(network, filename, save_format='h5'): `  saves a model’s weights:*  ` network `  is the model whose weights should be saved
*  ` filename `  is the path of the file that the weights should be saved to
*  ` save_format `  is the format in which the weights should be saved
* Returns:  ` None ` 

*  ` def load_weights(network, filename): `  loads a model’s weights:*  ` network `  is the model to which the weights should be loaded
*  ` filename `  is the path of the file that the weights should be loaded from
* Returns:  ` None ` 

```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 10-main.py
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
one_hot = __import__('3-one_hot').one_hot
train_model = __import__('8-train').train_model
model = __import__('9-model')
weights = __import__('10-weights')

if __name__ == '__main__':

    network = model.load_model('network2.h5')
    weights.save_weights(network, 'weights2.h5')
    del network

    network2 = model.load_model('network1.h5')
    print(network2.get_weights())
    weights.load_weights(network2, 'weights2.h5')
    print(network2.get_weights())

ubuntu@alexa-ml:~/0x06-keras$ ./10-main.py
[array([[-5.2766692e-34,  1.0211001e-31, -9.2275343e-32, ...,
         5.3849467e-32,  3.5366846e-32, -1.1615496e-31],
       [ 6.5714125e-32,  5.7210311e-33,  1.3677735e-31, ...,
        -1.2400788e-31, -8.0065348e-32, -9.9800035e-32],
       [-7.1492310e-33,  4.2906925e-32, -4.7658579e-32, ...,
         1.1775981e-31,  4.3518288e-32, -2.6758248e-32],
       ...,
       [ 1.2808156e-32,  9.1814467e-32, -5.6753514e-32, ...,
         7.7905256e-33,  1.2131969e-31, -1.2632251e-31],
       [ 6.9877068e-32,  1.1129491e-31,  1.2610196e-31, ...,
        -4.7109364e-32,  3.1981172e-32, -1.0631987e-31],
       [ 1.1556884e-31, -8.1352537e-32,  1.6378386e-32, ...,
        -1.0552921e-31,  7.2167354e-32,  4.8748133e-32]], dtype=float32), array([ 0.03572803,  0.02704948, -0.06407016,  0.00194305,  0.02806898,
        0.00439573,  0.00793941,  0.02440066,  0.08288945,  0.06507709,
       -0.02791167, -0.03755917,  0.07232751, -0.01963872,  0.02347199,
        0.01682168,  0.03198532,  0.05494657,  0.02957428,  0.02176409,
        0.02411527,  0.04528563,  0.0393339 ,  0.00469558, -0.00922394,
       -0.03075291, -0.01004305, -0.02057108,  0.030173  ,  0.01545076,
        0.04102982,  0.01177916,  0.02544381,  0.03478225,  0.02835602,
        0.04815533,  0.05428997, -0.03268172,  0.00376047, -0.00795928,
        0.08478205,  0.01744418, -0.02986404, -0.01100253, -0.00260294,
        0.05266333,  0.0322946 ,  0.05051466, -0.02134753, -0.02217137,
       -0.06027266, -0.01548774,  0.04770343,  0.00912898,  0.00300289,
       -0.00656792, -0.01150438,  0.05645321, -0.0379272 ,  0.00664067,
        0.0202676 ,  0.04772714, -0.00160436, -0.01808417,  0.03831816,
       -0.0133573 ,  0.02866623,  0.05114643,  0.01029238, -0.03089232,
        0.080378  , -0.00202724,  0.01476021,  0.08648331,  0.04530144,
        0.00655621, -0.01927962,  0.03633732,  0.00238061,  0.04095837,
        0.03348523, -0.03035999,  0.03006225,  0.03722331,  0.05355305,
        0.00847669,  0.04023221,  0.05600537, -0.01635163, -0.00542682,
       -0.00788083, -0.00557583, -0.01107722,  0.00847516,  0.00672889,
       -0.00964672, -0.00160439, -0.01396684,  0.00450707,  0.00297258,
        0.03728335,  0.08524852,  0.02013785, -0.00735155, -0.0187213 ,
        0.01005496,  0.00744352,  0.019471  ,  0.05979008,  0.01703474,
       -0.00366089,  0.01534579,  0.02000004,  0.07313205,  0.01862241,
        0.02912079, -0.02121609, -0.00296072,  0.05044704,  0.03172913,
        0.03088434, -0.02506353, -0.0195832 , -0.02435748,  0.04362251,
       -0.02579482, -0.01573297, -0.01226767,  0.00829114,  0.0396717 ,
       -0.03938232, -0.00859683,  0.07211725,  0.00166503,  0.03667877,
        0.04654745, -0.03339683,  0.08614333,  0.04708391,  0.06082468,
       -0.01622846,  0.03503462, -0.06387687,  0.02342206,  0.01152485,
        0.00729862,  0.01580816, -0.02218373,  0.00728566, -0.00141596,
        0.02891568,  0.00786343,  0.0134182 ,  0.00990814, -0.01966472,
       -0.02357975,  0.04518939, -0.01605585,  0.00015184, -0.01285644,
        0.0313772 ,  0.00913673,  0.08007984,  0.0315367 , -0.01633724,
       -0.00267332, -0.01688049,  0.00403324, -0.00716459,  0.0123584 ,
       -0.00765508,  0.00995467,  0.00102515,  0.010648  , -0.02189339,
        0.05167848,  0.02507534, -0.0064791 , -0.00625046, -0.02822823,
        0.00451714,  0.01516037,  0.01314682, -0.04274836, -0.05295856,
        0.05582904,  0.06017118,  0.02113582, -0.02064131, -0.00495149,
        0.06048177,  0.04518422, -0.01597561,  0.04273272,  0.01183975,
       -0.00661544,  0.03223367,  0.00714979,  0.05064922,  0.01326511,
        0.0304067 ,  0.01526198,  0.07872824,  0.03640524,  0.02759546,
        0.02150068,  0.00991441,  0.00264893,  0.04878159,  0.06916578,
        0.08115148, -0.01821562,  0.05519837,  0.01340283,  0.05022215,
        0.02775346,  0.01537535,  0.00538236,  0.01876048,  0.04990716,
        0.03581247,  0.00607798, -0.01246338, -0.01508911,  0.05289229,
        0.05267413, -0.0186104 , -0.00815254,  0.01428892, -0.04022172,
       -0.00599986,  0.04885618,  0.01184864,  0.01389428, -0.02163312,
       -0.01811125,  0.01325182, -0.03016399,  0.0367714 ,  0.06552329,
        0.08104204, -0.01820114, -0.01029395, -0.01274729,  0.03819685,
       -0.02630511,  0.08218563,  0.01900405, -0.02611693,  0.00961591,
       -0.04004422,  0.03416308,  0.05198948,  0.02062266,  0.03306704,
        0.00758218], dtype=float32), array([[ 0.06057525,  0.04610274,  0.0314379 , ..., -0.03455189,
        -0.03450404,  0.03527202],
       [ 0.05139154, -0.01432378,  0.00697539, ...,  0.02802093,
        -0.01218623,  0.04303247],
       [ 0.00342267, -0.07965525,  0.04281894, ..., -0.02228651,
        -0.06586848,  0.0545862 ],
       ...,
       [ 0.08417522,  0.00684381, -0.05272457, ...,  0.01080573,
        -0.02592853,  0.10335934],
       [ 0.04248647, -0.05907511,  0.06153928, ..., -0.01840765,
        -0.00528464,  0.07654285],
       [ 0.03054895,  0.07551213,  0.00050304, ...,  0.06399853,
        -0.06050977,  0.06998958]], dtype=float32), array([ 0.02207945,  0.08874245,  0.01955925,  0.03192516,  0.03605681,
       -0.01441539,  0.01228592,  0.02459625,  0.08242157, -0.00283897,
        0.03931531,  0.04991587,  0.01773308,  0.04404094, -0.01266472,
        0.01449195,  0.00738691,  0.04631189,  0.04401127,  0.02292646,
        0.06334504, -0.00944531,  0.0373167 ,  0.01456188,  0.0276206 ,
       -0.01328502,  0.02594788,  0.03723697,  0.02409855,  0.05562101,
        0.05144333,  0.01890165,  0.05265031,  0.02316085,  0.03711786,
        0.05497833,  0.01561588,  0.08316742,  0.02478545,  0.07432558,
        0.05775501,  0.01712603,  0.02154928,  0.04038092,  0.01719181,
        0.02305647,  0.0540844 ,  0.05100583,  0.00916396,  0.09203168,
        0.04331684,  0.03465919,  0.02861846,  0.05870032,  0.01358662,
        0.06960062,  0.04472882,  0.06749509,  0.0112569 ,  0.05235095,
        0.02668448,  0.07252821,  0.02885697,  0.05483273,  0.05073407,
        0.01564572,  0.01721702,  0.01563549,  0.02136309,  0.04647776,
        0.10203035,  0.00791027,  0.04111913, -0.00232819,  0.04315358,
        0.01797646, -0.00615086,  0.0325475 , -0.00515893,  0.02845294,
        0.0292187 ,  0.02295299,  0.02262103,  0.02891094,  0.04795255,
        0.03997972,  0.02401389,  0.08071194,  0.05048531,  0.05139327,
        0.0253941 ,  0.01844494,  0.01435926,  0.04723575, -0.02620387,
        0.0587049 ,  0.0719058 ,  0.05107126,  0.04496412, -0.01950491,
        0.04329295,  0.03368153, -0.00312412, -0.00443249, -0.00600208,
        0.0190295 ,  0.0248791 ,  0.08633869,  0.01835669,  0.03405545,
        0.04775435,  0.01674542,  0.07998931, -0.00992179, -0.01321032,
        0.00082625,  0.01482454,  0.01055695,  0.04684956, -0.00648892,
        0.01485881,  0.01958227,  0.05116258,  0.03434751,  0.01209834,
        0.01063125,  0.04900349,  0.06372915,  0.02697382,  0.07064598,
        0.06212712,  0.00616177,  0.02533741,  0.05405494,  0.0310637 ,
        0.08036011,  0.02902372,  0.02641093,  0.05878313,  0.03034348,
        0.02314351,  0.03709875,  0.02901304,  0.04632258, -0.02186893,
        0.02218891,  0.02307222,  0.06336652,  0.01408157,  0.06869112,
        0.07657564,  0.00962182,  0.01666116,  0.05188083, -0.01123118,
        0.05064956, -0.04075339,  0.02401677,  0.03129196,  0.08546796,
        0.07259216,  0.04751649,  0.05543597,  0.05121356,  0.00243889,
        0.046884  ,  0.05398446,  0.05036417,  0.01949767,  0.01721368,
       -0.00518446,  0.0065934 ,  0.04995292,  0.07322119,  0.05660386,
       -0.00394451,  0.02295695,  0.06533987,  0.00936233,  0.0758509 ,
        0.03796108,  0.05216072,  0.04329079,  0.06980798, -0.0369533 ,
        0.04325227,  0.06236925,  0.03979746,  0.04327119,  0.08398817,
        0.03019541,  0.01792978,  0.05198518,  0.02085854, -0.02270392,
        0.00552459,  0.05481207, -0.01005377,  0.00742923,  0.01170581,
        0.00124728,  0.06066781,  0.0211939 ,  0.06051454,  0.05949872,
        0.0379996 ,  0.01351421,  0.06108748,  0.05734986,  0.02845357,
        0.03123871,  0.01785115,  0.03015261, -0.00482263,  0.05283669,
        0.00938021, -0.00096629,  0.02004887, -0.00430004,  0.04903558,
        0.03036151, -0.03775753, -0.03154609, -0.00090312,  0.06843085,
        0.02035749,  0.05548052,  0.03839224,  0.07373424,  0.07035376,
       -0.00495044, -0.0310944 ,  0.02199706,  0.03524114,  0.01391647,
        0.02955253,  0.03206335,  0.10089013, -0.02981861, -0.02458523,
       -0.00373959,  0.05090476,  0.04792825,  0.05021743,  0.02923519,
        0.05490573,  0.02738685,  0.0325878 ,  0.00446014,  0.04547186,
        0.0056865 ,  0.05285616,  0.07182238, -0.00686257,  0.04683306,
        0.02847579], dtype=float32), array([[-0.21744603, -0.02716706, -0.05761837, ...,  0.2057561 ,
         0.07971057, -0.23918046],
       [ 0.00699057, -0.2549164 ,  0.02899307, ..., -0.18303062,
         0.21898668,  0.00541965],
       [ 0.15955094,  0.03575392, -0.09666783, ...,  0.3179339 ,
        -0.01369299,  0.01115547],
       ...,
       [ 0.04872404,  0.09290916, -0.10778884, ..., -0.03850388,
        -0.08451366,  0.07337962],
       [ 0.23901998, -0.11994278,  0.14685814, ..., -0.1056491 ,
         0.0055701 ,  0.19989884],
       [-0.16856587, -0.07066552, -0.06267236, ...,  0.19716835,
        -0.1394247 , -0.29388893]], dtype=float32), array([-0.00199759, -0.02747476,  0.01542481, -0.00580232,  0.00428221,
       -0.00604941, -0.00586602, -0.03101533,  0.05028969, -0.01097175],
      dtype=float32)]
[array([[-5.2766692e-34, -4.8469041e-33, -4.5029721e-32, ...,
         5.3849467e-32,  3.5366846e-32, -8.8805072e-33],
       [ 5.3237510e-32,  5.7210311e-33, -6.5247225e-33, ...,
        -1.4632824e-32, -4.9282095e-32,  4.7386293e-33],
       [-7.1492310e-33,  4.2906925e-32, -4.7658579e-32, ...,
         9.0034874e-33,  4.3518288e-32, -2.6758248e-32],
       ...,
       [ 1.2808156e-32,  4.4801863e-32, -5.6753514e-32, ...,
         7.7905256e-33,  1.4356277e-32, -2.7032432e-32],
       [ 4.7664503e-32,  8.5089334e-33,  2.6985013e-32, ...,
        -4.7109364e-32,  3.1981172e-32,  5.0471966e-33],
       [ 8.8359207e-33, -5.0076835e-32,  1.6378386e-32, ...,
         4.9723999e-33,  4.6501740e-32,  4.8748133e-32]], dtype=float32), array([ 2.42055748e-02,  4.97336127e-02, -1.31449163e-01, -3.48263904e-02,
        3.38328965e-02,  8.94753914e-03, -3.32898926e-03,  8.62065889e-03,
        1.13340266e-01,  1.13596939e-01, -3.45221981e-02, -8.94739106e-02,
        1.17520526e-01, -3.92694436e-02,  2.54333001e-02,  3.96549702e-02,
        3.62765752e-02,  1.03379473e-01,  5.12550324e-02, -1.53140957e-02,
        3.25467549e-02,  7.13156164e-02,  7.41765946e-02,  2.84082606e-03,
       -1.70718804e-02, -6.67905062e-02, -7.24370256e-02, -3.09881531e-02,
        4.12718989e-02,  4.31933999e-02,  6.86500371e-02, -2.52993274e-02,
       -8.67424719e-03,  4.20734212e-02,  2.75987908e-02,  4.17117178e-02,
        6.57536685e-02, -7.55800307e-02, -8.00409447e-03, -4.41815220e-02,
        1.06031306e-01, -6.16804883e-03, -5.61822802e-02, -1.34118628e-02,
       -7.21768430e-03, -1.67810090e-03,  3.80131602e-02, -2.56150216e-03,
       -2.15920210e-02, -3.67936231e-02, -1.57706484e-01, -4.93261144e-02,
        3.43423039e-02, -1.09447101e-02,  1.26106478e-02, -4.99122590e-02,
       -2.03058310e-02,  1.18195854e-01, -8.57081711e-02, -2.40898635e-02,
       -1.38498889e-02,  8.14961717e-02,  6.17014663e-03, -2.50044595e-02,
        5.88443540e-02, -6.71132579e-02,  3.02182473e-02, -3.57209035e-04,
        6.04537129e-03, -4.49447483e-02,  1.21123768e-01, -1.50266513e-02,
        2.98477411e-02,  1.56357229e-01,  7.42858127e-02, -1.37053197e-02,
       -1.92796178e-02, -6.30884722e-04, -9.14448500e-03,  7.78798535e-02,
        4.73481882e-03, -5.00369817e-02,  2.01115441e-02,  4.91318516e-02,
        6.89878613e-02, -7.41709897e-04,  4.10465077e-02,  5.66071868e-02,
       -2.91831605e-02, -1.24142794e-02,  1.59813855e-02, -5.82990013e-02,
       -1.46004912e-02, -7.04847230e-03, -1.53810484e-02, -3.53730768e-02,
       -2.62593385e-02, -3.42618972e-02, -6.40405994e-03,  2.41093263e-02,
        6.69784099e-02,  1.22419134e-01,  1.40971853e-03, -3.65416706e-02,
       -6.05292656e-02,  2.94122733e-02,  2.28075054e-03,  1.54874530e-02,
        2.44536772e-02,  4.10329774e-02, -6.52046949e-02,  2.84271035e-02,
        1.30639868e-02,  9.24794748e-02,  2.55401842e-02, -1.26035213e-02,
       -2.12160926e-02, -3.07128485e-02,  1.20450005e-01,  3.44891809e-02,
        9.42492764e-03, -4.62370738e-02, -4.61713448e-02, -5.96997105e-02,
        5.24189286e-02, -1.08038336e-01, -3.57228667e-02, -6.09593131e-02,
       -1.27689177e-02,  6.05372898e-02, -8.68709981e-02, -1.77335870e-02,
        1.10579222e-01,  3.58308130e-03,  6.44926131e-02,  2.26106066e-02,
       -1.23218082e-01,  1.50772229e-01,  3.68883461e-02,  1.78936440e-02,
       -1.62284579e-02,  2.71826703e-02, -1.41632140e-01, -2.41718553e-02,
       -1.47164920e-02, -1.56504184e-03, -8.02510604e-03, -5.70992380e-02,
        4.42996109e-03, -3.61215211e-02,  7.25488039e-03, -3.61059383e-02,
       -1.67860463e-02, -1.43339010e-02, -5.86435348e-02, -5.77094555e-02,
        6.61153495e-02, -1.86597425e-02, -5.48199890e-03, -2.26344187e-02,
        2.32470129e-02, -3.16553563e-02,  1.38850778e-01,  5.08232377e-02,
       -5.55736646e-02, -3.11112832e-02, -1.68804917e-02, -4.78236228e-02,
        8.85246415e-03,  3.73947397e-02, -6.33736998e-02, -3.13683078e-02,
       -5.55279292e-02, -1.91320153e-03, -7.63542280e-02,  3.06050871e-02,
        2.08393075e-02, -2.38516252e-03, -4.03844751e-03, -6.20031208e-02,
       -3.11615635e-02,  2.10694913e-02, -1.69931185e-02, -1.22885905e-01,
       -5.40405363e-02,  9.48599279e-02, -1.46141701e-05,  2.44354960e-02,
       -6.36112615e-02, -1.85025558e-02,  1.10179268e-01,  2.54274365e-02,
       -1.59756131e-02,  5.80502255e-03,  3.50507423e-02, -2.47005634e-02,
        2.37329677e-02,  1.61884520e-02, -6.23314828e-03, -5.45048378e-02,
       -1.45113319e-02,  3.74473259e-02,  4.51476574e-02,  4.61748466e-02,
        2.49474179e-02,  5.19658923e-02, -4.17965725e-02, -3.74812558e-02,
        2.69362181e-02,  7.13354498e-02,  1.19988434e-01, -7.17105940e-02,
        4.34395261e-02,  2.69286260e-02, -1.98554760e-03, -1.25212921e-02,
        4.57909796e-03,  1.14607428e-04,  4.33304757e-02,  7.02673495e-02,
        4.33294699e-02, -3.10730692e-02, -4.91303429e-02, -4.43950891e-02,
        8.53582248e-02,  6.49776459e-02, -2.55251769e-02, -4.71857861e-02,
       -1.97647642e-02, -1.07442155e-01, -3.75369266e-02,  5.64541370e-02,
        6.39467558e-04,  4.29542823e-04, -4.01261784e-02,  9.82200634e-03,
        1.86100341e-02, -5.52693531e-02,  5.80316670e-02,  1.01340197e-01,
        1.27654761e-01, -2.42472347e-02, -4.65300158e-02, -1.27472915e-02,
       -5.06722415e-03, -7.26248622e-02,  1.37946010e-01,  1.84302162e-02,
       -9.16350633e-02,  1.74229371e-03, -4.09353971e-02,  7.93040097e-02,
        9.29820910e-02,  2.79681641e-03, -1.35056628e-03, -2.48960941e-03],
      dtype=float32), array([[ 1.90781970e-02, -9.31159779e-03,  5.60704172e-02, ...,
        -1.14697311e-02,  7.66689423e-03,  7.87436310e-03],
       [ 5.11376187e-02, -1.64989606e-02,  1.52302729e-02, ...,
         3.87132987e-02, -1.69848874e-02,  6.08293787e-02],
       [ 2.03613075e-03, -7.25060105e-02,  1.29213445e-02, ...,
        -4.64127697e-02, -2.96940506e-02,  5.07990420e-02],
       ...,
       [ 6.87275231e-02,  4.87157056e-04,  3.81113612e-03, ...,
        -6.01525418e-03, -4.17754054e-02,  8.12370181e-02],
       [ 6.26001862e-28,  1.61327321e-12, -2.38097835e-22, ...,
         8.20580706e-25, -1.04949193e-19,  8.59037164e-22],
       [ 4.65337411e-02,  5.54108508e-02, -1.37382867e-02, ...,
         6.19266927e-02, -6.61780611e-02,  8.66155177e-02]], dtype=float32), array([-0.01868563,  0.1996879 , -0.00895025,  0.0173578 ,  0.10023709,
       -0.03503804, -0.04807546,  0.01893422,  0.14127974, -0.04424038,
        0.03609959,  0.10092543, -0.0115434 ,  0.04887414, -0.04527254,
       -0.01631377, -0.03616309,  0.04294399,  0.08644044,  0.03289045,
        0.14667207, -0.0580859 ,  0.0506365 ,  0.00165932,  0.03349096,
       -0.06461567,  0.04024672,  0.02451268,  0.03542558,  0.09147327,
        0.13787933, -0.01283798,  0.08955868, -0.05454201,  0.06415799,
        0.04712304,  0.00770633,  0.166734  ,  0.02794588,  0.15199189,
        0.11747128,  0.01486994,  0.02871312,  0.05449404, -0.03602317,
       -0.01333874,  0.08274704,  0.09028156,  0.00539852,  0.17305732,
        0.03388352,  0.02410749,  0.03252971,  0.0992787 , -0.0329654 ,
        0.10095967,  0.09207146,  0.18567972, -0.03686088,  0.08430403,
        0.00457946,  0.12816532,  0.07731209,  0.07737107,  0.06228883,
        0.00870382, -0.01480328,  0.00862901,  0.04710835,  0.12403201,
        0.19892661, -0.03729338,  0.04397079, -0.04526563,  0.0771606 ,
        0.0039387 , -0.04273853,  0.01281196, -0.00307695,  0.10373595,
        0.03555368,  0.04544834,  0.01787842,  0.02592109,  0.11883164,
        0.06086406,  0.01459295,  0.13227804,  0.11683677,  0.13622735,
        0.00235192, -0.02328737, -0.03794846,  0.0938736 , -0.06584721,
        0.13379525,  0.1438743 ,  0.1263114 ,  0.12829179, -0.08921356,
        0.02283293, -0.01118934, -0.00943715, -0.05311007, -0.03138095,
       -0.02292866,  0.0263631 ,  0.14976385,  0.00199215,  0.01065395,
        0.05893238, -0.00224998,  0.19775297, -0.05182957, -0.05962858,
       -0.02659106, -0.01557225, -0.05956717,  0.12984088, -0.05588398,
        0.01537142,  0.03161344,  0.09835552,  0.01060282, -0.00357315,
       -0.0316791 ,  0.12336725,  0.1072735 ,  0.10220348,  0.12345371,
        0.12902246, -0.00485598,  0.00633773,  0.13062255,  0.06457694,
        0.1543006 ,  0.09414724,  0.01904248,  0.1222837 ,  0.08334316,
        0.02941048,  0.04507583,  0.0428214 ,  0.1258853 , -0.04674396,
       -0.00865523,  0.01828937,  0.14067617, -0.00678011,  0.09859391,
        0.20188846, -0.03063613, -0.02241488,  0.10228955, -0.0091732 ,
        0.09561207, -0.04075339,  0.01657417,  0.01278578,  0.1749786 ,
        0.14774854,  0.07352936,  0.0938032 ,  0.10489926, -0.04541885,
        0.03370849,  0.10673981,  0.07275312,  0.00319357, -0.02043473,
       -0.03737572,  0.01174216,  0.07646789,  0.13456808,  0.10993656,
       -0.00711449,  0.05465424,  0.07678176, -0.01130569,  0.13136199,
        0.12488808,  0.12369888,  0.06577432,  0.13954146, -0.0369533 ,
        0.07225734,  0.12732355,  0.06783256,  0.10996079,  0.15201433,
        0.04795463, -0.02891155,  0.10057276, -0.01139818, -0.07155777,
       -0.02464746,  0.07380133, -0.07539964, -0.0370912 , -0.01562413,
       -0.02480589,  0.07312354,  0.02197213,  0.11165579,  0.12200221,
        0.0534094 , -0.01725672,  0.1233487 ,  0.15092307,  0.06231683,
        0.02700295,  0.00486124,  0.02487769, -0.07535981,  0.1105212 ,
        0.01117107,  0.01503461,  0.00755382, -0.05963881,  0.07816348,
        0.04380285, -0.09999963, -0.03154609,  0.02469472,  0.16588104,
        0.03116903,  0.11040416,  0.09537021,  0.16924927,  0.14196277,
       -0.08147729, -0.0310944 ,  0.01578016,  0.06446993, -0.00770771,
        0.04072778,  0.09360179,  0.1949345 , -0.08859233, -0.06656316,
       -0.04537171,  0.12998222,  0.1007067 ,  0.06708504,  0.04702223,
        0.05339693,  0.07188582, -0.00751081, -0.01630633,  0.1001667 ,
       -0.02479191,  0.13797905,  0.13946712, -0.0132787 ,  0.0645536 ,
        0.00065998], dtype=float32), array([[-0.11318333, -0.01394981, -0.08518942, ...,  0.17544281,
         0.01231085, -0.19481356],
       [-0.01007743, -0.26932693,  0.0530858 , ..., -0.15927719,
         0.2543153 ,  0.02261621],
       [ 0.14198484,  0.05500012, -0.15798448, ...,  0.30319262,
        -0.03104973,  0.02341664],
       ...,
       [-0.00364847,  0.09823479, -0.14419091, ..., -0.05911801,
        -0.16701627,  0.12995604],
       [ 0.24357872, -0.12123701,  0.08571428, ..., -0.07280003,
         0.03248561,  0.20008057],
       [-0.20092683, -0.08602812, -0.05572913, ...,  0.2714223 ,
        -0.06357553, -0.29062647]], dtype=float32), array([-0.01019929, -0.10848439,  0.01929379, -0.01769626, -0.00433212,
       -0.03878876, -0.02930045, -0.08742885,  0.1831188 ,  0.00656276],
      dtype=float32)]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 10-weights.py ` 
 Self-paced manual review  Panel footer - Controls 
### 11. Save and Load Configuration
          mandatory         Progress vs Score  Task Body Write the following functions:
*  ` def save_config(network, filename): `  saves a model’s configuration in JSON format:*  ` network `  is the model whose configuration should be saved
*  ` filename `  is the path of the file that the configuration should be saved to
* Returns:  ` None ` 

*  ` def load_config(filename): `  loads a model with a specific configuration:*  ` filename `  is the path of the file containing the model’s configuration in JSON format 
* Returns: the loaded model

```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 11-main.py
#!/usr/bin/env python3
"""
Main file
"""

# Force Seed - fix for Keras
SEED = 0

import os
os.environ['PYTHONHASHSEED'] = str(SEED)
import random
random.seed(SEED)
import numpy as np
np.random.seed(SEED)
import tensorflow as tf
tf.random.set_seed(SEED)
import tensorflow.keras as K

# Imports
model = __import__('9-model')
config = __import__('11-config')

if __name__ == '__main__':
    network = model.load_model('network1.h5')
    config.save_config(network, 'config1.json')
    del network

    network2 = config.load_config('config1.json')
    network2.summary()
    print(network2.get_weights())

ubuntu@alexa-ml:~/0x06-keras$ ./11-main.py
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 784)]             0         
_________________________________________________________________
dense (Dense)                (None, 256)               200960    
_________________________________________________________________
dropout (Dropout)            (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               65792     
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
=================================================================
Total params: 269,322
Trainable params: 269,322
Non-trainable params: 0
_________________________________________________________________
[array([[ 0.09755433, -0.04439786,  0.0159336 , ...,  0.06546799,
        -0.06438305,  0.11030034],
       [-0.03220614,  0.05014394, -0.00391707, ...,  0.03036241,
        -0.023662  ,  0.05960257],
       [-0.00576851,  0.02501452,  0.03901079, ...,  0.09244704,
         0.02196747, -0.01448548],
       ...,
       [-0.08673904, -0.02818543, -0.01874564, ..., -0.01084538,
         0.05287042, -0.09048827],
       [-0.07018229,  0.04759979,  0.01407349, ...,  0.0639261 ,
        -0.02276148,  0.0597288 ],
       [-0.06414918, -0.04054742, -0.03678115, ..., -0.08878446,
        -0.06040734, -0.07033222]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.], dtype=float32), array([[ 0.16218024, -0.11585914, -0.03359719, ...,  0.06486474,
         0.13461713,  0.07519221],
       [-0.04702299, -0.08220801,  0.1295613 , ...,  0.00597018,
         0.13171004, -0.0238106 ],
       [-0.11800576, -0.01402459,  0.07212522, ..., -0.10223424,
         0.19515261, -0.04257286],
       ...,
       [-0.11955044,  0.03555733, -0.11103541, ...,  0.01749619,
        -0.01059467, -0.0881863 ],
       [ 0.02619276,  0.07703771,  0.02254102, ...,  0.14659946,
         0.1243856 ,  0.00390353],
       [ 0.03410518, -0.06848464,  0.0537805 , ...,  0.10536972,
         0.06232681,  0.00542855]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.], dtype=float32), array([[-0.11802351, -0.06863644, -0.02219525, ..., -0.01295948,
        -0.03704168,  0.02199893],
       [-0.02810469,  0.12117346, -0.1583393 , ...,  0.06340335,
         0.15330832, -0.03095309],
       [ 0.08470405, -0.17145142,  0.03714371, ...,  0.05512213,
        -0.13882932,  0.02659133],
       ...,
       [ 0.06049234,  0.0100652 ,  0.18837386, ..., -0.1163343 ,
        -0.07376061,  0.07689571],
       [-0.10010697,  0.19503419, -0.03712299, ..., -0.06838577,
         0.05296772, -0.11932171],
       [-0.08302095, -0.03794345,  0.03074163, ..., -0.03965671,
        -0.00607733, -0.07140073]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 11-config.py ` 
 Self-paced manual review  Panel footer - Controls 
### 12. Test
          mandatory         Progress vs Score  Task Body Write a function   ` def test_model(network, data, labels, verbose=True): `   that tests a neural network:
*  ` network `  is the network model to test
*  ` data `  is the input data to test the model with
*  ` labels `  are the correct one-hot labels of  ` data ` 
*  ` verbose `  is a boolean that determines if output should be printed during the testing process
* Returns: the loss and accuracy of the model with the testing data, respectively
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 12-main.py 
#!/usr/bin/env python3

import numpy as np
one_hot = __import__('3-one_hot').one_hot
load_model = __import__('9-model').load_model
test_model = __import__('12-test').test_model


if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_test = datasets['X_test']
    X_test = X_test.reshape(X_test.shape[0], -1)
    Y_test = datasets['Y_test']
    Y_test_oh = one_hot(Y_test)

    network = load_model('network2.h5')
    print(test_model(network, X_test, Y_test_oh))
ubuntu@alexa-ml:~/0x06-keras$ ./12-main.py 
313/313 [==============================] - 2s 2ms/step - loss: 0.0926 - acc: 0.9839
[0.09264915436506271, 0.9839000105857849]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 12-test.py ` 
 Self-paced manual review  Panel footer - Controls 
### 13. Predict
          mandatory         Progress vs Score  Task Body Write a function   ` def predict(network, data, verbose=False): `   that makes a prediction using a neural network:
*  ` network `  is the network model to make the prediction with
*  ` data `  is the input data to make the prediction with
*  ` verbose `  is a boolean that determines if output should be printed during the prediction process
* Returns: the prediction for the data
```bash
ubuntu@alexa-ml:~/0x06-keras$ cat 13-main.py
#!/usr/bin/env python3

import numpy as np
one_hot = __import__('3-one_hot').one_hot
load_model = __import__('9-model').load_model
predict = __import__('13-predict').predict


if __name__ == '__main__':
    datasets = np.load('../data/MNIST.npz')
    X_test = datasets['X_test']
    X_test = X_test.reshape(X_test.shape[0], -1)
    Y_test = datasets['Y_test']

    network = load_model('network2.h5')
    Y_pred = predict(network, X_test)
    print(Y_pred)
    print(np.argmax(Y_pred, axis=1))
    print(Y_test)
ubuntu@alexa-ml:~/0x06-keras$ ./13-main.py
[[9.45855305e-08 1.23174982e-06 2.32663524e-06 ... 9.99959946e-01
  8.68574972e-08 1.64004960e-05]
 [1.32828617e-07 1.36074696e-05 9.99983311e-01 ... 1.28423885e-08
  3.21557025e-07 1.41133481e-12]
 [6.16613443e-06 9.99182522e-01 1.86337289e-04 ... 3.22229840e-04
  1.28644533e-04 1.20764213e-07]
 ...
 [2.61130857e-11 6.05865438e-08 2.87002843e-12 ... 4.44421914e-07
  2.17837393e-08 1.64936665e-07]
 [2.04758184e-08 8.73848605e-10 3.49703627e-10 ... 3.04102166e-09
  4.29783067e-05 7.22518101e-10]
 [7.48594005e-08 1.35518596e-09 4.54492266e-08 ... 1.25057568e-11
  2.57464561e-09 8.55768303e-11]]
[7 2 1 ... 4 5 6]
[7 2 1 ... 4 5 6]
ubuntu@alexa-ml:~/0x06-keras$

```
 Task URLs  Github information Repo:
* GitHub repository:  ` holbertonschool-machine_learning ` 
* Directory:  ` supervised_learning/0x06-keras ` 
* File:  ` 13-predict.py ` 
 Self-paced manual review  Panel footer - Controls 
×#### Recommended Sandbox
[Running only]() 
### 1 image(0/5 Sandboxes spawned)
### Ubuntu 16.04 - Python 3.5 - Tensorflow 1.12
Ubuntu 16.04 with Python 3.5 and Tensorflow 1.12
[Run]()
